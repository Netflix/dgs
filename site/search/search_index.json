{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"announcements/","title":"New and Noteworthy","text":""},{"location":"announcements/#dgs-framework-700-may-15-2023","title":"DGS Framework 7.0.0 (May 15, 2023)","text":"<p>The latest 7.0.0 release updates the version of graphql-java from graphql-java-19.5 -&gt; graphql-java 20.2. Graphql-java-20.0 introduces breaking changes. Refer to the notes here.</p> <p>Other dependencies updated include :</p> <ul> <li>graphql-java-extended-scalars: 19.1 -&gt; 20.2</li> <li>graphql-java-extended-validation: 19.1 -&gt; 20.0</li> <li>federation-graphql-java-support: 2.1.0 -&gt; 3.0.0</li> </ul>"},{"location":"announcements/#dgs-framework-600-now-on-spring-boot-300-january-17-2023","title":"DGS Framework 6.0.0 now on Spring Boot 3.0.0! (January 17, 2023)","text":"<p>The 6.0.0 release now supports Spring Boot 3.0.0.  This is a breaking change and requires the application to be using Spring Boot 3.0.0 and JDK 17. We will continue to maintain a separate 5.x.x release train for supporting Spring Boot 2.7 for the near future for any minor bug fixes and improvements.</p> <p>The following versions are updated: * Spring Boot 3.0.0 * Spring Framework 6.0.3 * Spring Security 6.0.1 * Spring Cloud 2022.0.0 * JDK target 17</p>"},{"location":"announcements/#other-breaking-changes","title":"Other Breaking Changes","text":""},{"location":"announcements/#use-graphqlcontext-instead-of-dgscontext-for-dataloaders","title":"Use GraphQLContext instead of DgsContext for dataloaders","text":"<p>Previously, the DGS framework passed DgsContext to dataloaders as context.  CustomContext is contained in DgsContext, and is generally retrieved with a static helper. <pre><code>MyContext context = DgsContext.getCustomContext(environment);\n</code></pre> The helper DgsContext::getCustomContext is able to pull MyContext from GraphQLContext, so this is non-breaking for users that employ the recommended helper method. This is potentially a breaking change for any user code that coerces dataloader context to DgsContext manually. Updating to using the recommended <code>getCustomContext</code> should fix any resulting issues. <pre><code>MyContext context = (DgsContext)environment.context;\n</code></pre></p>"},{"location":"announcements/#upcoming-release-of-the-dgs-framework-for-spring-boot-30-january-10-2023","title":"Upcoming Release of the DGS Framework for Spring Boot 3.0 (January 10, 2023)","text":"<p>We plan to release a new version 6.x of the DGS Framework supporting Spring Boot 3.0 by end of January 2023.  There are no known additional changes required to use the new version of the DGs framework. We will continue to maintain a separate release train for the DGS framework (5.x.x) on Spring Boot 2.7 till the end of 2023. Only patches and minor features will be available on the Spring Boot 2.7 compatible releases. </p>"},{"location":"configuration/","title":"Configuration","text":""},{"location":"configuration/#configuration","title":"Configuration","text":""},{"location":"configuration/#core-properties","title":"Core Properties","text":"Name Type Default Description dgs.graphql.path String <code>\"/graphql\"</code> Path to the endpoint that will serve GraphQL requests. dgs.graphql.introspection.enabled Boolean <code>true</code> Enables graphql introspection functionality. dgs.graphql.schema-json.enabled Boolean <code>true</code> Enables schema-json endpoint functionality. dgs.graphql.schema-json.path String <code>\"/schema.json\"</code> Path to the schema-json endpoint without trailing slash. dgs.graphql.schema-locations [String] <code>\"classpath*:schema/**/*.graphql*\"</code> Location of the GraphQL schema files. dgs.graphql.graphiql.enabled Boolean <code>true</code> Enables GraphiQL functionality. dgs.graphql.graphiql.path String <code>\"/graphiql\"</code> Path to the GraphiQL endpoint without trailing slash. dgs.graphql.graphiql.title String <code>\"Simple GraphiQL Example\"</code> Title of the GraphiQL page"},{"location":"configuration/#example-configure-the-location-of-the-graphql-schema-files","title":"Example: Configure the location of the GraphQL Schema Files","text":"<p>You can configure the location of your GraphQL schema files via the <code>dgs.graphql.schema-locations</code> property. By default it will attempt to load them from the <code>schema</code> directory via the Classpath, i.e. using <code>classpath*:schema/**/*.graphql*</code>. Let's go through an example, let's say you want to change the directory from being <code>schema</code> to <code>graphql-schemas</code>, you would define your configuration as follows:</p> <pre><code>dgs:\ngraphql:\nschema-locations:\n- classpath*:graphql-schemas/**/*.graphql*\n</code></pre> <p>Now, if you want to add additional locations to look for the GraphQL Schema files you add them to the list. For example, let's say we want to also look into your <code>graphql-experimental-schemas</code>:</p> <pre><code>dgs:\ngraphql:\nschema-locations:\n- classpath*:graphql-schemas/**/*.graphql*\n- classpath*:graphql-experimental-schemas/**/*.graphql*\n</code></pre>"},{"location":"configuration/#dgs-extended-scalars-graphql-dgs-extended-scalars","title":"DGS Extended Scalars: graphql-dgs-extended-scalars","text":"Name Type Default Description dgs.graphql.extensions.scalars.enabled Boolean <code>true</code> Registered the Scalar Extensions available in graphql-java-extended-scalars for the DGS Framework. dgs.graphql.extensions.scalars.chars.enabled Boolean <code>true</code> Will register the Char scalar extension. dgs.graphql.extensions.scalars.numbers.enabled Boolean <code>true</code> Will register all numeric scalar extensions (PositiveInt, NegativeInt, NonPositiveInt, NonNegativeInt, PositiveFloat, NegativeFloat, NonPositiveFloat, NonNegativeFloat, Long, Short, Byte, BigDecimal, BigInteger). dgs.graphql.extensions.scalars.objects.enabled Boolean <code>true</code> Will register the Object, Json, Url, and Locale scalar extensions. dgs.graphql.extensions.scalars.time-dates.enabled Boolean <code>true</code> Will register the DateTime, Date, Time and LocalTime scalar extensions. dgs.graphql.extensions.scalars.ids.enabled Boolean <code>true</code> Will register the UUID scalar extension."},{"location":"configuration/#dgs-extended-validation-graphql-dgs-extended-validation","title":"DGS Extended Validation: graphql-dgs-extended-validation","text":"Name Type Default Description dgs.graphql.extensions.validation.enabled Boolean <code>true</code> Registered the Validation Schema Directive Extensions available in graphql-java-extended-validation for the DGS Framework."},{"location":"configuration/#dgs-metrics-graphql-dgs-spring-boot-micrometer","title":"DGS Metrics: graphql-dgs-spring-boot-micrometer","text":"Name Type Default Description management.metrics.dgs-graphql.enabled Boolean <code>true</code> Enables DGS' GraphQL metrics, via micrometer. management.metrics.dgs-graphql.instrumentation.enabled Boolean <code>true</code> Enables DGS' GraphQL's base instrumentation; emits <code>gql.query</code>, <code>gql.resolver</code>, and <code>gql.error</code> meters. management.metrics.dgs-graphql.data-loader-instrumentation.enabled Boolean <code>true</code> Enables DGS' instrumentation for DataLoader; emits <code>gql.dataLoader</code> meters. management.metrics.dgs-graphql.tag-customizers.outcome.enabled Boolean <code>true</code> Enables DGS' GraphQL Outcome tag customizer. This adds an OUTCOME tag that is ether SUCCESS or ERROR to the emitted gql meters. management.metrics.dgs-graphql.query-signature.enabled Boolean <code>true</code> Enables DGS' <code>QuerySignatureRepository</code>; if available metrics will be tagged with the <code>gql.query.sig.hash</code>. management.metrics.dgs-graphql.query-signature.caching.enabled Boolean <code>true</code> Enables DGS' <code>QuerySignature</code> caching; if set to false the signature will always be calculated on each request. management.metrics.dgs-graphql.tags.limiter.limit Integer 100 The limit that will apply for this tag. The interpretation of this limit depends on the cardinality limiter itself. management.metrics.dgs-graphql.autotime.percentiles [Double] [] DGS Micrometer Timers percentiles, e.g. <code>[0.95, 0.99, 0.50]</code>. 1 management.metrics.dgs-graphql.autotime.percentiles-histogram Boolean <code>false</code> Enables publishing percentile histograms for the DGS Micrometer Timers. 1 <p>Hint</p> <p>You can configure percentiles, and enable percentile histograms, directly via the per-meter customizations available out of the box in Spring Boot. For example, to enable percentile histograms for all <code>gql.*</code> meters you can set the following property:</p> <pre><code>management.metrics.distribution.percentiles-histogram.gql=true\n</code></pre> <p>For more information please refer to Spring Boot's Per Meter Properties.</p> <ol> <li> <p>Spring Boot's Per Meter Properties can be used to configure percentiles, and histograms, out of the box.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"data-loaders/","title":"Data Loaders (N+1)","text":"<p>Data loaders solve the N+1 problem while loading data.</p>"},{"location":"data-loaders/#the-n1-problem-explained","title":"The N+1 Problem Explained","text":"<p>Say you query for a list of movies, and each movie includes some data about the director of the movie. Also assume that the Movie and Director entities are owned by two different services. In a na\u00efve implementation, to load 50 movies, you would have to call the Director service 50 times: once for each movie. This totals 51 queries: one query to get the list of movies, and 50 queries to get the director data for each movie. This obviously wouldn\u2019t perform very well.</p> <p>It would be much more efficient to create a list of directors to load, and load all of them at once in a single call. This first of all must be supported by the Director service, because that service needs to provide a way to load a list of Directors. The data fetchers in the Movie service need to be smart as well, to take care of batching the requests to the Directors service.</p> <p>This is where data loaders come in.</p>"},{"location":"data-loaders/#what-if-my-service-doesnt-support-loading-in-batches","title":"What If My Service Doesn\u2019t Support Loading in Batches?","text":"<p>What if (in this example) <code>DirectorServiceClient</code> doesn\u2019t provide a method to load a list of directors? What if it only provides a method to load a single director by ID? The same problem applies to REST services as well: what if there\u2019s no endpoint to load multiple directors? Similarly, to load from a database directly, you must write a query to load multiple directors. If such methods are unavailable, the providing service needs to fix this!</p>"},{"location":"data-loaders/#implementing-a-data-loader","title":"Implementing a Data Loader","text":"<p>The easiest way for you to register a data loader is for you to create a class that implements the <code>org.dataloader.BatchLoader</code> or <code>org.dataloader.MappedBatchLoader</code> interface. This interface is parameterized; it requires a type for the key and result of the <code>BatchLoader</code>. For example, if the identifiers for a Director are of type <code>String</code>, you could have a <code>org.dataloader.BatchLoader&lt;String, Director&gt;</code>. You must annotate the class with <code>@DgsDataLoader</code> so that the framework will register the data loader it represents.</p> <p>In order to implement the <code>BatchLoader</code> interface you must implement a <code>CompletionStage&lt;List&lt;V&gt;&gt; load(List&lt;K&gt; keys)</code> method.</p> <p>The following example is a data loader that loads data from an imaginary Director service:</p> <pre><code>package com.netflix.graphql.dgs.example.dataLoader;\nimport com.netflix.graphql.dgs.DgsDataLoader;\nimport org.dataloader.BatchLoader;\nimport java.util.List;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.CompletionStage;\nimport java.util.stream.Collectors;\n@DgsDataLoader(name = \"directors\")\npublic class DirectorsDataLoader implements BatchLoader&lt;String, Director&gt; {\n@Autowired\nDirectorServiceClient directorServiceClient;\n@Override\npublic CompletionStage&lt;List&lt;Director&gt;&gt; load(List&lt;String&gt; keys) {\nreturn CompletableFuture.supplyAsync(() -&gt; directorServiceClient.loadDirectors(keys));\n}\n}\n</code></pre> <p>The data loader is responsible for loading data for a given list of keys. In this example, it just passes on the list of keys to the backend that owns <code>Director</code> (this could for example be a [gRPC] service). However, you might also write such a service so that it loads data from a database. Although this example registers a data loader, nobody will use that data loader until you implement a data fetcher that uses it.</p>"},{"location":"data-loaders/#implementing-a-data-loader-with-try","title":"Implementing a Data Loader With Try","text":"<p>If you want to handle exceptions during fetching of partial results, you can return a list of <code>Try</code> objects from the loader.  The query result will contain partial results for the successful calls and  an error for the exception case.</p> <pre><code>package com.netflix.graphql.dgs.example.dataLoader;\nimport com.netflix.graphql.dgs.DgsDataLoader;\nimport org.dataloader.BatchLoader;\nimport org.dataloader.Try;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport java.util.List;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.CompletionStage;\nimport java.util.stream.Collectors;\n@DgsDataLoader(name = \"directors\")\npublic class DirectorsDataLoader implements BatchLoader&lt;String, Try&lt;Director&gt;&gt; {\n@Autowired\nprivate DirectorServiceClient directorServiceClient;\n@Override\npublic CompletionStage&lt;List&lt;Try&lt;Director&gt;&gt;&gt; load(List&lt;String&gt; keys) {\nreturn CompletableFuture.supplyAsync(() -&gt; keys.stream()\n.map(key -&gt; Try.tryCall(() -&gt; directorServiceClient.loadDirectors(keys)))\n.collect(Collectors.toList()));\n}\n}\n</code></pre>"},{"location":"data-loaders/#provide-as-lambda","title":"Provide as Lambda","text":"<p>Because <code>BatchLoader</code> is a functional interface (an interface with only a single method), you can also provide it as a lambda expression. Technically this is exactly the same as providing a class; it\u2019s really just another way of writing it:</p> <pre><code>@DgsComponent\npublic class ExampleBatchLoaderFromField {\n@DgsDataLoader(name = \"directors\")\npublic BatchLoader&lt;String, Director&gt; directorBatchLoader = keys -&gt; CompletableFuture.supplyAsync(() -&gt; directorServiceClient.loadDirectors(keys));\n}\n</code></pre>"},{"location":"data-loaders/#mappedbatchloader","title":"MappedBatchLoader","text":"<p>The <code>BatchLoader</code> interface creates a <code>List</code> of values for a <code>List</code> of keys. You can also use the <code>MappedBatchLoader</code> which creates a <code>Map</code> of key/values for a <code>Set</code> of values. The latter is a better choice if you do not expect all keys to have a value. You register a <code>MappedBatchLoader</code> in the same way as you register a <code>BatchLoader</code>:</p> <pre><code>@DgsDataLoader(name = \"directors\")\npublic class DirectorsDataLoader implements MappedBatchLoader&lt;String, Director&gt; {\n@Autowired\nDirectorServiceClient directorServiceClient;\n@Override\npublic CompletionStage&lt;Map&lt;String, Director&gt;&gt; load(Set&lt;String&gt; keys) {\nreturn CompletableFuture.supplyAsync(() -&gt; directorServiceClient.loadDirectors(keys));\n}\n}\n</code></pre>"},{"location":"data-loaders/#using-a-data-loader","title":"Using a Data Loader","text":"<p>The following is an example of a data fetcher that uses a data loader:</p> <pre><code>@DgsComponent\npublic class DirectorDataFetcher {\n@DgsData(parentType = \"Movie\", field = \"director\")\npublic CompletableFuture&lt;Director&gt; director(DataFetchingEnvironment dfe) {\nDataLoader&lt;String, Director&gt; dataLoader = dfe.getDataLoader(\"directors\");\nString id = dfe.getArgument(\"directorId\");\nreturn dataLoader.load(id);\n}\n}\n</code></pre> <p>The code above is mostly just a regular data fetcher. However, instead of actually loading the data from another service or database, it uses the data loader to do so. You can retrieve a data loader from the <code>DataFetchingEnvironment</code> with its <code>getDataLoader()</code> method. This requires  you to pass the name of the data loader as a string. The other change to the data fetcher is that it returns a <code>CompletableFuture</code> instead of the actual type you\u2019re loading. This enables the framework to do work asynchronously, and is a requirement for batching.</p>"},{"location":"data-loaders/#using-the-dgsdatafetchingenvironment","title":"Using the DgsDataFetchingEnvironment","text":"<p>You can also get the data loader in a type-safe way by using our custom <code>DgsDataFetchingEnvironment</code>, which is an enhanced version of <code>DataFetchingEnvironment</code> in <code>graphql-java</code>, and provides <code>getDataLoader()</code> using the classname.</p> <pre><code>@DgsComponent\npublic class DirectorDataFetcher {\n@DgsData(parentType = \"Movie\", field = \"director\")\npublic CompletableFuture&lt;Director&gt; director(DgsDataFetchingEnvironment dfe) {\nDataLoader&lt;String, Director&gt; dataLoader = dfe.getDataLoader(DirectorsDataLoader.class);\nString id = dfe.getArgument(\"directorId\");\nreturn dataLoader.load(id);\n}\n}\n</code></pre> <p>The same works if you have <code>@DgsDataLoader</code> defined as a lambda instead of on a class as shown here. If you have multiple <code>@DgsDataLoader</code> lambdas defined as fields in the same class, you won't be able to use this feature. It is recommended that you use <code>getDataLoader()</code> with the loader name passed as a string in such cases.</p> <p>Note that there is no logic present about how batching works exactly; this is all handled by the framework! The framework will recognize that many directors need to be loaded when many movies are loaded, batch up all the calls to the data loader, and call the data loader with a list of IDs instead of a single ID. The data loader implemented above already knows how to handle a list of IDs, and that way it avoids the N+1 problem.</p>"},{"location":"data-loaders/#using-spring-features-such-as-securitycontextholder-inside-a-completablefuture","title":"Using Spring Features such as SecurityContextHolder inside a CompletableFuture","text":"<p>When you write async data fetchers, the code will run on worker threads. Spring internally stores some context, for example to make the SecurityContextHolder work, on the thread context however. This context wouldn\u2019t be available inside code running on a different thread, which makes fetching the Principal associated with the request not work.</p> <p>Spring Boot has a solution for this: it manages a thread pool that does have this context carry over. You can inject this solution in the following way:</p> <pre><code>@Autowired\n@DefaultExecutor\nprivate Executor executor;\n</code></pre> <p>You must pass in the executor as the second argument of the <code>supplyAsync()</code> method which is typically used to make data fetchers asynchronous.</p> <pre><code>@DgsData(parentType = \"Query\", field = \"list_things\")\npublic CompletableFuture&lt;List&lt;Thing&gt;&gt; resolve(DataFetchingEnvironment environment) {\nreturn CompletableFuture.supplyAsync(() -&gt; {\nreturn myService.getThings();\n}, executor);\n</code></pre>"},{"location":"data-loaders/#scheduled-data-loaders-with-dispatch-predicates","title":"Scheduled Data Loaders with Dispatch Predicates","text":"<p>The framework now supports setting up a Dispatch Predicate on a per data loader basis.  This allows you to configure when the batch is dispatched based on queue depth or time.  Note that the predicate will be applied for the data loader that you set the predicate up for and not across all data loaders. Here is how you can set up a <code>DispatchPredicate</code> for an example data loader: <pre><code>@DgsDataLoader(name = \"messagesWithScheduledDispatch\")\npublic class MessageDataLoaderWithDispatchPredicate implements BatchLoader&lt;String, String&gt; {\n@DgsDispatchPredicate\nDispatchPredicate pred = DispatchPredicate.dispatchIfLongerThan(Duration.ofSeconds(2));\n@Override\npublic CompletionStage&lt;List&lt;String&gt;&gt; load(List&lt;String&gt; keys) {\nreturn CompletableFuture.supplyAsync(() -&gt; keys.stream().map(key -&gt; \"hello, \" + key + \"!\").collect(Collectors.toList()));\n}\n}\n</code></pre></p> <p>In addition to defining the <code>load</code> method for the data loader class, you can specify a DispatchPredicate annotated with <code>@DgsDispatchPredicate</code> to apply that for the specific data loader.</p>"},{"location":"data-loaders/#thread-pool-optimization","title":"Thread Pool Optimization","text":"<p>Using <code>supplyAsync()</code> without a second argument will cause the main work of a data loader to run on a common thread pool  shared with most other async operations in your application. If that data loader is responsible for calling a slow service and/or subject to heavy load, the common thread pool could become fully saturated. In the worst case, this could result in application \"freezes\" as each data loader in the application awaits a free thread from the fixed-size common pool. </p> <p>To account for this, IO-bound data loaders should instead maintain their own dedicated thread pool rather than use the common pool. When choosing a thread pool, it's recommended to review the options under the Executors Javadoc, but a safe default for IO bound workloads is usually <code>Executors.newCachedThreadPool()</code>. As opposed to the fixed-size common thread pool, <code>Executors.newCachedThreadPool()</code> will  create new threads on-demand if all previously-created threads are saturated, but still prefers thread re-use when possible.</p> <pre><code>@Configuration\npublic class SlowDataLoaderConfiguration {\n@Bean(name = \"SlowDataLoaderThreadPool\")\nExecutor slowDataLoaderExecutor() {\nreturn Executors.newCachedThreadPool();\n}\n}\n</code></pre> <p>Individual Bean names combined with <code>@Qualifier</code> annotations will result in the proper executor being autowired to the corresponding data loader.</p> <pre><code>@DgsDataLoader(name = \"slow\")\npublic class SlowDataLoader implements MappedBatchLoader&lt;String, Snail&gt; {\n@Autowired\n@Qualifier(\"SlowDataLoaderThreadPool\")\nExecutor dedicatedExecutor;\n@Autowired\nDirectorServiceClient directorServiceClient;\n@Override\npublic CompletionStage&lt;Map&lt;String, Snail&gt;&gt; load(Set&lt;String&gt; keys) {\n// This slow operation will now run on a dedicated thread pool instead of the common pool\nreturn CompletableFuture.supplyAsync(() -&gt; directorServiceClient.loadSlowData(keys), dedicatedExecutor);\n}\n}\n</code></pre> <p>Note that a custom executor will not carry Spring Security context automatically like the <code>@DefaultExecutor</code> would.  Further documentation on passing Spring Security context between threads can be found in the Spring Security Concurrency docs.</p>"},{"location":"data-loaders/#caching","title":"Caching","text":"<p>Batching is the most important aspect of preventing N+1 problems. Data loaders also support caching, however. If the same key is loaded multiple times, it will only be loaded once. For example, if a list of movies is loaded, and some movies are directed by the same director, the director data will only be retrieved once.</p> <p>Caching is Disabled by Default in DGS 1</p> <p>Version 1 of the DGS framework disables caching by default, but you can switch it on in the <code>@DgsDataLoader</code> annotation:</p> <pre><code>@DgsDataLoader(name = \"directors\", caching=true)\nclass DirectorsBatchLoader implements BatchLoader&lt;String, Director&gt; {}\n</code></pre> <p>You do not need to make this change in version 2 of the DGS framework, because that version enables caching by default.</p>"},{"location":"data-loaders/#batch-size","title":"Batch Size","text":"<p>Sometimes it\u2019s possible to load multiple items at once, but to a certain limit. When loading from a database for example, an <code>IN</code> query could be used, but maybe with the limitation of a maximum number of IDs to provide. The <code>@DgsDataLoader</code> has a <code>maxBatchSize</code> annotation that you can use to configure this behavior. By default it does not specify a maximum batch size.</p>"},{"location":"data-loaders/#data-loader-scope","title":"Data Loader Scope","text":"<p>Data loaders are wired up to only span a single request. This is what most use cases require. Spanning multiple requests can introduce difficult-to-debug issues.</p>"},{"location":"datafetching/","title":"Data fetching","text":"<p>In the getting started guide we introduced the <code>@DgsQuery</code> annotation, which you use to create a data fetcher. In this section, we look at some of the finer details of datafetchers.</p>"},{"location":"datafetching/#the-dgsdata-dgsquery-dgsmutation-and-dgssubscription-annotations","title":"The @DgsData, @DgsQuery, @DgsMutation and @DgsSubscription Annotations","text":"<p>You use the <code>@DgsData</code> annotation on a Java/Kotlin method to make that method a datafetcher. The method must be in a <code>@DgsComponent</code> class. The <code>@DgsData</code> annotation has two parameters:</p> Parameter Description <code>parentType</code> This is the type that contains the field. <code>field</code> The field that the datafetcher is responsible for <p>For example, we have the following schema.</p> <p><pre><code>type Query {\n   shows: [Show]\n}\n\ntype Show {\n  title: String\n  actors: [Actor]\n}\n</code></pre> We can implement this schema with a single datafetcher.</p> <pre><code>@DgsComponent\npublic class ShowDataFetcher {\n@DgsData(parentType = \"Query\", field = \"shows\")\npublic List&lt;Show&gt; shows() {\n//Load shows from a database and return the list of Show objects\nreturn shows;\n}\n}\n</code></pre> <p>If the <code>field</code> parameter is not set, the method name will be used as the field name. The <code>@DgsQuery</code>, <code>@DgsMutation</code> and <code>@DgsSubscription</code> annotations are shorthands to define datafetchers on the <code>Query</code>, <code>Mutation</code> and <code>Subscription</code> types. The following definitions are all equivalent.</p> <pre><code>@DgsData(parentType = \"Query\", field = \"shows\")\npublic List&lt;Show&gt; shows() { .... }\n// The \"field\" argument is omitted. It uses the method name as the field name.\n@DgsData(parentType = \"Query\")\npublic List&lt;Show&gt; shows() { .... }\n// The parentType is \"Query\", the field name is derived from the method name.\n@DgsQuery\npublic List&lt;Show&gt; shows() { .... }\n// The parentType is \"Query\", the field name is explicitly specified.\n@DgsQuery(field = \"shows\")\npublic List&lt;Show&gt; shows() { .... }\n</code></pre> <p>Notice how a datafetcher can return complex objects or lists of objects. You don't have to create a separate datafetcher for each field. The framework will take care of only returning the fields that are specified in the query. For example, if a user queries:</p> <p><pre><code>{\n    shows {\n        title\n    }\n}\n</code></pre> Although we're returning Show objects, which in the example contains both a <code>title</code> and an <code>actors</code> field, the <code>actors</code> field gets stripped off before the response gets sent back.</p>"},{"location":"datafetching/#child-datafetchers","title":"Child Datafetchers","text":"<p>The previous example assumed that you could load a list of <code>Show</code> objects from your database with a single query. It wouldn't matter which fields the user included in the GraphQL query; the cost of loading the shows would be the same. What if there is an extra cost to specific fields? For example, what if loading actors for a show requires an extra query? It would be wasteful to run the extra query to load actors if the <code>actors</code> field doesn't get returned to the user.</p> <p>In such scenarios, it's better to create a separate datafetcher for the expensive field.</p> <p><pre><code>@DgsQuery\npublic List&lt;Show&gt; shows() {\n//Load shows, which doesn't include \"actors\"\nreturn shows;\n}\n@DgsData(parentType = \"Show\", field = \"actors\")\npublic List&lt;Actor&gt; actors(DgsDataFetchingEnvironment dfe) {\nShow show = dfe.getSource();\nactorsService.forShow(show.getId());\nreturn actors;\n}\n</code></pre> The <code>actors</code> datafetcher only gets executed when the <code>actors</code> field is included in the query. The <code>actors</code> datafetcher also introduces a new concept; the <code>DgsDataFetchingEnvironment</code>. The <code>DgsDataFetchingEnvironment</code> gives access to the <code>context</code>, the query itself, data loaders, and the <code>source</code> object. The source object is the object that contains the field. For this example, the source is the <code>Show</code> object, which you can use to get the show's identifier to use in the query for actors.</p> <p>Do note that the <code>shows</code> datafetcher is returning a list of <code>Show</code>, while the <code>actors</code> datafetcher fetches the actors for a single show. The framework executes the <code>actors</code> datafetcher for each <code>Show</code> returned by the <code>shows</code> datafetcher. If the actors get loaded from a database, this would now cause an N+1 problem. To solve the N+1 problem, you use data loaders.</p> <p>Note: There are more complex scenarios with nested datafetchers, and ways to pass context between related datafetchers. See the nested datafetchers guide for more advanced use-cases.</p>"},{"location":"datafetching/#multiple-dgsdata-annotations-via-dgsdatalist","title":"Multiple @DgsData Annotations Via @DgsData.List","text":"<p>Since v4.6.0, methods can be annotated with multiple <code>@DgsData</code> annotations. Effectively you can resolve multiple GraphQL Type fields via the same method implementation. To do so you will have to leverage the <code>@DgsData.List</code> annotation, for example:</p> JavaKotlin <pre><code>@DgsData.List({\n@DgsData(parentType = \"Query\", field = \"movies\"),\n@DgsData(parentType = \"Query\", field = \"shows\")\n})\n</code></pre> <pre><code>@DgsData.List(\nDgsData(parentType = \"Query\", field = \"movies\"),\nDgsData(parentType = \"Query\", field = \"shows\")\n)\n</code></pre> <p>Tip</p> <p>Both <code>@DgsQuery</code> and <code>@DgsMutation</code> can't be defined multiple times in a single method. Please use <code>@DgsData</code> instead and explicitly define the <code>parentType</code> to match either <code>Query</code> or <code>Mutation</code>.</p>"},{"location":"datafetching/#using-inputargument","title":"Using @InputArgument","text":"<p>It's very common for GraphQL queries to have one or more input arguments. According to the GraphQL specification, an input argument can be:</p> <ul> <li>An input type</li> <li>A scalar</li> <li>An enum</li> </ul> <p>Other types, such as output types, unions and interfaces, are not allowed as input arguments.</p> <p>You can get input arguments as method arguments in a datafetcher method using the <code>@InputArgument</code> annotation.</p> <pre><code>type Query {\n    shows(title: String, filter: ShowFilter): [Show]\n}\n\ninput ShowFilter {\n   director: String\n   genre: ShowGenre\n}\n\nenum ShowGenre {\n   commedy, action, horror\n}\n</code></pre> <p>We can write a datafetcher with the following signature: <pre><code>@DgsQuery\npublic List&lt;Show&gt; shows(@InputArgument String title, @InputArgument ShowFilter filter)\n</code></pre></p> <p>The <code>@InputArgument</code> annotation will use the name of the method argument to match it with the name of an input argument sent in the query. Optionally we can specify the <code>name</code> argument in the <code>@InputArgument</code> annotation, if the argument name doesn't match the method argument name.</p> <p>The framework converts input arguments to Java/Kotlin types. The first step for converting input arguments is <code>graphql-java</code> using scalar implementations to convert raw string input into whatever type the scalar represents. A GraphQL <code>Int</code> becomes an <code>Integer</code> in Java, a formatted date string becomes a <code>LocalDateTime</code> (depending on the scalars you're using!), lists become an instance of <code>java.util.ArrayList</code>. Input objects are represented as a <code>Map&lt;String, Object&gt;</code> in <code>graphql-java</code>.</p> <p>The next step is the DGS Framework converting the <code>Map&lt;String, Object&gt;</code> to the Java/Kotlin classes that you use for the <code>@InputArgument</code>. For Java classes, the framework creates a new instance of the class using the no-arg constructor. This implies that a no-arg constructor is required. It then sets each field of the instance to the input argument values.</p> <p>For Kotlin Data classes, the instance can only be created by passing in all arguments in the constructor. This means you have to make sure to make fields optional in the data class when the fields are optional in the GraphQL schema!</p> <p>If you're using the Codegen plugin (you really should!), the input types will work perfectly out of the box.</p> <p>Input argument conversion isn't JSON</p> <p>It's easy to confuse the conversion described above with JSON deserialization as you are familiar with in libraries such as Jackson. Although it looks similar, the mechanisms are completely unrelated.  Input arguments aren't JSON, and the Scalar mechanism is really the core of how conversion works. This also means that Jackson annotations on Java/Kotlin types are not used at all. </p> <p>Defining scalars, and scalars in codegen</p> <p>@InputArgument is designed to work well with scalars. More information about defining custom scalars in the framework can be found here. For a scalar you typically either create a class representing the value, or use an existing type.  Such types need to be mapped in Codegen configuration so that they don't (incorrectly) get generated.</p>"},{"location":"datafetching/#nullability-in-kotlin-for-input-arguments","title":"Nullability in Kotlin for Input Arguments","text":"<p>If you're using Kotlin you must consider if an input type is nullable. If the schema defines an input argument as nullable, the code must reflect this by using a nullable type. If a non-nullable type receives a null value, Kotlin will throw an exception.</p> <p>For example:</p> <p><pre><code># name is a nullable input argument\nhello(name: String): String\n</code></pre> You must write the datafetcher function as: <pre><code>fun hello(@InputArgument hello: String?)\n</code></pre></p> <p>In Java you don't have to worry about this, types can always be null assuming you are using boxed types.  Generally, we recommend using boxed types instead of unboxed if you are expecting null input. You do need to null check in your datafetching code for handling possible null inputs. If using unboxed types, null input will result in exceptions.</p>"},{"location":"datafetching/#using-inputargument-with-lists","title":"Using @InputArgument with lists","text":"<p>An input argument can also be a list. If the list type is an input type, you must specify the type explicitly in the <code>@InputArgument</code> annotation.</p> <pre><code>type Query {\n    hello(people:[Person]): String\n}\n</code></pre> <pre><code>public String hello(@InputArgument(collectionType = Person.class) List&lt;Person&gt; people)\n</code></pre>"},{"location":"datafetching/#using-optional-with-inputargument","title":"Using Optional with @InputArgument","text":"<p>Input arguments are often defined as optional in schemas. Your datafetcher code needs to null-check arguments to check if they were provided. Instead of null-checks you can wrap an input argument in an Optional.</p> <pre><code>public List&lt;Show&gt; shows(@InputArgument(collectionType = ShowFilter.class) Optional&lt;ShowFilter&gt; filter)\n</code></pre> <p>You do need to provide the type in the <code>collectionType</code> argument when using complex types, similar to using lists. If the argument is not provided, the value will be <code>Optional.empty()</code>. It's a matter of preference to use <code>Optional</code> or not.</p>"},{"location":"datafetching/#codegen-constants","title":"Codegen Constants","text":"<p>In the examples of <code>@DgsData</code> so far, we used string values for the <code>parentType</code> and <code>field</code> arguments. If you are using code generation you can instead use generated constants. Codegen creates a <code>DgsConstants</code> class with constants for each type and field in your schema. Using this we can write a datafetcher as follows:</p> <pre><code>type Query {\n    shows: [Show]\n}\n</code></pre> <pre><code>@DgsData(parentType = DgsConstants.QUERY_TYPE, field = DgsConstants.QUERY.Shows)\npublic List&lt;Show&gt; shows() {}\n</code></pre> <p>The benefit of using constants is that you can detect issues between your schema and datafetchers at compile time.</p>"},{"location":"datafetching/#requestheader-requestparam-and-cookievalue","title":"@RequestHeader, @RequestParam and @CookieValue","text":"<p>Sometimes you need to evaluate HTTP headers, or other elements of the request, in a datafetcher. You can easily get an HTTP header value by using the <code>@RequestHeader</code> annotation. The <code>@RequestHeader</code> annotation is the same annotation as used in Spring WebMVC.</p> <pre><code>@DgsQuery\npublic String hello(@RequestHeader String host)\n</code></pre> <p>Technically, headers are lists of values. If multiple values are set, you can retrieve them as a list by using a List as your argument type. Otherwise, the values are concatenated to a single String. Similar to <code>@InputArgument</code> it's possible to wrap a header or parameter in an <code>Optional</code>.</p> <p>Similarly, you can get request parameters using <code>@RequestParam</code>. Both <code>@RequestHeader</code> and <code>@RequestParam</code> support a <code>defaultValue</code> and <code>required</code> argument. If a <code>@RequestHeader</code> or <code>@RequestParam</code> is <code>required</code>, doesn't have a <code>defaultValue</code> and isn't provided, a <code>DgsInvalidInputArgumentException</code> is thrown.</p> <p>To easily get access to cookie values you can use Spring's <code>@CookieValue</code> annotation.</p> <pre><code>@DgsQuery\npublic String usingCookieWithDefault(@CookieValue(defaultValue = \"defaultvalue\") myCookie: String) {\nreturn myCookie\n}\n</code></pre> <p><code>@CookieValue</code> supports a <code>defaultValue</code> and the <code>required</code> argument. You can also use an <code>Optional&lt;String&gt;</code> for a <code>@CookieValue</code> if it's not required.</p>"},{"location":"datafetching/#using-dgsrequestdata-to-get-access-to-the-request-object","title":"Using DgsRequestData to get access to the request object","text":"<p>You can get access to the request object, representing the HTTP request itself, as well. It's stored on the <code>DgsContext</code> object in the <code>DgsDataFetchingEnvironment</code>. </p> <p>Because Spring WebMVC and Spring Webflux use different types to represent the request, the <code>DgsRequestData</code> is different depending on what environment (WebMVC/WebFlux) you're running in. The <code>DgsRequestData</code> interface only gives access to the request headers and the <code>extensions</code>. To get the actual request object, you need to cast the <code>DgsRequestData</code> to the correct implementation. This is either <code>DgsWebMvcRequestData</code> or <code>DgsReactiveRequestData</code>. Let's use this in an example to set a cookie, which is done through the response object.</p> <p>Let's look at a WebMVC example first. From the <code>DgsWebMvcRequestData</code> you can get the <code>WebRequest</code>, which can be further cast to a <code>ServletWebRequest</code>.</p> <pre><code>@DgsQuery\n@DgsMutation\npublic String updateCookie(@InputArgument String value, DgsDataFetchingEnvironment dfe) {\nDgsWebMvcRequestData requestData = (DgsWebMvcRequestData) dfe.getDgsContext().getRequestData();\nServletWebRequest webRequest = (ServletWebRequest) requestData.getWebRequest();\njavax.servlet.http.Cookie cookie = new javax.servlet.http.Cookie(\"mydgscookie\", value);\nwebRequest.getResponse().addCookie(cookie);\nreturn value;\n}\n</code></pre> <p>Now let's try the same with WebFlux. <code>DgsRequestData</code> is now an instance of <code>DgsReactiveRequestData</code>, which gives access to the <code>ServerRequest</code>.</p> <pre><code>@DgsMutation\npublic String updateCookie(@InputArgument String value, DgsDataFetchingEnvironment dfe) {\nDgsReactiveRequestData requestData = (DgsReactiveRequestData) dfe.getDgsContext().getRequestData();\nServerRequest serverRequest = requestData.getServerRequest();\nserverRequest.exchange().getResponse()\n.addCookie(ResponseCookie.from(\"mydgscookie\", \"webfluxupdated\").build());\nreturn value;\n}\n</code></pre>"},{"location":"datafetching/#using-data-fetcher-context","title":"Using data fetcher context","text":"<p>The <code>DgsRequestData</code> object described in the previous section is part of the data fetching context. The DGS Framework adds the <code>DgsRequestData</code> to the data fetching context. You can also add your own data to the context, for use in data fetchers. The context is initialized per request, before query execution starts.</p> <p>You can customize the context and add your own data by creating a <code>DgsCustomContextBuilder</code>.</p> <pre><code>@Component\npublic class MyContextBuilder implements DgsCustomContextBuilder&lt;MyContext&gt; {\n@Override\npublic MyContext build() {\nreturn new MyContext();\n}\n}\npublic class MyContext {\nprivate final String customState = \"Custom state!\";\npublic String getCustomState() {\nreturn customState;\n}\n}\n</code></pre> <p>If you require access to the request, e.g. to read HTTP headers, you can implement the <code>DgsCustomContextBuilderWithRequest</code> interface instead.</p> <pre><code>@Component\npublic class MyContextBuilder implements DgsCustomContextBuilderWithRequest&lt;MyContext&gt; {\n@Override\npublic MyContext build(Map&lt;String, Object&gt; extensions, HttpHeaders headers, WebRequest webRequest) {\n//e.g. you can now read headers to set up context\nreturn new MyContext();\n}\n}\n</code></pre> <p>A data fetcher can now retrieve the context by calling the <code>getCustomContext()</code> method:</p> <pre><code>@DgsData(parentType = \"Query\", field = \"withContext\")\npublic String withContext(DataFetchingEnvironment dfe) {\nMyContext customContext = DgsContext.getCustomContext(dfe);\nreturn customContext.getCustomState();\n}\n</code></pre> <p>Similarly, custom context can be used in a DataLoader.</p> <pre><code>@DgsDataLoader(name = \"exampleLoaderWithContext\")\npublic class ExampleLoaderWithContext implements BatchLoaderWithContext&lt;String, String&gt; {\n@Override\npublic CompletionStage&lt;List&lt;String&gt;&gt; load(List&lt;String&gt; keys, BatchLoaderEnvironment environment) {\nMyContext context = DgsContext.getCustomContext(environment);\nreturn CompletableFuture.supplyAsync(() -&gt; keys.stream().map(key -&gt; context.getCustomState() + \" \" + key).collect(Collectors.toList()));\n}\n}\n</code></pre>"},{"location":"error-handling/","title":"Error Handling","text":"<p>It is common in GraphQL to support error reporting by adding an <code>errors</code> block to a response. Responses can contain both data and errors, for example when some fields where resolved successfully, but other fields had errors. A field with an error is set to null, and an error is added to the <code>errors</code> block.</p> <p>The DGS framework has an exception handler out-of-the-box that works according to the specification described in the <code>Error Specification</code> section on this page. This exception handler handles exceptions from data fetchers. Any <code>RuntimeException</code> is translated to a <code>GraphQLError</code> of type <code>INTERNAL</code>. For some specific exception types, a more specific GraphQL error type is used.</p> Exception type GraphQL error type description <code>AccessDeniedException</code> <code>PERMISSION_DENIED</code> When a <code>@Secured</code> check fails <code>DgsEntityNotFoundException</code> <code>NOT_FOUND</code> Thrown by the developer when a requested entity (e.g. based on query parameters) isn't found"},{"location":"error-handling/#mapping-custom-exceptions","title":"Mapping custom exceptions","text":"<p>It can be useful to map application specific exceptions to meaningful exceptions back to the client. You can do this by registering a <code>DataFetcherExceptionHandler</code>. Make sure to delegate to the <code>DefaultDataFetcherExceptionHandler</code> class, this is the default exception handler of the framework. If you don't delegate to this class, you lose the framework's built-in exception handler.</p> <p>The following is an example of a custom exception handler implementation.</p> <pre><code>@Component\npublic class CustomDataFetchingExceptionHandler implements DataFetcherExceptionHandler {\n@Override\npublic CompletableFuture&lt;DataFetcherExceptionHandlerResult&gt; handleException(DataFetcherExceptionHandlerParameters handlerParameters) {\nif (handlerParameters.getException() instanceof MyException) {\nMap&lt;String, Object&gt; debugInfo = new HashMap&lt;&gt;();\ndebugInfo.put(\"somefield\", \"somevalue\");\nGraphQLError graphqlError = TypedGraphQLError.newInternalErrorBuilder()\n.message(\"This custom thing went wrong!\")\n.debugInfo(debugInfo)\n.path(handlerParameters.getPath()).build();\nDataFetcherExceptionHandlerResult result = DataFetcherExceptionHandlerResult.newResult()\n.error(graphqlError)\n.build();\nreturn CompletableFuture.completedFuture(result);\n} else {\nreturn DataFetcherExceptionHandler.super.handleException(handlerParameters);\n}\n}\n}\n</code></pre> <p>The following data fetcher throws <code>MyException</code>.</p> <pre><code>@DgsComponent\npublic class HelloDataFetcher {\n@DgsData(parentType = \"Query\", field = \"hello\")\n@DgsEnableDataFetcherInstrumentation(false)\npublic String hello(DataFetchingEnvironment dfe) {\nthrow new MyException();\n}\n}\n</code></pre> <p>Querying the <code>hello</code> field results in the following response.</p> <pre><code>{\n\"errors\": [\n{\n\"message\": \"This custom thing went wrong!\",\n\"locations\": [],\n\"path\": [\n\"hello\"\n],\n\"extensions\": {\n\"errorType\": \"INTERNAL\",\n\"debugInfo\": {\n\"somefield\": \"somevalue\"\n}\n}\n}\n],\n\"data\": {\n\"hello\": null\n}\n}\n</code></pre>"},{"location":"error-handling/#error-specification","title":"Error specification","text":"<p>There are two families of errors we typically encounter for GraphQL:</p> <ol> <li>Comprehensive Errors.    These are unexpected errors and do not represent a condition that the end user can be expected to fix.    Errors of this sort are generally applicable to many types and fields.    Such errors appear in the <code>errors</code> array in the GraphQL response.</li> <li>Errors as Data.    These are errors that are informative to the end user (for example: \u201cthis title is not available in your country\u201d or \u201cyour account has been suspended\u201d).    Errors of this sort are typically specific to a particular use case and apply only to certain fields or to a certain subset of fields.    These errors are part of the GraphQL schema.</li> </ol>"},{"location":"error-handling/#the-graphqlerror-interface","title":"The GraphQLError Interface","text":"<p>The GraphQL specification provides minimal guidance on the structure of an error. The only required field is a <code>message</code> String, which has no defined format. In Studio Edge we would like to have a stronger, more expressive contract. Here is the definition we are using:</p> field type description <code>message</code> (non-nullable) <code>String!</code> a string description of the error intended for the developer as a guide to understand and correct the error <code>locations</code> <code>[Location]</code> an array of code locations, where each location is a map with the keys <code>line</code> and <code>column</code>, both natural numbers starting from 1 that describe the beginning of an associated syntax element <code>path</code> <code>[String | Int]</code> if the error is associated with one or more particular fields in the response, this field of the error details the paths of those response fields that experienced the error (this allows clients to identify whether a <code>null</code> result is intentional or caused by a runtime error) <code>extensions</code> <code>[TypedError]</code> see \u201cThe TypedError Interface\u201d below <pre><code>\"\"\"\nError format as defined in GraphQL Spec\n\"\"\"\ninterface GraphQLError {\n    message: String! // Required by GraphQL Spec\n    locations: [Location] // See GraphQL Spec\n    path: [String | Int] // See GraphQL Spec\n    extensions: TypedError\n}\n</code></pre> <p>See the GraphQL specification: Errors for more information.</p>"},{"location":"error-handling/#the-typederror-interface","title":"The TypedError Interface","text":"<p>Studio Edge defines <code>TypedError</code> as follows:</p> field type description <code>errorType</code> (non-nullable) <code>ErrorType!</code> an enumerated error code that is meant as a fairly coarse characterization of an error, sufficient for client-side branching logic <code>errorDetail</code> <code>ErrorDetail</code> an enumeration that provides more detail about the error, including its specific cause (the elements of this enumeration are subject to change and are not documented here) <code>origin</code> <code>String</code> the name of the source that issued the error (for instance the name of a backend service, DGS, gateway, client library, or client app) <code>debugInfo</code> <code>DebugInfo</code> if the request included a flag indicating that it wanted debug information, this field contains that additional information (such as a stack trace or additional reporting from an upstream service) <code>debugUri</code> <code>String</code> the URI of a page that contains additional information that may be helpful in debugging the error (this could be a generic page for errors of this sort, or a specific page about the particular error instance) <pre><code>interface TypedError {\n    \"\"\"\n    An error code from the ErrorType enumeration.\n    An errorType is a fairly coarse characterization\n    of an error that should be sufficient for client\n    side branching logic.\n    \"\"\"\n    errorType: ErrorType!\n\n    \"\"\"\n    The ErrorDetail is an optional field which will\n    provide more fine grained information on the error\n    condition. This allows the ErrorType enumeration to\n    be small and mostly static so that application branching\n    logic can depend on it. The ErrorDetail provides a\n    more specific cause for the error. This enumeration\n    will be much larger and likely change/grow over time.\n    \"\"\"\n    errorDetail: ErrorDetail\n\n    \"\"\"\n    Indicates the source that issued the error. For example, could\n    be a backend service name, a domain graph service name, or a\n    gateway. In the case of client code throwing the error, this\n    may be a client library name, or the client app name.\n    \"\"\"\n    origin: String\n\n    \"\"\"\n    Optionally provided based on request flag\n    Could include e.g. stacktrace or info from\n    upstream service\n    \"\"\"\n    debugInfo: DebugInfo\n\n    \"\"\"\n    Http URI to a page detailing additional\n    information that could be used to debug\n    the error. This information may be general\n    to the class of error or specific to this\n    particular instance of the error.\n    \"\"\"\n    debugUri: String\n}\n</code></pre>"},{"location":"error-handling/#the-errortype-enumeration","title":"The ErrorType Enumeration","text":"<p>The following table shows the available <code>ErrorType</code> <code>enum</code> values:</p> type description HTTP analog <code>BAD_REQUEST</code> This indicates a problem with the request. Retrying the same request is not likely to succeed. An example would be a query or argument that cannot be deserialized. 400 Bad Request <code>FAILED_PRECONDITION</code> The operation was rejected because the system is not in a state required for the operation\u2019s execution. For example, the directory to be deleted is non-empty, an <code>rmdir</code> operation is applied to a non-directory, etc. Use <code>UNAVAILABLE</code> instead if the client can retry just the failing call without waiting for the system state to be explicitly fixed. 400 Bad Request, or 500 Internal Server Error <code>INTERNAL</code> This indicates that an unexpected internal error was encountered: some invariants expected by the underlying system have been broken. This error code is reserved for serious errors. 500 Internal Server Error <code>NOT_FOUND</code> This could apply to a resource that has never existed (e.g. bad resource id), or a resource that no longer exists (e.g. cache expired). Note to server developers: if a request is denied for an entire class of users, such as gradual feature rollout or undocumented allowlist, <code>NOT_FOUND</code> may be used. If a request is denied for some users within a class of users, such as user-based access control, <code>PERMISSION_DENIED</code> must be used. 404 Not Found <code>PERMISSION_DENIED</code> This indicates that the requester does not have permission to execute the specified operation. <code>PERMISSION_DENIED</code> must not be used for rejections caused by exhausting some resource or quota. <code>PERMISSION_DENIED</code> must not be used if the caller cannot be identified (use <code>UNAUTHENTICATED</code> instead for those errors). This error does not imply that the request is valid or the requested entity exists or satisfies other pre-conditions. 403 Forbidden <code>UNAUTHENTICATED</code> This indicates that the request does not have valid authentication credentials but the route requires authentication. 401 Unauthorized <code>UNAVAILABLE</code> This indicates that the service is currently unavailable. This is most likely a transient condition, which can be corrected by retrying with a backoff. 503 Unavailable <code>UNKNOWN</code> This error may be returned, for example, when an error code received from another address space belongs to an error space that is not known in this address space. Errors raised by APIs that do not return enough error information may also be converted to this error. If a client sees an <code>errorType</code> that is not known to it, it will be interpreted as <code>UNKNOWN</code>. Unknown errors must not trigger any special behavior. They may be treated by an implementation as being equivalent to <code>INTERNAL</code>. 520 Unknown Error  The HTTP analogs are only rough mappings that are given here to provide a quick conceptual explanation of the semantics of the error by showing their analogs in the HTTP specification."},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#common-patterns","title":"Common Patterns","text":"<p>We have several example applications that demonstrate the use of the DGS framework in a fully working example. Each example has detailed documentation in the corresponding GitHub repository, and is further explained in code comments.</p> <ul> <li>Java DGS example: An implementation of a typical GraphQL service</li> <li>Kotlin DGS example: The same example as above, but implemented in Kotlin</li> <li>Federation examples: A Federated GraphQL example, using Apollo Gateway</li> </ul>"},{"location":"examples/#community-contributions","title":"Community Contributions","text":"<p>We welcome contributions from the community. Check out some of these examples for more patterns.</p> <ul> <li>Authorization using custom directives: Authz using a custom @secured directive</li> </ul>"},{"location":"federation/","title":"Federation","text":"<p>Federation is based on the Federation spec.</p> <p>A DGS is federation-compatible out of the box with the ability to reference and extend federated types.</p> <p>There is more federation documentation available</p> <ul> <li>Read the Federation Spec.</li> <li>Check out Federated Testing to learn how to write tests for federated queries.</li> </ul>"},{"location":"federation/#federation-example-dgs","title":"Federation Example DGS","text":"<p>This is a DGS example that demonstrates how to implement a federated type, and test federated queries. The source code in this guide comes from the Federation example app. We highly recommend cloning the project and use the IDE while following this guide.</p> <p>The example project has the following set up:</p> <ol> <li>A federated gateway is set up using Apollo's federation gateway libraries.</li> <li>The Shows DGS defines and owns the <code>Show</code> type.</li> <li>The Reviews DGS adds a <code>reviews</code> field to the <code>Show</code> type.</li> </ol> <p>Info</p> <p>If you are completely new to the DGS framework, please take a look at the DGS Getting Started guide, which also contains an introduction video. The remainder of the guide on this page assumes basic GraphQL and DGS knowledge, and focuses on more advanced use cases.</p>"},{"location":"federation/#defining-a-federated-type","title":"Defining a federated type","text":"<p>The Shows DGS defines the <code>Show</code> type with fields id, title and releaseYear.  Note that the <code>id</code> field is marked as the key.  The example has one key, but you can have multiple keys as well <code>@key(fields:\"fieldA fieldB\")</code> This indicates to the gateway that the <code>id</code> field will be used for identifying the corresponding Show in the Shows DGS and must be specified for federated types. <pre><code>type Query {\n  shows(titleFilter: String): [Show]\n}\n\ntype Show @key(fields: \"id\") {\n  id: ID\n  title: String\n  releaseYear: Int\n}\n</code></pre></p>"},{"location":"federation/#extending-a-federated-type","title":"Extending a federated Type","text":"<p>To extend a type you redefine the type in your own schema, using directive <code>@extends</code> to instruct that it's a type extension. <code>@key</code> is required to indicate the field that the gateway will use to identify the original <code>Show</code> for a query. In this case, the key is the <code>id</code> field.</p> <p><pre><code>type Show @key(fields: \"id\") @extends {\n  id: ID @external\n  reviews: [Review]\n}\n\ntype Review {\n  starRating: Int\n}\n</code></pre> When redefining a type, only the id field, and the fields you're adding need to be listed. Other fields, such as <code>title</code> for <code>Show</code> type are provided by the Shows DGS and do not need to be specified unless you are using it in the schema. Federation makes sure the fields provided by all DGSs are combined into a single type for returning the results of a query.</p> <p>Info</p> <p>Don't forget to use the @external directive if you define a field that doesn't belong to your DGS, but you need to reference it.</p>"},{"location":"federation/#implementing-a-federated-type","title":"Implementing a Federated Type","text":"<p>The very first step to get started is to generate Java types that represent the schema. This is configured in <code>build.gradle</code> as described in the manual. When running <code>./gradlew build</code> the Java types are generated into the <code>build/generated</code> folder, which are then automatically added to the classpath.</p>"},{"location":"federation/#provide-an-entity-fetcher","title":"Provide an Entity Fetcher","text":"<p>Let's go through an example of the following query sent to the gateway: <pre><code>query {\n  shows {\n    title\n    reviews {\n      starRating\n    }\n  }\n}\n</code></pre></p> <p>The gateway first fetches the list of all the shows from the Shows DGS containing the title and id fields. <pre><code>query {\n  shows {\n    __typename\n    id\n    title\n  }\n}\n</code></pre></p> <p>Next, the gateway sends the following <code>_entities</code> query to the Reviews DGS using the list of <code>id</code>s from the first query: <pre><code>query($representations: [_Any!]!) {\n  _entities(representations: $representations) {\n    ... on Show {\n      reviews {\n        starRating\n      }\n    }\n  }\n}  \n</code></pre></p> <p>This query comes with the following variables: <pre><code>{\n\"representations\": [    {          \"__typename\": \"Show\",\n\"id\": 1\n},\n,\n{\n\"__typename\": \"Show\",\n\"id\": 2\n},\n{\n\"__typename\": \"Show\",\n\"id\": 3\n},\n{\n\"__typename\": \"Show\",\n\"id\": 4\n},\n{\n\"__typename\": \"Show\",\n\"id\": 5\n}\n]        } </code></pre></p> <p>The Reviews DGS needs to implement an <code>entity fetcher</code> to handle this query. An entity fetcher is responsible for creating an instance of a <code>Show</code> based on the representation in the <code>_entities</code> query above. The DGS framework does most of the heavy lifting, and all we have to do is provide the following:</p> <p>Full code <pre><code>@DgsEntityFetcher(name = \"Show\")\npublic Show movie(Map&lt;String, Object&gt; values) {\nreturn new Show((String) values.get(\"id\"), null);\n}\n</code></pre></p> <p>Tip</p> <p>Remember that the Show Java type here is generated by codegen. It's generated from the schema, so it only has the fields our schema specifies.</p> <p>Info</p> <p>Methods annotated using <code>@DgsEntityFetcher</code> are expected to return a concrete type (in this example: <code>Show</code>), <code>CompletionStage&lt;T&gt;</code> (e.g. <code>CompletableFuture&lt;T&gt;</code>), or Reactor <code>Mono&lt;T&gt;</code> instance.</p> <p>Instances of Reactor <code>Flux&lt;T&gt;</code> are not supported. When your scenario warrants returning a collection of concrete types, we suggest using <code>Flux#collectList</code>.</p>"},{"location":"federation/#providing-data-with-a-data-fetcher","title":"Providing Data with a Data Fetcher","text":"<p>Now the DGS knows how to create a Show instance when an <code>_entities</code> query is received, we can specify how to hydrate data for the reviews field.</p> <p>Full code <pre><code>@DgsData(parentType = \"Show\", field = \"reviews\")\npublic List&lt;Review&gt; reviews(DgsDataFetchingEnvironment dataFetchingEnvironment)  {\nShow show = dataFetchingEnvironment.getSource();\nreturn reviews.get(show.getId());\n}\n</code></pre></p>"},{"location":"federation/#testing-a-federated-query","title":"Testing a Federated Query","text":"<p>You can always manually test federated queries by running the gateway and your DGS locally.  You can also manually test a federated query against just your DGS, without the gateway, using the <code>_entities</code> query to replicate the call made to your DGS by the gateway.</p> <p>For automated tests, the QueryExecutor gives a way to run queries from unit tests, with very little startup overhead (in the order of 500ms). We can capture (or manually write) the <code>_entities</code> query that the gateway sends to the DGS. When running the query through the (locally running) gateway, the DGS will log the query that it receives. Simply copy this query in a <code>QueryExecutor</code> test, and that verifies the DGS in isolation.</p> <pre><code>@SpringBootTest(classes = {DgsAutoConfiguration.class, ReviewsDatafetcher.class})\nclass ReviewsDatafetcherTest {\n@Autowired\nDgsQueryExecutor dgsQueryExecutor;\n@Test\nvoid shows() {\nMap&lt;String,Object&gt; representation = new HashMap&lt;&gt;();\nrepresentation.put(\"__typename\", \"Show\");\nrepresentation.put(\"id\", \"1\");\nList&lt;Map&lt;String, Object&gt;&gt; representationsList = new ArrayList&lt;&gt;();\nrepresentationsList.add(representation);\nMap&lt;String, Object&gt; variables = new HashMap&lt;&gt;();\nvariables.put(\"representations\", representationsList);\nList&lt;Review&gt; reviewsList = dgsQueryExecutor.executeAndExtractJsonPathAsObject(\n\"query ($representations:[_Any!]!) {\" +\n\"_entities(representations:$representations) {\" +\n\"... on Show {\" +\n\"   reviews {\" +\n\"       starRating\" +\n\"}}}}\",\n\"data['_entities'][0].reviews\", variables, new TypeRef&lt;&gt;() {});\nassertThat(reviewsList)\n.isNotNull()\n.hasSize(3);\n}\n}\n</code></pre> <p>To help build the federated <code>_entities</code> query, you can also use the <code>EntitiesGraphQLQuery</code> available in <code>graphql-dgs-client</code> package along with code generation. Here is an example of the same test that uses the builder API:</p> <pre><code>@SpringBootTest(classes = {DgsAutoConfiguration.class, ReviewsDatafetcher.class})\nclass ReviewssDatafetcherTest {\n@Autowired\nDgsQueryExecutor dgsQueryExecutor;\n@Test\nvoid showsWithEntitiesQueryBuilder() {\nEntitiesGraphQLQuery entitiesQuery = new EntitiesGraphQLQuery.Builder().addRepresentationAsVariable(ShowRepresentation.newBuilder().id(\"1\").build()).build();\nGraphQLQueryRequest request = new GraphQLQueryRequest(entitiesQuery, new EntitiesProjectionRoot().onShow().reviews().starRating());\nList&lt;Review&gt; reviewsList = dgsQueryExecutor.executeAndExtractJsonPathAsObject(\nrequest.serialize(),\n\"data['_entities'][0].reviews\", entitiesQuery.getVariables(), new TypeRef&lt;&gt;() {\n});\nassertThat(reviewsList).isNotNull();\nassertThat(reviewsList.size()).isEqualTo(3);\n}\n}\n</code></pre> <p>For more details on the API and how to set it up for tests, please refer to our documentation here.</p>"},{"location":"federation/#customizing-the-default-federation-resolver","title":"Customizing the Default Federation Resolver","text":"<p>In the example above the GraphQL <code>Show</code> type name maps to the Java <code>Show</code> type. There are also cases where the GraphQL and Java type names don't match, specially when working with existing code. If any of your class names do not match your schema type names, you need to provide this class with a way to map between them. To do this, return a map from the <code>typeMapping()</code> method in your own implementation of the <code>DefaultDgsFederationResolver</code>. In the following example we map the GraphQL <code>Show</code> type to a <code>ShowId</code> Java type.</p> <pre><code>@DgsComponent\npublic class FederationResolver extends DefaultDgsFederationResolver {\nprivate final Map&lt;Class&lt;?&gt;, String&gt; types = new HashMap&lt;&gt;();\n@PostConstruct\npublic void init() {\n//The Show type is represented by the ShowId class.\ntypes.put(ShowId.class, \"Show\");\n}\n@Override\npublic Map&lt;Class&lt;?&gt;, String&gt; typeMapping() {\nreturn types;\n}\n}\n</code></pre>"},{"location":"generating-code-from-schema/","title":"Code Generation","text":"<p>The DGS Code Generation plugin generates code during your project\u2019s build process based on your Domain Graph Service\u2019s GraphQL schema file. The plugin generates the following:</p> <ul> <li>Data types for types, input types, enums and interfaces.</li> <li>A <code>DgsConstants</code> class containing the names of types and fields</li> <li>Example data fetchers</li> <li>A type safe query API that represents your queries</li> </ul>"},{"location":"generating-code-from-schema/#quick-start","title":"Quick Start","text":"<p>Code generation is typically integrated in the build. A Gradle plugin has always been available, and recently a Maven plugin was made available by the community.</p> <p>To apply the plugin, update your project\u2019s <code>build.gradle</code> file to include the following: <pre><code>// Using plugins DSL\nplugins {\nid \"com.netflix.dgs.codegen\" version \"[REPLACE_WITH_CODEGEN_PLUGIN_VERSION]\"\n}\n</code></pre></p> <p>Alternatively, you can set up classpath dependencies in your buildscript: <pre><code>buildscript {\ndependencies{\nclasspath 'com.netflix.graphql.dgs.codegen:graphql-dgs-codegen-gradle:[REPLACE_WITH_CODEGEN_PLUGIN_VERSION]'\n}\n}\napply plugin: 'com.netflix.dgs.codegen'\n</code></pre></p> <p>Next, you need to add the task configuration as shown here:</p> <pre><code>generateJava{\nschemaPaths = [\"${projectDir}/src/main/resources/schema\"] // List of directories containing schema files\npackageName = 'com.example.packagename' // The package name to use to generate sources\ngenerateClientv2 = true // Enable generating the type safe query API\n}\n</code></pre>   NOTE: Please use the latest version of the plugin, available here <p>The plugin adds a <code>generateJava</code> Gradle task that runs as part of your project\u2019s build. <code>generateJava</code> generates the code in the project\u2019s <code>build/generated</code> directory. Note that on a Kotlin project, the <code>generateJava</code> task generates Kotlin code by default (yes the name is confusing). This folder is automatically added to the project's classpath. Types are available as part of the package specified by the <code>packageName.types</code>, where you specify the value of packageName as a configuration in your <code>build.gradle</code> file. Please ensure that your project\u2019s sources refer to the generated code using the specified package name.</p> <p><code>generateJava</code> generates the data fetchers and places them in <code>build/generated-examples</code>.</p>   NOTE: generateJava does NOT add the data fetchers that it generates to your project\u2019s sources.  These fetchers serve mainly as a basic boilerplate code that require further implementation from you.  <p>You can exclude parts of the schema from code-generation by placing them in a different schema directory that is not specified as part of the <code>schemaPaths</code> for the plugin.</p>"},{"location":"generating-code-from-schema/#fixing-the-could-not-initialize-class-graphqlparserantlrgraphqllexer-problem","title":"Fixing the \"Could not initialize class graphql.parser.antlr.GraphqlLexer\" problem","text":"<p>Gradle's plugin system uses a flat classpath for all plugins, which makes it very easy to run into classpath conflicts. One of the dependencies of the Codegen plugin is ANTLR, which is unfortuanatly used by some other plugins as well. If you see an error such as <code>Could not initialize class graphql.parser.antlr.GraphqlLexer</code> this typically indicates a classpath conflict. If this happens, please change the ordering of the plugins in your build script. ANTLR is typically backwards, but not forwards, compatible.</p> <p>For multi-module projects means you need to declare the Codegen plugin in the root build file, without applying it:</p> <pre><code>plugins {\nid(\"com.netflix.dgs.codegen\") version \"[REPLACE_WITH_CODEGEN_PLUGIN_VERSION]\" apply false\n//other plugins\n}\n</code></pre> <p>In the module where the plugin should be applied, you specify the plugin in the plugins block again, but without the version.</p> <pre><code>plugins {\nid(\"com.netflix.dgs.codegen\")\n}\n</code></pre> <p>If you're using the old <code>buildscript</code> syntax, you add the plugin dependency to the root <code>buildscript</code>, but only <code>apply</code> in the module.</p>"},{"location":"generating-code-from-schema/#generating-code-from-external-schemas-in-jars","title":"Generating code from external schemas in JARs","text":"<p>You can also specify external dependencies containing schemas to use for generation by declaring it as a dependency in the <code>dgsCodegen</code> configuration. The plugin will scan all <code>.graphql</code> and <code>.graphqls</code> files and generate those classes under the same <code>build/generated</code> directory. This is useful if you have external dependencies containing some shared types that you want to add to your schema for code generation.  Not that this does NOT affect your project's schema, and is only for code generation.</p> <pre><code>dependencies {\n// other dependencies\ndgsCodegen 'com.netflix.graphql.dgs:example-schema:x.x.x'\n}\n</code></pre>"},{"location":"generating-code-from-schema/#mapping-existing-types","title":"Mapping existing types","text":"<p>Codegen tries to generate a type for each type it finds in the schema, with a few exceptions.</p> <ol> <li>Basic scalar types - are mapped to corresponding Java/Kotlin types (String, Integer etc.)</li> <li>Date and time types - are mapped to corresponding <code>java.time</code> classes</li> <li>PageInfo and RelayPageInfo - are mapped to <code>graphql.relay</code> classes</li> <li>Types mapped with a <code>typeMapping</code> configuration</li> </ol> <p>When you have existing classes that you want to use instead of generating a class for a certain type, you can configure the plugin to do so using a <code>typeMapping</code>. The <code>typeMapping</code> configuration is a <code>Map</code> where each key is a GraphQL type and each value is a fully qualified Java/Kotlin type.</p> <pre><code>generateJava{\ntypeMapping = [\"MyGraphQLType\": \"com.mypackage.MyJavaType\"]\n}\n</code></pre>"},{"location":"generating-code-from-schema/#generating-client-apis","title":"Generating Client APIs","text":"NOTE: There is a new API for generating client APIs. The old generateClient will be deprecated soon. See more here <p>The code generator can also create client API classes. You can use these classes to query data from a GraphQL endpoint using Java, or in unit tests using the <code>QueryExecutor</code>. The Java GraphQL Client is useful for server-to-server communication. A GraphQL Java Client is available as part of the framework.</p> <p>Code generation creates a <code>field-nameGraphQLQuery</code> for each Query and Mutation field. The <code>*GraphQLQuery</code> query class contains fields for each parameter of the field. For each type returned by a Query or Mutation, code generation creates a <code>*ProjectionRoot</code>. A projection is a builder class that specifies which fields get returned.</p> <p>The following is an example usage of a generated API:</p> <pre><code>GraphQLQueryRequest graphQLQueryRequest =\nnew GraphQLQueryRequest(\nnew TicksGraphQLQuery.Builder()\n.first(first)\n.after(after)\n.build(),\nnew TicksConnectionProjectionRoot()\n.edges()\n.node()\n.date()\n.route()\n.name()\n.votes()\n.starRating()\n.parent()\n.grade());\n</code></pre> <p>This API was generated based on the following schema. The <code>edges</code> and <code>node</code> types are because the schema uses pagination. The API allows for a fluent style of writing queries, with almost the same feel of writing the query as a String, but with the added benefit of code completion and type safety.</p> <pre><code>type Query @extends {\n    ticks(first: Int, after: Int, allowCached: Boolean): TicksConnection\n}\n\ntype Tick {\n    id: ID\n    route: Route\n    date: LocalDate\n    userStars: Int\n    userRating: String\n    leadStyle: LeadStyle\n    comments: String\n}\n\ntype Votes {\n    starRating: Float\n    nrOfVotes: Int\n}\n\ntype Route {\n    routeId: ID\n    name: String\n    grade: String\n    style: Style\n    pitches: Int\n    votes: Votes\n    location: [String]\n}\n\ntype TicksConnection {\n    edges: [TickEdge]\n}\n\ntype TickEdge {\n    cursor: String!\n    node: Tick\n}\n</code></pre>"},{"location":"generating-code-from-schema/#generateclientv2","title":"generateClientv2","text":"<p>There is a new API for generating client API. To turn on the new version, use the <code>generateClientv2</code> Gradle configuration option. Note that the v1 API will be deprecated soon. </p> <p>This new version relies on the use of generics and solves: 1) Not being able to handle cycles in the schema, and 2) Not being able to generate on larger schemas due to too many classes getting generated and out of memory errors. We only generate one class per type in the new implementation.</p> <p>The projection root needs to be instantiated differently for the v2 API.</p> <p>v1 API: <pre><code>String query = new GraphQLQueryRequest(\nnew MoviesGraphQLQuery(),\nnew MoviesProjectionRoot().movieId()).serialize();\n</code></pre></p> <p>v2 API: <pre><code>String query = new GraphQLQueryRequest(\nnew MoviesGraphQLQuery(),\nnew MoviesProjectionRoot&lt;&gt;().movieId()).serialize();\n</code></pre></p> <p>Kotlin Projects:  Kotlin does not support the Java diamond operator (&lt;&gt;) for inferring type arguments. Instead, pass in Nothing as both arguments.</p> <p>v2 API Kotlin: <pre><code>String query = new GraphQLQueryRequest(\nnew MoviesGraphQLQuery(),\nnew MoviesProjectionRoot&lt;Nothing, Nothing&gt;().movieId()).serialize();\n</code></pre></p>"},{"location":"generating-code-from-schema/#generating-query-apis-for-external-services","title":"Generating Query APIs for external services","text":"<p>Generating a Query API like above is very useful for testing your own DGS. The same type of API can also be useful when interacting with another GraphQL service, where your code is a client of that service. This is typically done using the DGS Client.</p> <p>When you use code generation both for your own schema, and an internal schema, you might want different code generation configuration for both. The recommendation is to create a separate module in your project containing the schema of the external service and the codegen configuration to just generate a Query API. The following is example configuration that only generates a Query API.</p> <pre><code>generateJava {\nschemaPaths = [\"${projectDir}/composed-schema.graphqls\"]\npackageName = \"some.other.service\"\ngenerateClientv2 = true\ngenerateDataTypes = false\nskipEntityQueries = true\nincludeQueries = [\"hello\"]\nincludeMutations = [\"\"]\nshortProjectionNames = true\nmaxProjectionDepth = 2\n}\n</code></pre>"},{"location":"generating-code-from-schema/#limiting-generated-code-for-client-api","title":"Limiting generated code for Client API","text":"<p>If your schema is large or has a lot of cycles, it is not ideal to generate client APIs for the entire schema, since you will end up with a large number of projections. This can cause code generation to slow down significantly, or run out of memory depending on your schema. We have a few configuration parameters that help tune this so you can limit the generation of client API to only what is required.</p> <p><pre><code>generateJava {\n...\ngenerateClientv2 = true\nskipEntityQueries = true\nincludeQueries = [\"hello\"]\nincludeMutations = [\"\"]\nincludeSubscriptions = [\"\"]\nmaxProjectionDepth = 2\n}\n</code></pre> Firstly, you can specify exactly which queries/mutation/subscriptions to generate for via <code>includeQueries</code>, <code>includeMutations</code>, and <code>includeSubscriptions</code>. <code>skipEntityQueries</code> is only used if you are constructing federated <code>_entities</code> queries for testing purposes, so you can also set that to restrict the amount of generated code. Finally, <code>maxProjectionDepth</code> will instruct codegen to stop generating beyond 2 levels of the graph from the query root. The default is 10. This will help further limit the number of projections as well.</p>"},{"location":"generating-code-from-schema/#generating-classes-with-custom-annotations","title":"Generating classes with Custom Annotations","text":"<p>This feature provides the ability to support any custom annotation on the generated POJOs using the @annotate directive in graphQL. The <code>@annotate</code> directive can be placed on type, input or fields in the graphQL. This feature is turned off by default and can be enabled by setting generateCustomAnnotation to true in build.gradle.</p> <p><pre><code>generateJava {\n...\ngenerateCustomAnnotations = true\n}\n</code></pre> @annotate contains 4 fields:</p> <ul> <li>name - Mandatory field. Name of the annotation. Eg: ValidPerson. You can have the package along with the annotation name. eg: <code>com.test.ValidPerson</code>. The package value given with the annotation name takes precedence over the mapped package in build.gradle.</li> <li>type - Optional field. This variable is used to map the annotation package in build.gradle. The package if given with annotation name will take precedence over this value. But if neither are given an empty string is used.</li> <li>inputs - Optional field. Contains the inputs to the annotation in key-value pairs. Eg: <code>inputs: {types: [HUSBAND, WIFE]}</code>. Inputs can be of types: String, int, float, enums, list, map, class, etc. For class inputs, refer to Example with Class Object </li> <li>target - Optional field. Refers to the site targets for the annotations. Refer to use target site doc for the target site available values.</li> </ul> <p>@annotate definition in the graphQL: <pre><code>\"Custom Annotation\"\ndirective @annotate(\n    name: String!\n    type: String\n    inputs: JSON\n    target: String\n) repeatable on OBJECT | FIELD_DEFINITION | INPUT_OBJECT | INPUT_FIELD_DEFINITION\n</code></pre> Custom annotations specified in the schema will require corresponding implementations by the resolvers to avoid runtime errors. Some examples: <pre><code>type Person @annotate(name: \"ValidPerson\", type: \"validator\", inputs: {types: [HUSBAND, WIFE]}) {\n       name: String @annotate(name: \"com.test.anotherValidator.ValidName\")\n       type: String @annotate(name: \"ValidType\", type: \"personType\", inputs: {types: [PRIMARY, SECONDARY]}) \n}\n</code></pre> The package mapping for the annotation and enums can be provided in the build.gradle file. <pre><code>generateJava {\n...\ngenerateCustomAnnotations = true\nincludeImports = [\"validator\": \"com.test.validator\"]\nincludeEnumImports = [\"ValidPerson\": [\"types\": \"com.enums\"]]\n}\n</code></pre> Generated POJO in Java. Please note that this feature is also available in Kotlin. <pre><code>package com.netflix.graphql.dgs.codegen.tests.generated.types;\n\nimport com.test.anotherValidator.ValidName;\nimport com.test.validator.ValidPerson;\nimport java.lang.Object;\nimport java.lang.Override;\nimport java.lang.String;\n\n@ValidPerson(\n    types = [com.enums.HUSBAND, com.enums.WIFE]\n)\npublic class Person {\n  @ValidName\n  private String name;\n\n  @ValidType(\n      types = [com.personType.enum.PRIMARY, com.personType.enum.SECONDARY]\n  )\n  private String type;\n\n  public Person() {\n  }\n\n  public Person(String name, String type) {\n    this.name = name;\n    this.type = type;\n  }\n\n  public String getName() {\n    return name;\n  }\n\n  public void setName(String name) {\n    this.name = name;\n  }\n\n  public String getType() {\n    return type;\n  }\n\n  public void setType(String type) {\n    this.type = type;\n  }\n\n  @Override\n  public String toString() {\n    return \"Person{\" + \"name='\" + name + \"',\" +\"type='\" + type + \"'\" +\"}\";\n  }\n\n  @Override\n  public boolean equals(Object o) {\n    if (this == o) return true;\n        if (o == null || getClass() != o.getClass()) return false;\n        Person that = (Person) o;\n        return java.util.Objects.equals(name, that.name) &amp;&amp;\n                            java.util.Objects.equals(type, that.type);\n  }\n\n  @Override\n  public int hashCode() {\n    return java.util.Objects.hash(name, type);\n  }\n}\n</code></pre></p> <p>Example with Class Object:</p> <p>Since GraphQL parser does not have built-in support for class objects, a class is represented as a string ending with \".class\" in the schema</p> <p><pre><code>type Person @annotate(name: \"ValidPerson\", type: \"validator\", inputs: {groups: \"BasicValidation.class\"}) {\n    name: String @annotate(name: \"com.test.anotherValidator.ValidName\")\n}\n</code></pre> The package mapping for the annotation and classes can be provided in the build.gradle file. If mapping is not provided, input will be treated as a string.</p> <p><pre><code>generateJava {\n...\ngenerateCustomAnnotations = true,\nincludeImports = mapOf(Pair(\"validator\", \"com.test.validator\")),\nincludeClassImports = mapOf(\"ValidPerson\" to mapOf(Pair(\"BasicValidation\", \"com.test.validator.groups\")))\n}\n</code></pre> Generated POJO in Java. Note: In Kotlin, using the same schema above will generate <code>BasicValidation::class</code> <pre><code>package com.netflix.graphql.dgs.codegen.tests.generated.types;\n\nimport com.test.anotherValidator.ValidName;\nimport com.test.validator.ValidPerson;\nimport com.test.validator.groups.BasicValidation;\nimport java.lang.Object;\nimport java.lang.Override;\nimport java.lang.String;\n\n@ValidPerson(\n    groups = BasicValidation.class\n)\npublic class Person {\n  @ValidName\n  private String name;\n\n  public Person() {\n  }\n\n  public Person(String name) {\n    this.name = name;\n  }\n\n  public String getName() {\n    return name;\n  }\n\n  public void setName(String name) {\n    this.name = name;\n  }\n\n  @Override\n  public String toString() {\n    return \"Person{\" + \"name='\" + name + \"'\" +\"}\";\n  }\n\n  @Override\n  public boolean equals(Object o) {\n    if (this == o) return true;\n        if (o == null || getClass() != o.getClass()) return false;\n        Person that = (Person) o;\n        return java.util.Objects.equals(name, that.name);\n  }\n\n  @Override\n  public int hashCode() {\n    return java.util.Objects.hash(name);\n  }\n}\n</code></pre></p> <p>Example with target site: <pre><code>type Person @deprecated(reason: \"This is going bye bye\") @annotate(name: \"ValidPerson\", type: \"validator\", inputs: {types: [HUSBAND, WIFE]}) {\n    name: String @annotate(name: \"com.test.anotherValidator.ValidName\", target: \"field\") @annotate(name: \"com.test.nullValidator.NullValue\")\n}\n</code></pre> Generated POJO in Java. <pre><code>package com.netflix.graphql.dgs.codegen.tests.generated.types;\n\nimport com.test.anotherValidator.ValidName;\nimport com.test.nullValidator.NullValue;\nimport com.test.validator.ValidPerson;\nimport java.lang.Deprecated;\nimport java.lang.Object;\nimport java.lang.Override;\nimport java.lang.String;\n\n/**\n * This is going bye bye\n */\n@Deprecated\n@ValidPerson(\n    types = [com.enums.HUSBAND, com.enums.WIFE]\n)\npublic class Person {\n  @ValidName\n  @NullValue\n  private String name;\n\n  public Person() {\n  }\n\n  public Person(String name) {\n    this.name = name;\n  }\n\n  public String getName() {\n    return name;\n  }\n\n  public void setName(String name) {\n    this.name = name;\n  }\n\n  @Override\n  public String toString() {\n    return \"Person{\" + \"name='\" + name + \"'\" +\"}\";\n  }\n\n  @Override\n  public boolean equals(Object o) {\n    if (this == o) return true;\n        if (o == null || getClass() != o.getClass()) return false;\n        Person that = (Person) o;\n        return java.util.Objects.equals(name, that.name);\n  }\n\n  @Override\n  public int hashCode() {\n    return java.util.Objects.hash(name);\n  }\n}\n</code></pre></p>"},{"location":"generating-code-from-schema/#configuring-code-generation","title":"Configuring code generation","text":"<p>Code generation has many configuration switches. The following table shows the Gradle configuration options, but the same options are available command line and in Maven as well.</p> Configuration property Description Default Value schemaPaths List of files/directories containing schemas src/main/resources/schema packageName Base package name of generated code subPackageNameClient Sub package name for generated Query API client subPackageNameDatafetchers Sub package name for generated data fetchers datafetchers subPackageNameTypes Sub package name for generated data types types language Either <code>java</code> or <code>kotlin</code> Autodetected from project typeMapping A Map where each key is a GraphQL type, and the value the FQN of a Java class generateBoxedTypes Always use boxed types for primitives false (boxed types are used only for nullable fields) generateClient Generate a Query API. This is version 1 of the API, which will be deprecated soon. false generateClientv2 Generate a Query API. This is version 2 of the API. false generateDataTypes Generate data types. Useful for only generating a Query API. Input types are still generated when <code>generateClientv2</code> is true. true generateInterfaces Generate interfaces for data classes. This is useful if you would like to extend the generated POJOs for more context and use interfaces instead of the data classes in your data fetchers. false generatedSourcesDir Build directory for Gradle build includeQueries Generate Query API only for the given list of Query fields All queries defined in schema includeMutations Generate Query API only for the given list of Mutation fields All mutations defined in schema includeSubscriptions Generate Query API only for the given list of Subscription fields All subscriptions defined in schema skipEntityQueries Disable generating Entity queries for federated types false shortProjectionNames Shorten class names of projection types. These types are not visible to the developer. false maxProjectionDepth Maximum projection depth to generate. Useful for (federated) schemas with very deep nesting 10 includeImports Maps the custom annotation type to the package, the annotations belong to. Only used when generateCustomAnnotations is enabled. includeEnumImports Maps the custom annotation and enum argument names to the enum packages. Only used when generateCustomAnnotations is enabled. includeClassImports Maps the custom annotation and class names to the class packages. Only used when generateCustomAnnotations is enabled. generateCustomAnnotations Enable/disable generation of custom annotation false"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#create-a-new-spring-boot-application","title":"Create a new Spring Boot application","text":"<p>The DGS framework is now based on Spring Boot 3.0, so get started by creating a new Spring Boot 3.0 application if you don't have one already. Note that you can still use the DGS framework with Spring Boot 2.7 by using a 5.5.x release. The 6.x release of the framework requires Spring Boot 3. The Spring Initializr is an easy way to do so. You can use either Gradle or Maven with Java 17 or Kotlin. We do recommend Gradle because we have a really cool code generation plugin for it!</p> <p>The only Spring dependency needed is Spring Web.</p> <p></p> <p>Open the project in an IDE (Intellij recommended).</p>"},{"location":"getting-started/#requirements","title":"Requirements","text":"<p>The latest 6.x release and onwards will require Spring Boot 3.0 for your project.  You will also need JDK 17. If your application is on Spring Boot 2.7, you will need to use the 5.5.x release train of the DGS framework. If your application is on Spring Boot 2.6, you will need to use 5.4.x or earlier.</p>"},{"location":"getting-started/#adding-the-dgs-framework-dependency","title":"Adding the DGS Framework Dependency","text":"<p>Add the platform dependencies to your Gradle or Maven configuration. The <code>com.netflix.graphql.dgs:graphql-dgs-platform-dependencies</code> dependency is a platform/BOM dependency, which aligns the versions of the individual modules and transitive dependencies of the framework. The <code>com.netflix.graphql.dgs:graphql-dgs-spring-boot-starter</code> is a Spring Boot starter that includes everything you need to get started building a DGS. If you're building on top of <code>WebFlux</code>, use <code>com.netflix.graphql.dgs:graphql-dgs-webflux-starter</code> instead.</p> GradleGradle KotlinMaven <pre><code>repositories {\nmavenCentral()\n}\ndependencies {\nimplementation(platform(\"com.netflix.graphql.dgs:graphql-dgs-platform-dependencies:latest.release\"))\nimplementation \"com.netflix.graphql.dgs:graphql-dgs-spring-boot-starter\"\n}\n</code></pre> <pre><code>repositories {\nmavenCentral()\n}\ndependencies {\nimplementation(platform(\"com.netflix.graphql.dgs:graphql-dgs-platform-dependencies:latest.release\"))\nimplementation(\"com.netflix.graphql.dgs:graphql-dgs-spring-boot-starter\")\n}\n</code></pre> <pre><code>&lt;dependencyManagement&gt;\n&lt;dependencies&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.netflix.graphql.dgs&lt;/groupId&gt;\n&lt;artifactId&gt;graphql-dgs-platform-dependencies&lt;/artifactId&gt;\n&lt;!-- The DGS BOM/platform dependency. This is the only place you set version of DGS --&gt;\n&lt;version&gt;4.9.16&lt;/version&gt;\n&lt;type&gt;pom&lt;/type&gt;\n&lt;scope&gt;import&lt;/scope&gt;\n&lt;/dependency&gt;\n&lt;/dependencies&gt;\n&lt;/dependencyManagement&gt;\n&lt;dependencies&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n&lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.netflix.graphql.dgs&lt;/groupId&gt;\n&lt;artifactId&gt;graphql-dgs-spring-boot-starter&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n&lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;\n&lt;scope&gt;test&lt;/scope&gt;\n&lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre> <p>Important</p> <p>The DGS framework uses Kotlin 1.5. If you use Spring Boot Gradle Plugin 2.3, you will have to be explicit on the Kotlin version that will be available. This plugin will downgrade the transitive 1.5 Kotlin version to 1.3. You can be explicit by setting it via Gradle's extensions as follows:</p> GradleGradle Kotlin <pre><code>ext['kotlin.version'] = '1.4.31'\n</code></pre> <pre><code>extra[\"kotlin.version\"] = \"1.4.31\"\n</code></pre>"},{"location":"getting-started/#creating-a-schema","title":"Creating a Schema","text":"<p>The DGS framework is designed for schema first development. The framework picks up any schema files in the <code>src/main/resources/schema</code> folder. Create a schema file in: <code>src/main/resources/schema/schema.graphqls</code>.</p> <pre><code>type Query {\n    shows(titleFilter: String): [Show]\n}\n\ntype Show {\n    title: String\n    releaseYear: Int\n}\n</code></pre> <p>This schema allows querying for a list of shows, optionally filtering by title.</p>"},{"location":"getting-started/#implement-a-data-fetcher","title":"Implement a Data Fetcher","text":"<p>Data fetchers are responsible for returning data for a query. Create two new classes <code>example.ShowsDataFetcher</code> and <code>Show</code> and add the following code. Note that we have a Codegen plugin that can do this automatically, but in this guide we'll manually write the classes.</p> JavaKotlin <pre><code>@DgsComponent\npublic class ShowsDatafetcher {\nprivate final List&lt;Show&gt; shows = List.of(\nnew Show(\"Stranger Things\", 2016),\nnew Show(\"Ozark\", 2017),\nnew Show(\"The Crown\", 2016),\nnew Show(\"Dead to Me\", 2019),\nnew Show(\"Orange is the New Black\", 2013)\n);\n@DgsQuery\npublic List&lt;Show&gt; shows(@InputArgument String titleFilter) {\nif(titleFilter == null) {\nreturn shows;\n}\nreturn shows.stream().filter(s -&gt; s.getTitle().contains(titleFilter)).collect(Collectors.toList());\n}\n}\npublic class Show {\nprivate final String title;\nprivate final Integer releaseYear;\npublic Show(String title, Integer releaseYear) {\nthis.title = title;\nthis.releaseYear = releaseYear;\n}\npublic String getTitle() {\nreturn title;\n}\npublic Integer getReleaseYear() {\nreturn releaseYear;\n}\n}\n</code></pre> <pre><code>@DgsComponent\nclass ShowsDataFetcher {\nprivate val shows = listOf(\nShow(\"Stranger Things\", 2016),\nShow(\"Ozark\", 2017),\nShow(\"The Crown\", 2016),\nShow(\"Dead to Me\", 2019),\nShow(\"Orange is the New Black\", 2013))\n@DgsQuery\nfun shows(@InputArgument titleFilter : String?): List&lt;Show&gt; {\nreturn if(titleFilter != null) {\nshows.filter { it.title.contains(titleFilter) }\n} else {\nshows\n}\n}\ndata class Show(val title: String, val releaseYear: Int)\n}\n</code></pre> <p>That's all the code needed, the application is ready to be tested!</p>"},{"location":"getting-started/#test-the-app-with-graphiql","title":"Test the app with GraphiQL","text":"<p>Start the application and open a browser to http://localhost:8080/graphiql. GraphiQL is a query editor that comes out of the box with the DGS framework. Write the following query and tests the result.</p> <pre><code>{\n    shows {\n        title\n        releaseYear\n    }\n}\n</code></pre> <p>Note that unlike with REST, you have to specifically list which fields you want to get returned from your query. This is where a lot of the power from GraphQL comes from, but a surprise to many developers new to GraphQL.</p> <p>The GraphiQL editor is really just a UI that uses the <code>/graphql</code> endpoint of your service. You could now connect a UI to your backend as well, for example using React and the Apollo Client.</p>"},{"location":"getting-started/#install-the-intellij-plugin","title":"Install the Intellij plugin","text":"<p>If you are an Intellij user, there is a plugin available for DGS. The plugin supports navigation between schema files and code and many hints and quick fixes. You can install the plugin from the Jetbrains plugin repository here.</p> <p></p>"},{"location":"getting-started/#next-steps","title":"Next steps","text":"<p>Now that you have a first GraphQL service running, we recommend improving this further by doing the following:</p> <ul> <li>Use the DGS Platform BOM to align DGS Framework dependencies.</li> <li>Learn more about datafetchers</li> <li>Use the Gradle CodeGen plugin - this will generate the data types for you.</li> <li>Write query tests in JUnit</li> <li>Look at example projects</li> </ul>"},{"location":"mutations/","title":"Mutations","text":"<p>The DGS framework supports Mutations with the same constructs as data fetchers, using the <code>@DgsData</code> annotation. The following is a simple example of a mutation:</p> <pre><code>type Mutation {\n    addRating(title: String, stars: Int):Rating\n}\n\ntype Rating {\n    avgStars: Float\n}\n</code></pre> <pre><code>@DgsComponent\npublic class RatingMutation {\n@DgsData(parentType = \"Mutation\", field = \"addRating\")\npublic Rating addRating(DataFetchingEnvironment dataFetchingEnvironment) {\nint stars = dataFetchingEnvironment.getArgument(\"stars\");\nif(stars &lt; 1) {\nthrow new IllegalArgumentException(\"Stars must be 1-5\");\n}\nString title = dataFetchingEnvironment.getArgument(\"title\");\nSystem.out.println(\"Rated \" + title + \" with \" + stars + \" stars\") ;\nreturn new Rating(stars);\n}\n}\n</code></pre> <p>Note that the code above retrieves the input data for the Mutation by calling the <code>DataFetchingEnvironment.getArgument</code> method, just as data fetchers do for their arguments.</p>"},{"location":"mutations/#input-types","title":"Input Types","text":"<p>In the example above the input was two standard scalar types. You can also use complex types, and you should define these as <code>input</code> types in your schema. An <code>input</code> type is almost the same as a <code>type</code> in GraphQL, but with some extra rules.</p> <p>According to the GraphQL specification an input type should always be passed to the data fetcher as a <code>Map</code>. This means the <code>DataFetchingEnvironment.getArgument</code> for an input type is a <code>Map</code>, and not the Java/Kotlin representation that you might have. The framework has a convenience mechanism around this, which will be discussed next. Let's first look at an example that uses DataFetchingEnvironment directly.</p> <pre><code>type Mutation {\n    addRating(input: RatingInput):Rating\n}\n\ninput RatingInput {\n    title: String,\n    stars: Int\n}\n\ntype Rating {\n    avgStars: Float\n}\n</code></pre> <pre><code>@DgsComponent\npublic class RatingMutation {\n@DgsData(parentType = \"Mutation\", field = \"addRating\")\npublic Rating addRating(DataFetchingEnvironment dataFetchingEnvironment) {\nMap&lt;String,Object&gt; input = dataFetchingEnvironment.getArgument(\"input\");\nRatingInput ratingInput = new ObjectMapper().convertValue(input, RatingInput.class);\nSystem.out.println(\"Rated \" + ratingInput.getTitle() + \" with \" + ratingInput.getStars() + \" stars\") ;\nreturn new Rating(ratingInput.getStars());\n}\n}\nclass RatingInput {\nprivate String title;\nprivate int stars;\npublic String getTitle() {\nreturn title;\n}\npublic void setTitle(String title) {\nthis.title = title;\n}\npublic int getStars() {\nreturn stars;\n}\npublic void setStars(int stars) {\nthis.stars = stars;\n}\n}\n</code></pre>"},{"location":"mutations/#input-arguments-as-data-fetcher-method-parameters","title":"Input arguments as data fetcher method parameters","text":"<p>The framework makes it easier to get input arguments. You can specify arguments as method parameters of a data fetcher.</p> <pre><code>@DgsComponent\npublic class RatingMutation {\n@DgsData(parentType = \"Mutation\", field = \"addRating\")\npublic Rating addRating(@InputArgument(\"input\") RatingInput ratingInput) {\n//No need for custom parsing anymore!\nSystem.out.println(\"Rated \" + ratingInput.getTitle() + \" with \" + ratingInput.getStars() + \" stars\") ;\nreturn new Rating(ratingInput.getStars());\n}\n}\n</code></pre> <p>The <code>@InputArgument</code> annotation is important to specify the name of the input argument, because arguments can be specified in any order. If no annotation is present, the framework tries to use the parameter name, but this is only possible if the code is compiled with specific compiler settings. Input type parameters can be combined with a <code>DataFetchingEnvironment</code> parameter.</p> <pre><code>@DgsComponent\npublic class RatingMutation {\n@DgsData(parentType = \"Mutation\", field = \"addRating\")\npublic Rating addRating(@InputArgument(\"input\") RatingInput ratingInput, DataFetchingEnvironment dfe) {\n//No need for custom parsing anymore!\nSystem.out.println(\"Rated \" + ratingInput.getTitle() + \" with \" + ratingInput.getStars() + \" stars\") ;\nSystem.out.println(\"DataFetchingEnvironment: \" + dfe.getArgument(ratingInput));\nreturn new Rating(ratingInput.getStars());\n}\n}\n</code></pre>"},{"location":"query-execution-testing/","title":"Testing","text":"<p>The DGS framework allows you to write lightweight tests that partially bootstrap the framework, just enough to run queries.</p>"},{"location":"query-execution-testing/#example","title":"Example","text":"<p>Before writing tests, make sure that JUnit is enabled. If you created a project with Spring Initializr this configuration should already be there.</p> GradleGradle KotlinMaven <pre><code>dependencies {\ntestImplementation 'org.springframework.boot:spring-boot-starter-test'\n}\ntest {\nuseJUnitPlatform()\n}\n</code></pre> <pre><code>tasks.withType&lt;Test&gt; {\nuseJUnitPlatform()\n}\n</code></pre> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n&lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;\n&lt;scope&gt;test&lt;/scope&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Create a test class with the following contents to test the <code>ShowsDatafetcher</code> from the getting started example.</p> JavaKotlin <pre><code>import com.netflix.graphql.dgs.DgsQueryExecutor;\nimport com.netflix.graphql.dgs.autoconfig.DgsAutoConfiguration;\nimport org.junit.jupiter.api.Test;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.boot.test.context.SpringBootTest;\nimport java.util.List;\nimport static org.assertj.core.api.Assertions.assertThat;\n@SpringBootTest(classes = {DgsAutoConfiguration.class, ShowsDatafetcher.class})\nclass ShowsDatafetcherTest {\n@Autowired\nDgsQueryExecutor dgsQueryExecutor;\n@Test\nvoid shows() {\nList&lt;String&gt; titles = dgsQueryExecutor.executeAndExtractJsonPath(\n\" { shows { title releaseYear }}\",\n\"data.shows[*].title\");\nassertThat(titles).contains(\"Ozark\");\n}\n}\n</code></pre> <pre><code>import com.netflix.graphql.dgs.DgsQueryExecutor\nimport com.netflix.graphql.dgs.autoconfig.DgsAutoConfiguration\nimport org.assertj.core.api.Assertions.assertThat\nimport org.junit.jupiter.api.Test\nimport org.springframework.beans.factory.annotation.Autowired\nimport org.springframework.boot.test.context.SpringBootTest\n@SpringBootTest(classes = [DgsAutoConfiguration::class, ShowsDataFetcher::class])\nclass ShowsDataFetcherTest {\n@Autowired\nlateinit var dgsQueryExecutor: DgsQueryExecutor\n@Test\nfun shows() {\nval titles : List&lt;String&gt; = dgsQueryExecutor.executeAndExtractJsonPath(\"\"\"\n            {\n                shows {\n                    title\n                    releaseYear\n                }\n            }\n        \"\"\".trimIndent(), \"data.shows[*].title\")\nassertThat(titles).contains(\"Ozark\")\n}\n}\n</code></pre> <p>The <code>@SpringBootTest</code> annotation makes this a Spring test. If you do not specify <code>classes</code> explicitly, Spring will start all components on the classpath. For a small application this is fine, but for applications with components that are \"expensive\" to start we can speed up the test by only adding the classes we need for the test. In this case we need to include the DGS framework itself using the <code>DgsAutoConfiguration</code> class, and the <code>ShowsDatafetcher</code>.</p> <p>To execute queries, inject <code>DgsQueryExecutor</code> in the test. This interface has several methods to execute a query and get back the result. It executes the exact same code as a query on the <code>/graphql</code> endpoint would, but you won\u2019t have to deal with HTTP in your tests. The <code>DgsQueryExecutor</code> methods accept JSON paths, so that the methods can easily extract just the data from the response that you\u2019re interested in. The <code>DgsQueryExecutor</code> also includes methods (e.g. <code>executeAndExtractJsonPathAsObject</code>) to deserialize the result to a Java class, which uses Jackson under the hood. The JSON paths are supported by the open source JsonPath library.</p> <p>Write a few more tests, for example to verify the behavior with using the <code>titleFilter</code> of <code>ShowsDatafetcher</code>. You can run the tests from the IDE, or from Gradle/Maven, just like any JUnit test.</p>"},{"location":"query-execution-testing/#building-graphql-queries-for-tests","title":"Building GraphQL Queries for Tests","text":"<p>In the examples shown previously, we handcrafted the query string. This is simple enough for queries that are small and straightforward. However, constructing longer query strings can be tedious, specially in Java without support for multi-line Strings. For this, we can use the GraphQLQueryRequest to build the graphql request in combination with the code generation plugin to generate the classes needed to use the request builder. This provides a convenient type-safe way to build your queries.</p> <p>To set up code generation to generate the required classes to use for building your queries, follow the instructions here.</p> <p>Now we can write a test that uses <code>GraphQLQueryRequest</code> to build the query and extract the response using <code>GraphQLResponse</code>.</p> JavaKotlin <pre><code>@Test\npublic void showsWithQueryApi() {\nGraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest(\nnew ShowsGraphQLQuery.Builder().titleFilter(\"Oz\").build(),\nnew ShowsProjectionRoot().title()\n);\nList&lt;String&gt; titles = dgsQueryExecutor.executeAndExtractJsonPath(graphQLQueryRequest.serialize(), \"data.shows[*].title\");\nassertThat(titles).containsExactly(\"Ozark\");\n}\n</code></pre> <pre><code>@Test\nfun showsWithQueryApi() {\nval graphQLQueryRequest = GraphQLQueryRequest(\nShowsGraphQLQuery.Builder()\n.titleFilter(\"Oz\")\n.build(),\nShowsProjectionRoot().title())\nval titles = dgsQueryExecutor.executeAndExtractJsonPath&lt;List&lt;String&gt;&gt;(graphQLQueryRequest.serialize(), \"data.shows[*].title\")\nassertThat(titles).containsExactly(\"Ozark\")\n}\n</code></pre> <p>The <code>GraphQLQueryRequest</code> is available as part of the graphql-client module and is used to build the query string, and wrap the response respectively. You can also refer to the GraphQLClient JavaDoc for more details on the list of supported methods.</p>"},{"location":"query-execution-testing/#mocking-external-service-calls-in-tests","title":"Mocking External Service Calls in Tests","text":"<p>It\u2019s not uncommon for a data fetcher to talk to external systems such as a database or a gRPC service. If it does so within a test, this adds two problems:</p> <ol> <li>It adds latency; your tests are going to run slower when they make a lot of external calls.</li> <li>It adds flakiness: Did your code introduce a bug, or did something go wrong in the external system?</li> </ol> <p>In many cases it\u2019s better to mock these external services. Spring already has good support for doing so with the @Mockbean annotation, which you can leverage in your DGS tests.</p>"},{"location":"query-execution-testing/#example_1","title":"Example","text":"<p>Let's update the <code>Shows</code> example to load shows from an external data source, instead of just returning a fixed list. For the sake of the example we'll just move the fixed list of shows to a new class that we'll annotate <code>@Service</code>. The data fetcher is updated to use the injected <code>ShowsService</code>.</p> JavaKotlin <pre><code>public interface ShowsService {\nList&lt;Show&gt; shows();\n}\n@Service\npublic class ShowsServiceImpl implements ShowsService {\n@Override\npublic List&lt;Show&gt; shows() {\nreturn List.of(\nnew Show(\"Stranger Things\", 2016),\nnew Show(\"Ozark\", 2017),\nnew Show(\"The Crown\", 2016),\nnew Show(\"Dead to Me\", 2019),\nnew Show(\"Orange is the New Black\", 2013)\n);\n}\n}\n</code></pre> <pre><code>interface ShowsService {\nfun shows(): List&lt;ShowsDataFetcher.Show&gt;\n}\n@Service\nclass BasicShowsService : ShowsService {\noverride fun shows(): List&lt;ShowsDataFetcher.Show&gt; {\nreturn listOf(\nShowsDataFetcher.Show(\"Stranger Things\", 2016),\nShowsDataFetcher.Show(\"Ozark\", 2017),\nShowsDataFetcher.Show(\"The Crown\", 2016),\nShowsDataFetcher.Show(\"Dead to Me\", 2019),\nShowsDataFetcher.Show(\"Orange is the New Black\", 2013)\n)\n}\n}\n@DgsComponent\nclass ShowsDataFetcher {\n@Autowired\nlateinit var showsService: ShowsService\n@DgsData(parentType = \"Query\", field = \"shows\")\nfun shows(@InputArgument(\"titleFilter\") titleFilter: String?): List&lt;Show&gt; {\nreturn if (titleFilter != null) {\nshowsService.shows().filter { it.title.contains(titleFilter) }\n} else {\nshowsService.shows()\n}\n}\n}\n</code></pre> <p>For the sake of the example the shows are still in-memory, imagine that the service would actually call out to an external data store. Let's try to mock this service in the test!</p> JavaKotlin <pre><code>@SpringBootTest(classes = {DgsAutoConfiguration.class, ShowsDataFetcher.class})\npublic class ShowsDataFetcherTests {\n@Autowired\nDgsQueryExecutor dgsQueryExecutor;\n@MockBean\nShowsService showsService;\n@BeforeEach\npublic void before() {\nMockito.when(showsService.shows()).thenAnswer(invocation -&gt; List.of(new Show(\"mock title\", 2020)));\n}\n@Test\npublic void showsWithQueryApi() {\nGraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest(\nnew ShowsGraphQLQuery.Builder().build(),\nnew ShowsProjectionRoot().title()\n);\nList&lt;String&gt; titles = dgsQueryExecutor.executeAndExtractJsonPath(graphQLQueryRequest.serialize(), \"data.shows[*].title\");\nassertThat(titles).containsExactly(\"mock title\");\n}\n}\n</code></pre> <pre><code>@SpringBootTest(classes = [DgsAutoConfiguration::class, ShowsDataFetcher::class])\nclass ShowsDataFetcherTest {\n@Autowired\nlateinit var\ndgsQueryExecutor:DgsQueryExecutor\n@MockBean\nlateinit var\nshowsService:ShowsService\n@BeforeEach\nfun before() {\nMockito.`when`(showsService.shows()).thenAnswer {\nlistOf(ShowsDataFetcher.Show(\"mock title\", 2020))\n}\n}\n@Test\nfun shows() {\nval titles :List&lt;String&gt; =dgsQueryExecutor.executeAndExtractJsonPath(\"\"\"\n                    {\n                        shows {\n                            title\n                            releaseYear\n                        }\n                    }\n                \"\"\".trimIndent(), \"data.shows[*].title\")\nassertThat(titles).contains(\"mock title\")\n}\n}\n</code></pre>"},{"location":"query-execution-testing/#testing-exceptions","title":"Testing Exceptions","text":"<p>The tests you wrote so far are mostly happy paths. Failure scenarios are also easy to test. We use the mocked example from above to force an exception.</p> JavaKotlin <pre><code>@Test\nvoid showsWithException() {\nMockito.when(showsService.shows()).thenThrow(new RuntimeException(\"nothing to see here\"));\nExecutionResult result = dgsQueryExecutor.execute(\" { shows { title releaseYear }}\");\nassertThat(result.getErrors()).isNotEmpty();\nassertThat(result.getErrors().get(0).getMessage()).isEqualTo(\"java.lang.RuntimeException: nothing to see here\");\n}\n</code></pre> <pre><code>@Test\nfun showsWithException() {\nMockito.`when`(showsService.shows()).thenThrow(RuntimeException(\"nothing to see here\"))\nval result = dgsQueryExecutor.execute(\"\"\"\n        {\n            shows {\n                title\n                releaseYear\n            }\n        }\n    \"\"\".trimIndent())\nassertThat(result.errors).isNotEmpty\nassertThat(result.errors[0].message).isEqualTo(\"java.lang.RuntimeException: nothing to see here\")\n}\n</code></pre> <p>When an error happens while executing the query, the errors are wrapped in a <code>QueryException</code>. This allows you to easily inspect the error. The <code>message</code> of the <code>QueryException</code> is the concatenation of all the errors. The <code>getErrors()</code> method gives access to the individual errors for further inspection.</p>"},{"location":"query-execution-testing/#testing-with-client","title":"Testing with Client","text":"<p>If you are interested in testing the web layer as well, you can use the Java GraphQL Client. Following is a simple example:</p> Java <pre><code>import com.netflix.graphql.dgs.client.GraphQLResponse;\nimport com.netflix.graphql.dgs.client.MonoGraphQLClient;\nimport org.junit.jupiter.api.Test;\nimport org.springframework.boot.test.context.SpringBootTest;\nimport org.springframework.boot.web.server.LocalServerPort;\nimport org.springframework.web.reactive.function.client.WebClient;\nimport java.util.List;\nimport static org.junit.jupiter.api.Assertions.assertTrue;\n@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT)\nclass ShowsDatafetcherTest {\nfinal MonoGraphQLClient monoGraphQLClient;\npublic ShowsDatafetcherTest(@LocalServerPort Integer port) {\nWebClient webClient = WebClient.create(\"http://localhost:\" + port.toString() + \"/graphql\");\nthis.monoGraphQLClient = MonoGraphQLClient.createWithWebClient(webClient);\n}\n@Test\nvoid shows() {\nString query = \"{ shows { title releaseYear }}\";\n// Read more about executeQuery() at https://netflix.github.io/dgs/advanced/java-client/\nGraphQLResponse response =\nmonoGraphQLClient.reactiveExecuteQuery(query).block();\nList&lt;?&gt; titles = response.extractValueAsObject(\"shows[*].title\", List.class);\nassertTrue(titles.contains(\"Ozark\"));\n}\n}\n</code></pre>"},{"location":"scalars/","title":"Adding Custom Scalars","text":"<p>It is easy to add a custom scalar type in the DGS framework: Create a class that implements the <code>graphql.schema.Coercing</code> interface and annotate it with the <code>@DgsScalar</code> annotation. Also make sure the scalar type is defined in your [GraphQL] schema!</p> <p>For example, this is a simple <code>LocalDateTime</code> implementation:</p> <pre><code>@DgsScalar(name=\"DateTime\")\npublic class DateTimeScalar implements Coercing&lt;LocalDateTime, String&gt; {\n@Override\npublic String serialize(Object dataFetcherResult) throws CoercingSerializeException {\nif (dataFetcherResult instanceof LocalDateTime) {\nreturn ((LocalDateTime) dataFetcherResult).format(DateTimeFormatter.ISO_DATE_TIME);\n} else {\nthrow new CoercingSerializeException(\"Not a valid DateTime\");\n}\n}\n@Override\npublic LocalDateTime parseValue(Object input) throws CoercingParseValueException {\nreturn LocalDateTime.parse(input.toString(), DateTimeFormatter.ISO_DATE_TIME);\n}\n@Override\npublic LocalDateTime parseLiteral(Object input) throws CoercingParseLiteralException {\nif (input instanceof StringValue) {\nreturn LocalDateTime.parse(((StringValue) input).getValue(), DateTimeFormatter.ISO_DATE_TIME);\n}\nthrow new CoercingParseLiteralException(\"Value is not a valid ISO date time\");\n}\n}\n</code></pre> <p>Schema: <pre><code>scalar DateTime\n</code></pre></p> <p>Important</p> <p>If you are Building GraphQL Queries for Tests, make sure to pass your custom scalars in <code>GraphQLQueryRequest</code> constructor as descibred in Scalars in DGS Client</p>"},{"location":"scalars/#registering-custom-scalars","title":"Registering Custom Scalars","text":"<p>In more recent versions of <code>graphql-java</code> (&gt;v15.0) some scalars, most notably the <code>Long</code> scalar, are no longer available by default. These are non-standard scalars that are difficult for clients (e.g. JavaScript) to handle reliably. As a result of the deprecation, you will need to add them explicitly, and to do this you have a few options.</p> <p>Tip</p> <p>Go to the graphql-java-extended-scalars project page to see the full list of scalars supported by this library. There you will also find examples of the scalars used in both schemas as well as example queries.</p>"},{"location":"scalars/#automatically-register-scalar-extensions-via-graphql-dgs-extended-scalars","title":"Automatically Register Scalar Extensions via graphql-dgs-extended-scalars","text":"<p>The DGS Framework, as of version 3.9.2, has the <code>graphql-dgs-extended-scalars</code> module. This module provides an auto-configuration that will register automatically the scalar extensions defined in the <code>com.graphql-java:graphql-java-extended-scalars</code> library. To use it you need to...</p> <ol> <li>Add the <code>com.netflix.graphql.dgs:graphql-dgs-extended-scalars</code> dependency to your build. If you are using the    DGS BOM you don't need to specify a version for it, the BOM will recommend one.</li> <li>Define the scalar in your schema</li> </ol> <p>Other mapping available on extended scalars doc</p> <p>The <code>graphql-java-extended-scalars</code> module offers a few knobs you can use to turn off registration.</p> Property Description dgs.graphql.extensions.scalars.time-dates.enabled If set to <code>false</code>, it will not register the DateTime, Date, and Time scalar extensions. dgs.graphql.extensions.scalars.objects.enabled If set to <code>false</code>, it will not register the Object, Json, Url, and Locale scalar extensions. dgs.graphql.extensions.scalars.numbers.enabled If set to <code>false</code>, it will not register all numeric scalar extensions such as PositiveInt, NegativeInt, etc. dgs.graphql.extensions.scalars.chars.enabled If set to <code>false</code>, it will not register the GraphQLChar extension. dgs.graphql.extensions.scalars.enabled If set to <code>false</code>, it will disable automatic registration of all of the above. <p>Important</p> <p>Are you using the code generation Gradle Plugin?</p> <p>The <code>graphql-java-extended-scalars</code>  module doesn't modify the behavior of such plugin, you will need to explicit define the type mappings. For example, let's say we want to use both the <code>Url</code> and <code>PositiveInt</code> Scalars. You will have to add the mapping below to your build file.</p> GradleGradle Kotlin <pre><code>generateJava {\ntypeMapping = [\n\"Url\" : \"java.net.URL\",\n\"PositiveInt\" : \"java.lang.Integer\"\n]\n}\n</code></pre> <pre><code>generateJava {\ntypeMapping = mutableMapOf(\n\"Url\" to \"java.net.URL\",\n\"PositiveInt\" to \"java.lang.Integer\"\n)\n}\n</code></pre>"},{"location":"scalars/#register-scalar-extensions-via-dgsruntimewiring","title":"Register Scalar Extensions via DgsRuntimeWiring","text":"<p>You can also register the Scalar Extensions manually. To do so you need to...</p> <ol> <li>Add the <code>com.graphql-java:graphql-java-extended-scalars</code> dependency to your build. If you are using the    DGS BOM you don't need to specify a version for it, the BOM will recommend one.</li> <li>Define the scalar in your schema</li> <li>Register the scalar.</li> </ol> <p>Here is an example of how you would set that up:</p> <p>Schema: <pre><code>scalar Long\n</code></pre> You can register the <code>Long</code> scalar manually with the DGS Framework as shown here: <pre><code>@DgsComponent\npublic class LongScalarRegistration {\n@DgsRuntimeWiring\npublic RuntimeWiring.Builder addScalar(RuntimeWiring.Builder builder) {\nreturn builder.scalar(Scalars.GraphQLLong);\n}\n}\n</code></pre></p>"},{"location":"videos/","title":"Videos","text":""},{"location":"videos/#dgs-framework-graphql-for-spring-boot-openvalue","title":"DGS Framework - GraphQL for Spring Boot @ OpenValue","text":""},{"location":"videos/#dgs-new-features-and-tips-march-2021","title":"DGS new features and tips - March 2021","text":""},{"location":"videos/#tech-tips-short-videos-about-a-specific-feature","title":"Tech tips (short videos about a specific feature)","text":""},{"location":"videos/#tech-tip-1-input-arguments","title":"Tech Tip #1 - Input Arguments","text":""},{"location":"videos/#tech-tip-2-code-generation","title":"Tech Tip #2 - Code generation","text":""},{"location":"videos/#tech-tip-3-building-federated-queries-for-tests-using-codegen","title":"Tech Tip #3 - Building federated queries for tests using codegen","text":""},{"location":"videos/#tech-tip-4-type-resolvers","title":"Tech Tip #4 - Type Resolvers","text":""},{"location":"advanced/context-passing/","title":"Nested data fetchers","text":"<p>Commonly, the datafetcher for a nested field requires properties from its parent object to load its data.</p> <p>Take the following schema example.</p> <pre><code>type Query {\n  shows: [Show]\n}\n\ntype Show {\n  # The showId may or may not be there, depending on the scenario.\n  showId: ID\n  title: String\n  reviews: [Review]\n}\n\ntype Review {\n  starRating: Int\n}\n</code></pre> <p>Let's assume our backend already has methods available to Shows and Reviews from a datastore.  Note that for this example, the <code>getShows</code> method does not return reviews. The <code>getReviewsForShow</code> method loads reviews for a show, given the show id.</p> <pre><code>interface ShowsService {\nList&lt;Show&gt; getShows(); //Does not include reviews\nList&lt;Review&gt; getReviewsForShow(int showId);   }\n</code></pre> <p>For this scenario, you likely want to have two datafetchers, one for shows and one for reviews. There are different options for implementing the datafetcher, which each has pros and cons depending on the scenario. We'll go over the different scenarios and options.</p>"},{"location":"advanced/context-passing/#the-easy-case-using-getsource","title":"The easy case - Using getSource","text":"<p>In the example schema the <code>Show</code> type has a <code>showId</code>. Having the showId available makes loading reviews in a separate datafetcher very easy. The <code>DataFetcherEnvironment</code> has a <code>getSource()</code> method that returns the parent loaded for a field.</p> <pre><code>@DgsData(parentType = \"Query\", field = \"shows\")\nList&lt;Show&gt; shows() {\nreturn showsService.getShows();\n}\n@DgsData(parentType = \"Show\", field = \"reviews\")\nList&lt;Review&gt; reviews(DgsDataFetchingEnvironment dfe) {\nShow show = dfe.getSource();\nreturn showsService.getReviewsForShow(show.getShowId());\n} </code></pre> <p>This example is the easiest and most common scenario, but only possible if the <code>showId</code> field is available on the <code>Show</code> type.</p>"},{"location":"advanced/context-passing/#no-showid-use-an-internal-type","title":"No showId - Use an internal type","text":"<p>Sometimes you don't want to expose the <code>showId</code> field in the schema, or our types are not set up to carry this field for other reasons. For example, for 1:1 and N:1 it's not that common to model the relationship as a key in the Java model. Whatever the reason is, the scenario we look at here is that we don't have the <code>showId</code> available on <code>Show</code>.</p> <p>If we remove <code>showId</code> from the schema and use codegen, the generated <code>Show</code> type will not have <code>showId</code> field either. Not having the <code>showId</code> field makes loading reviews a bit more complicated, because now we can't get the <code>showId</code> from the <code>Show</code> type using <code>getSource()</code>.</p> <p>The <code>getShowsForService(int showId)</code> method indicates that internally (probably in the datastore), a show does have an id. In such a scenario, we likely have a different internal representation of <code>Show</code> than exposed in the API. For the remainder of the example, we'll call this the <code>InternalShow</code> type which the <code>ShowsService</code> returns.</p> <pre><code>interface ShowsService {\nList&lt;InternalShow&gt; getShows(); //Does not include reviews\nList&lt;Review&gt; getReviewsForShow(int showId);   }\nclass InternalShow {\nint showId;\nSring title;\n// getters and setters\n}\n</code></pre> <p>However, the <code>Show</code> type in the GraphQL schema does not have a <code>showId</code>.</p> <pre><code>type Show {\n  title: String\n  reviews: [Review]\n}\n</code></pre> <p>The good news is that you can have fields set on your internal instances either not in the schema, or not queried. The framework drops this extra data while creating a response.</p> <p>We could create an extra <code>ShowWithId</code> wrapper class that either extends or composes the (generated) <code>Show</code> type, and adds a <code>showId</code> field.</p> <pre><code>class ShowWithId {\nString showId;\nShow show;\n//Delegate all show fields\nString getTitle() {\nreturn show.getTitle();\n}\nstatic ShowWithId fromInternalShow(InternalShow internal) {\n//Create Show instance and store id.\n}\n....\n}\n</code></pre> <p>The <code>shows</code> datafetcher should return the wrapper type instead of just <code>Show</code>.</p> <pre><code>@DgsData(parentType = \"Query\", field = \"shows\")\nList&lt;ShowWithId&gt; shows() {\nreturn showsService.getShows().stream()\n.map(ShowWithId::fromInternalShow)\n.collect(Collectors.toList());\n}\n</code></pre> <p>As said, the extra field doesn't affect the response to the client at all.</p>"},{"location":"advanced/context-passing/#no-showid-use-local-context","title":"No showId - Use local context","text":"<p>Using wrapper types works well when the schema type and internal type are mostly similar. An alternative way is to use \"local context\". A datafetcher can return a <code>DataFetcherResult&lt;T&gt;</code>, which contains <code>data</code>, <code>errors</code> and <code>localContext</code>. The <code>data</code> and <code>errors</code> fields are the data and errors you would normally return directly from your datafetcher. The <code>localContext</code> field can hold any data you want to pass down to child datafetchers. The <code>localContext</code> can be retrieved in the child datafetcher from the <code>DataFetchingEnvironment</code> and is passed down to the next level child datafetchers if not overwritten.</p> <p>In the following example the <code>shows</code> datafetcher creates a <code>DataFetcherResult</code> that holds the list of <code>Show</code> instances (not the internal type). The <code>localContext</code> is set to a map with each <code>show</code> as key, and the <code>showId</code> as value.</p> <pre><code>@DgsData(parentType = \"Query\", field = \"shows\")\npublic DataFetcherResult&lt;List&lt;Show&gt;&gt; shows(@InputArgument(\"titleFilter\") String titleFilter) {\nList&lt;InternalShow&gt; internalShows = getShows(titleFilter);\nList&lt;Show&gt; shows = internalShows.stream()\n.map(s -&gt; Show.newBuilder().title(s.getTitle()).build())\n.collect(Collectors.toList());\nreturn DataFetcherResult.&lt;List&lt;Show&gt;&gt;newResult()\n.data(shows)\n.localContext(internalShows.stream()\n.collect(Collectors.toMap(s -&gt; Show.newBuilder().title(s.getTitle()).build(), InternalShow::getId)))\n.build();\n}\nprivate List&lt;InternalShow&gt; getShows(String titleFilter) {\nif (titleFilter == null) {\nreturn showsService.shows();\n}\nreturn showsService.shows().stream().filter(s -&gt; s.getTitle().contains(titleFilter)).collect(Collectors.toList());\n}\n</code></pre> <p>The <code>reviews</code> datafetcher can now use a combination of the <code>getSource</code> and <code>getLocalContext</code> methods to get the <code>showId</code> for a show.</p> <pre><code>@DgsData(parentType = \"Show\", field = \"reviews\")\npublic CompletableFuture&lt;List&lt;Review&gt;&gt; reviews(DgsDataFetchingEnvironment dfe) {\nMap&lt;Show, Integer&gt; shows = dfe.getLocalContext();\nShow show = dfe.getSource();\nreturn showsService.getReviewsForShow(shows.get(show));\n}\n</code></pre> <p>A benefit of this approach is that in contrast with <code>getSource</code>, the <code>localContext</code> gets passed down to the next level of child datafechers as well.</p>"},{"location":"advanced/context-passing/#pre-loading","title":"Pre-loading","text":"<p>Suppose our internal datastore allows us to load shows and reviews together efficiently, for example using a SQL join query.  In that case, it can be more efficient to pre-load reviews in the <code>shows</code> datafetcher. In the <code>shows</code> datafetcher we can check if the <code>reviews</code> field was included in the query, and only if it is, load the reviews. Depending on the Java/Kotlin types we use, the <code>Show</code> type may or may not have a <code>reviews</code> field. If we use DGS codegen it will, and we can just set the <code>reviews</code> field when creating the <code>Show</code> instances in the <code>shows</code> datafetcher. If the type returned by the <code>shows</code> datafetcher does not have a <code>reviews</code> field, we can again use the <code>localContext</code> to pass on the review data to a <code>reviews</code> datafetcher. Below is an example of pre-loading and using <code>localContext</code>.</p> <pre><code>@DgsData(parentType = \"Query\", field = \"shows\")\npublic DataFetcherResult&lt;List&lt;Show&gt;&gt; shows(DataFetchingEnvironment dfe) {\nList&lt;Show&gt; shows = showsService.shows();\nif (dfe.getSelectionSet().contains(\"reviews\")) {\nMap&lt;Integer, List&lt;Review&gt;&gt; reviewsForShows = reviewsService.reviewsForShows(shows.stream().map(Show::getId).collect(Collectors.toList()));\nreturn DataFetcherResult.&lt;List&lt;Show&gt;&gt;newResult()\n.data(shows)\n.localContext(reviewsForShows)\n.build();\n} else {\nreturn DataFetcherResult.&lt;List&lt;Show&gt;&gt;newResult().data(shows).build();\n}\n}\n@DgsData(parentType = \"Show\", field = \"reviews\")\npublic List&lt;Review&gt; reviews(DgsDataFetchingEnvironment dfe) {\nShow show = dfe.getSource();\n//Load the reviews from the pre-loaded localContext.\nMap&lt;Integer, List&lt;Review&gt;&gt; reviewsForShows = dfe.getLocalContext();\nreturn reviewsForShows.get(show.getId());\n}\n</code></pre>"},{"location":"advanced/custom-datafetcher-context/","title":"Data Fetching Context","text":"<p>Each data fetcher in [GraphQL] Java has a context. A data fetcher gets access to its context by calling <code>DataFetchingEnvironment.getContext()</code>. This is a common mechanism to pass request context to data fetchers and data loaders. The DGS framework has its own <code>DgsContext</code> implementation, which is used for log instrumentation among other things. It is designed in such a way that you can extend it with your own custom context.</p> <p>To create a custom context, implement a Spring bean of type <code>DgsCustomContextBuilder</code>. Write the <code>build()</code> method so that it creates an instance of the type that represents your custom context object:</p> <pre><code>@Component\npublic class MyContextBuilder implements DgsCustomContextBuilder&lt;MyContext&gt; {\n@Override\npublic MyContext build() {\nreturn new MyContext();\n}\n}\npublic class MyContext {\nprivate final String customState = \"Custom state!\";\npublic String getCustomState() {\nreturn customState;\n}\n}\n</code></pre> <p>A data fetcher can now retrieve the context by calling the <code>getCustomContext()</code> method:</p> <pre><code>@DgsData(parentType = \"Query\", field = \"withContext\")\npublic String withContext(DataFetchingEnvironment dfe) {\nMyContext customContext = DgsContext.getCustomContext(dfe);\nreturn customContext.getCustomState();\n}\n</code></pre> <p>Similarly, custom context can be used in a DataLoader.</p> <pre><code>@DgsDataLoader(name = \"exampleLoaderWithContext\")\npublic class ExampleLoaderWithContext implements BatchLoaderWithContext&lt;String, String&gt; {\n@Override\npublic CompletionStage&lt;List&lt;String&gt;&gt; load(List&lt;String&gt; keys, BatchLoaderEnvironment environment) {\nMyContext context = DgsContext.getCustomContext(environment);\nreturn CompletableFuture.supplyAsync(() -&gt; keys.stream().map(key -&gt; context.getCustomState() + \" \" + key).collect(Collectors.toList()));\n}\n}\n</code></pre>"},{"location":"advanced/custom-object-mapper/","title":"Custom Object Mapper","text":"<p>In certain applications, you may want to customize the serialization of the GraphQL response, for e.g., by adding additional modules. This can be done by setting up a custom object mapper that overrides the default provided by the DGS framework in your application's configuration class. Note that this mapper is only used for serialization of outgoing GraphQL responses. Also, this mechanism will NOT affect how custom scalars are serialized - those would rely on your scalar implementation's serialization logic handled by <code>graphql-java</code></p> <p>To create a custom object mapper, implement a Spring bean of type <code>ObjectMapper</code> with <code>@Qualifier(\"dgsObjectMapper\")</code>.</p> <pre><code>  @Bean\n@Qualifier(\"dgsObjectMapper\")\npublic ObjectMapper dgsObjectMapper() {\nObjectMapper customMapper = new ObjectMapper()\ncustomMapper.registerModule(JavaTimeModule());\nreturn customMapper;\n}\n</code></pre>"},{"location":"advanced/dynamic-schemas/","title":"Dynamic schemas","text":"<p>We strongly recommend primarily using schema-first development. Most DGSs have a schema file and use the declarative, annotation-based programming model to create data fetchers and such. That said, there are scenarios where generating the schema from another source, possibly dynamically, is required.</p>"},{"location":"advanced/dynamic-schemas/#creating-a-schema-from-code","title":"Creating a schema from code","text":"<p>Create a schema from code by using the <code>@DgsTypeDefinitionRegistry</code> annotation. Use the <code>@DgsTypeDefinitionRegistry</code> on methods inside a <code>@DgsComponent</code> class to provide a <code>TypeDefinitionRegistry</code>.</p> <p>The <code>TypeDefinitionRegistry</code> is part of the graphql-java API. You use a <code>TypeDefinitionRegistry</code> to programmatically define a schema.</p> <p>Note that you can mix static schema files with one or more <code>DgsTypeDefinitionRegistry</code> methods. The result is a schema with all the registered types merged. This way, you can primarily use a schema-first workflow while falling back to <code>@DgsTypeDefinitionRegistry</code> to add some dynamic parts to the schema.</p> <p>The following is an example of a <code>DgsTypeDefinitionRegistry</code>.</p> <pre><code>@DgsComponent\npublic class DynamicTypeDefinitions {\n@DgsTypeDefinitionRegistry\npublic TypeDefinitionRegistry registry() {\nTypeDefinitionRegistry typeDefinitionRegistry = new TypeDefinitionRegistry();\nObjectTypeExtensionDefinition query = ObjectTypeExtensionDefinition.newObjectTypeExtensionDefinition().name(\"Query\").fieldDefinition(\nFieldDefinition.newFieldDefinition().name(\"randomNumber\").type(new TypeName(\"Int\")).build()\n).build();\ntypeDefinitionRegistry.add(query);\nreturn typeDefinitionRegistry;\n}\n}\n</code></pre> <p>This <code>TypeDefinitionRegistry</code> creates a field <code>randomNumber</code> on the <code>Query</code> object type.</p>"},{"location":"advanced/dynamic-schemas/#creating-datafetchers-programmatically","title":"Creating datafetchers programmatically","text":"<p>If you're creating schema elements dynamically, it's likely you also need to create datafetchers dynamically. You can use the <code>@DgsCodeRegistry</code> annotation to add datafetchers programmatically. A method annotated <code>@DgsCodeRegistry</code> takes two arguments:</p> <p>GraphQLCodeRegistry.Builder codeRegistryBuilder TypeDefinitionRegistry registry</p> <p>The method must return the modified GraphQLCodeRegistry.Builder.</p> <p>The following is an example of a programmatically created datafetcher for the field created in the previous example.</p> <pre><code>@DgsComponent\npublic class DynamicDataFetcher {\n@DgsCodeRegistry\npublic GraphQLCodeRegistry.Builder registry(GraphQLCodeRegistry.Builder codeRegistryBuilder, TypeDefinitionRegistry registry) {\nDataFetcher&lt;Integer&gt; df = (dfe) -&gt; new Random().nextInt();\nFieldCoordinates coordinates = FieldCoordinates.coordinates(\"Query\", \"randomNumber\");\nreturn codeRegistryBuilder.dataFetcher(coordinates, df);\n}\n}\n</code></pre>"},{"location":"advanced/dynamic-schemas/#changing-schemas-at-runtime","title":"Changing schemas at runtime","text":"<p>It's helpful to combine creating schemas/datafetchers at runtime with dynamically re-loading the schema in some very rare use-cases. You can achieve this by implementing your own <code>ReloadSchemaIndicator</code>.  You can use an external signal (e.g., reading from a message queue) to have the framework recreate the schema by executing the <code>@DgsTypeDefinitionRegistry</code> and <code>@DgsCodeRegistry</code> again.  If these methods create the schema based on external input, you have a system that can dynamically rewire its API.</p> <p>For obvious reasons, this isn't an approach that you should use for typical APIs; stable APIs are generally the thing to aim for!</p>"},{"location":"advanced/federated-testing/","title":"Federated Testing","text":"<p>Federation allows you to extend or reference existing types in a graph.  Your DGS fulfills a part of the query based on the schema that is owned by your DGS, while the gateway is responsible for fetching data from other DGSs. </p>"},{"location":"advanced/federated-testing/#testing-federated-queries-without-the-gateway","title":"Testing Federated Queries without the Gateway","text":"<p>You can test federated queries for your DGS in isolation by replicating the format of the query that the gateway would send to your DGS.  This does not involve the gateway, and thus the parts of the query response that your DGS is not responsible for will not be hydrated.  This technique is useful if you want to verify that your DGS is able to return the appropriate data, in response to a federated query. </p> <p>Let's look at an example of a schema that extends the <code>Movie</code> type that is already defined by another DGS. <pre><code>type Movie @key(fields: \"movieId\")  @extends {\n    movieId: Int @external\n    script: MovieScript\n}\n\ntype MovieScript  {\n    title: String\n    director: String\n    actors: [Actor]\n}\n\ntype Actor {\n    name: String\n    gender: String\n    age: Int\n}\n</code></pre> Now you want to verify that your DGS is able to fulfill the Movie query by hydrating the <code>script</code> field based on the <code>movieId</code> field.  Normally, the gateway would send an _entities query in the following format: <pre><code> query ($representations: [_Any!]!) {\n_entities(representations: $representations) {\n... on Movie {\nmovieId\nscript { title }\n}}}\n</code></pre> The <code>representations</code> input is a variable map containing the <code>__typename</code> field set to <code>Movie</code> and <code>movieId</code> set to a value, e.g., <code>12345</code>.</p> <p>You can now set up a Query Executor test by either manually constructing the query, or you can generate the federated query using the <code>Entities Query Builder API</code> available through client code generation.</p> <p>Here is an example of a test that uses a manually constructed <code>_entities</code> query for <code>Movie</code>: <pre><code>@Test\nvoid federatedMovieQuery() throws IOException {\nString query = \"query ($representations: [_Any!]!) {\" +\n\"_entities(representations: $representations) {\" +\n\"... on Movie {\" +\n\"movieId \" +\n\"script { title }\" +\n\"}}}\";\nMap&lt;String, Object&gt; variables = new HashMap&lt;&gt;();\nMap&lt;String,Object&gt; representation = new HashMap&lt;&gt;();\nrepresentation.put(\"__typename\", \"Movie\");\nrepresentation.put(\"movieId\", 1);\nvariables.put(\"representations\", List.of(representation));\nDocumentContext context = queryExecutor.executeAndGetDocumentContext(query, variables);\nGraphQLResponse response = new GraphQLResponse(context.jsonString());\nMovie movie = response.extractValueAsObject(\"data._entities[0]\", Movie.class);\nassertThat(movie.getScript().getTitle()).isEqualTo(\"Top Secret\");\n}\n</code></pre></p>"},{"location":"advanced/federated-testing/#using-the-entities-query-builder-api","title":"Using the Entities Query Builder API","text":"<p>Alternatively, you can generate the federated query by using EntitiesGraphQLQuery to build the graphql request in combination with the code generation plugin to generate the classes needed to use the request builder.  This provides a convenient type-safe way to build your queries.</p> <p>To set up code generation to generate the required classes to use for building your queries, follow the instructions here.</p> <p>You will also need to add <code>com.netflix.graphql.dgs:graphql-dgs-client:latest.release</code> dependency to build.gradle.  </p> <p>Now we can write a test that uses <code>EntitiesGraphQLQuery</code> along with <code>GraphQLQueryRequest</code> and <code>EntitiesProjectionRoot</code> to build the query. Finally, you can also extract the response using <code>GraphQLResponse</code>. </p> <p>This set up is shown here: <pre><code>@Test\nvoid federatedMovieQueryAPI() throws IOException {\n// constructs the _entities query with variable $representations containing a \n// movie representation that represents { __typename: \"Movie\"  movieId: 12345 }\nEntitiesGraphQLQuery entitiesQuery = new EntitiesGraphQLQuery.Builder()\n.addRepresentationAsVariable(\nMovieRepresentation.newBuilder().movieId(1122).build()\n)\n.build();\n// sets up the query and the field selection set using the EntitiesProjectionRoot\nGraphQLQueryRequest request = new GraphQLQueryRequest(\nentitiesQuery,\nnew EntitiesProjectionRoot().onMovie().movieId().script().title());\nString query  = request.serialize();\n// pass in the constructed _entities query with the variable map containing representations\nDocumentContext context = queryExecutor.executeAndGetDocumentContext(query, entitiesQuery.getVariables());\nGraphQLResponse response = new GraphQLResponse(context.jsonString());\nMovie movie = response.extractValueAsObject(\"data._entities[0]\", Movie.class);\nassertThat(movie.getScript().getTitle()).isEqualTo(\"Top Secret\");\n}\n</code></pre> Check out this video for a demo on how to configure and write the above test.</p> <p></p>"},{"location":"advanced/file-uploads/","title":"File Uploads","text":"<p>In GraphQL, you model a file upload operation as a GraphQL mutation request from a client to your DGS.</p> <p>The following sections describe how you implement file uploads and downloads using a Multipart POST request. For more context on file uploads and best practices, see Apollo Server File Upload Best Practices by Khalil Stemmler from Apollo Blog.</p>"},{"location":"advanced/file-uploads/#multipart-file-upload","title":"Multipart File Upload","text":"<p>A multipart request is an HTTP request that contains multiple parts in a single request: the mutation query, file data, JSON objects, and whatever else you like. You can use Apollo\u2019s upload client, or even a simple cURL, to send along a stream of file data using a multipart request that you model in your schema as a Mutation.</p> <p></p> <p>See GraphQL multipart request specification for the specification of a multipart <code>POST</code> request for uploading files using GraphQL mutations.</p> <p>The DGS framework supports the <code>Upload</code> scalar with which you can specify files in your mutation query as a <code>MultipartFile</code>. When you send a multipart request for file upload, the framework processes each part and assembles the final GraphQL query that it hands to your data fetcher for further processing.</p> <p>Here is an example of a Mutation query that uploads a file to your DGS:</p> <pre><code>scalar Upload\n\nextend type Mutation  {\n    uploadScriptWithMultipartPOST(input: Upload!): Boolean\n}\n</code></pre> <p>Note that you need to declare the <code>Upload</code> scalar in your schema, although the implementation is provided by the DGS framework. In your DGS, add a data fetcher to handle this as a <code>MultipartFile</code> as shown here:</p> <pre><code>@DgsData(parentType = DgsConstants.MUTATION.TYPE_NAME, field = \"uploadScriptWithMultipartPOST\")\npublic boolean uploadScript(DataFetchingEnvironment dfe) throws IOException {\n// NOTE: Cannot use @InputArgument  or Object Mapper to convert to class, because MultipartFile cannot be\n// deserialized\nMultipartFile file = dfe.getArgument(\"input\");\nString content = new String(file.getBytes());\nreturn ! content.isEmpty();\n}\n</code></pre> <p>Note that you will not be able to use a Jackson object mapper to deserialize a type that contains a <code>MultipartFile</code>, so you will need to explicitly get the file argument from your input.</p> <p>On your client, you can use <code>apollo-upload-client</code> to send your Mutation query as a multipart <code>POST</code> request with file data. Here\u2019s how you configure your link:</p> <pre><code>import { createUploadLink } from 'apollo-upload-client'\nconst uploadLink = createUploadLink({ uri: uri })\nconst authedClient = authLink &amp;&amp; new ApolloClient({\nlink: uploadLink)),\ncache: new InMemoryCache()\n})\n</code></pre> <p>Once you set this up, set up your Mutation query and the pass the file that the user selected as a variable:</p> <pre><code>// query for file uploads using multipart post\nconst UploadScriptMultipartMutation_gql = gql`\n  mutation uploadScriptWithMultipartPOST($input: Upload!) {\n    uploadScriptWithMultipartPOST(input: $input)\n  }\n`;\nfunction MultipartScriptUpload() {\nconst [\nuploadScriptMultipartMutation,\n{\nloading: mutationLoading,\nerror: mutationError,\ndata: mutationData,\n},\n] = useMutation(UploadScriptMultipartMutation_gql);\nconst [scriptMultipartInput, setScriptMultipartInput] = useState&lt;any&gt;();\nconst onSubmitScriptMultipart = () =&gt; {\nconst fileInput = scriptMultipartInput.files[0];\nuploadScriptMultipartMutation({\nvariables: { input: fileInput },\n});\n};\nreturn (\n&lt;div&gt;\n&lt;h3&gt; Upload script using multipart HTTP POST&lt;/h3&gt;\n&lt;form\nonSubmit={e =&gt; {\ne.preventDefault();\nonSubmitScriptMultipart();\n}}&gt;\n&lt;label&gt;\n&lt;input\ntype=\"file\"\nref={ref =&gt; {\nsetScriptMultipartInput(ref!);\n}}\n/&gt;\n&lt;/label&gt;\n&lt;br /&gt;\n&lt;br /&gt;\n&lt;button type=\"submit\"&gt;Submit&lt;/button&gt;\n&lt;/form&gt;\n&lt;/div&gt;\n);\n}\n</code></pre> <p>The <code>Upload</code> scalar is mapped to <code>MultipartFile</code> in the <code>build.gradle</code> file of the Codegen plugin. You can rename or remove the <code>Upload</code> scalar by modifying this <code>typeMapping</code>. Read more about the <code>typeMapping</code> configuration here. <pre><code>generateJava {\ntypeMapping = [Upload: \"org.springframework.web.multipart.MultipartFile\"]\n}\n</code></pre></p>"},{"location":"advanced/graphqlcontext-leveraging/","title":"Leveraging GraphQLContext","text":"<p>As of graphql-java version 17, GraphQLContext is now the approved context mechanism (replacing the previously used opaque Context) allowing passing of pertinent context via key/value pairs to provide frameworks and userspace code the capability to contribute, share and leverage various  pieces of context independent of each other.</p> <p>To make this easily leverageable by DGS customers, a new interface has been provided for which any Spring Beans registered  that implement the <code>GraphQLContextContributor</code>  interface, their <code>contribute</code> method will be invoked before any normal instrumentation classes are invoked, allowing  implementors to inspect anything provided via the DgsRequestData (i.e.: http headers), and set values on the provided  GraphQLContext via a provided GraphQLContext.Builder.</p>"},{"location":"advanced/graphqlcontext-leveraging/#example","title":"Example:","text":"<pre><code>@Component\npublic class ExampleGraphQLContextContributor implements GraphQLContextContributor {\n@Override\npublic void contribute(@NotNull GraphQLContext.Builder builder, @Nullable Map&lt;String, ?&gt; extensions, @Nullable DgsRequestData dgsRequestData) {\nif (dgsRequestData != null &amp;&amp; dgsRequestData.getHeaders() != null) {\nString contributedContextHeader = dgsRequestData.getHeaders().getFirst(\"x-context-contributor-header\");\nif (CONTEXT_CONTRIBUTOR_HEADER_VALUE.equals(\"enabled\")) {\nbuilder.put('exampleContributorEnabled', \"true\");\n}\n}\n}\n}\n</code></pre> <p>You can now also use the GraphQLContext with the data loader as of DGS Framework 6.0.0: <pre><code>@DgsDataLoader(name = \"exampleLoaderWithGraphQLContext\")\npublic class ExampleLoaderWithGraphQLContext implements BatchLoaderWithContext&lt;String, String&gt; {\n@Override\npublic CompletionStage&lt;List&lt;String&gt;&gt; load(List&lt;String&gt; keys, BatchLoaderEnvironment environment) {\nGraphQLContext graphQLContext = environment.getContext();\nreturn CompletableFuture.supplyAsync(() -&gt; keys.stream().map((Function&lt;String, String&gt;) graphQLContext::get).collect(Collectors.toList()));\n}\n}\n</code></pre></p>"},{"location":"advanced/instrumentation/","title":"Adding instrumentation for tracing and logging","text":"<p>It can be extremely valuable to add tracing, metrics and logging to your GraphQL API. At Netflix we publish tracing spans and metrics for each datafetcher to our distributed tracing/metrics backends, and log queries and query results to our logging backend. The implementations we use at Netflix are highly specific for our infrastructure, but it's easy to add your own to the framework!</p> <p>Internally the DGS framework uses GraphQL Java. GraphQL Java supports the concept of <code>instrumentation</code>. In the DGS framework we can easily add one or more instrumentation classes by implementing the <code>graphql.execution.instrumentation.Instrumentation</code> interface and register the class as <code>@Component</code>. The easiest way to implement the <code>Instrumentation</code> interface is to extend <code>graphql.execution.instrumentation.SimpleInstrumentation</code>.</p> <p>The following is an example of an implementation that outputs the execution time for each data fetcher, and the total query execution time, to the logs. Most likely you would want to replace the log output with writing to your tracing/metrics backend. Note that the code example accounts for async data fetchers. If we wouldn't do this, the result for an async data fetcher would always be 0, because the actual processing happens later.</p> JavaKotlin <pre><code>@Component\npublic class ExampleTracingInstrumentation extends SimpleInstrumentation {\nprivate final static Logger LOGGER = LoggerFactory.getLogger(ExampleTracingInstrumentation.class);\n@Override\npublic InstrumentationState createState() {\nreturn new TracingState();\n}\n@Override\npublic InstrumentationContext&lt;ExecutionResult&gt; beginExecution(InstrumentationExecutionParameters parameters) {\nTracingState tracingState = parameters.getInstrumentationState();\ntracingState.startTime = System.currentTimeMillis();\nreturn super.beginExecution(parameters);\n}\n@Override\npublic DataFetcher&lt;?&gt; instrumentDataFetcher(DataFetcher&lt;?&gt; dataFetcher, InstrumentationFieldFetchParameters parameters) {\n// We only care about user code\nif(parameters.isTrivialDataFetcher()) {\nreturn dataFetcher;\n}\nreturn environment -&gt; {\nlong startTime = System.currentTimeMillis();\nObject result = dataFetcher.get(environment);\nif(result instanceof CompletableFuture) {\n((CompletableFuture&lt;?&gt;) result).whenComplete((r, ex) -&gt; {\nlong totalTime = System.currentTimeMillis() - startTime;\nLOGGER.info(\"Async datafetcher {} took {}ms\", findDatafetcherTag(parameters), totalTime);\n});\n} else {\nlong totalTime = System.currentTimeMillis() - startTime;\nLOGGER.info(\"Datafetcher {} took {}ms\", findDatafetcherTag(parameters), totalTime);\n}\nreturn result;\n};\n}\n@Override\npublic CompletableFuture&lt;ExecutionResult&gt; instrumentExecutionResult(ExecutionResult executionResult, InstrumentationExecutionParameters parameters) {\nTracingState tracingState = parameters.getInstrumentationState();\nlong totalTime = System.currentTimeMillis() - tracingState.startTime;\nLOGGER.info(\"Total execution time: {}ms\", totalTime);\nreturn super.instrumentExecutionResult(executionResult, parameters);\n}\nprivate String findDatafetcherTag(InstrumentationFieldFetchParameters parameters) {\nGraphQLOutputType type = parameters.getExecutionStepInfo().getParent().getType();\nGraphQLObjectType parent;\nif (type instanceof GraphQLNonNull) {\nparent = (GraphQLObjectType) ((GraphQLNonNull) type).getWrappedType();\n} else {\nparent = (GraphQLObjectType) type;\n}\nreturn  parent.getName() + \".\" + parameters.getExecutionStepInfo().getPath().getSegmentName();\n}\nstatic class TracingState implements InstrumentationState {\nlong startTime;\n}\n}\n</code></pre> <pre><code>@Component\nclass ExampleTracingInstrumentation: SimpleInstrumentation() {\nval logger : Logger = LoggerFactory.getLogger(ExampleTracingInstrumentation::class.java)\noverride fun createState(): InstrumentationState {\nreturn TraceState()\n}\noverride fun beginExecution(parameters: InstrumentationExecutionParameters): InstrumentationContext&lt;ExecutionResult&gt; {\nval state: TraceState = parameters.getInstrumentationState()\nstate.traceStartTime = System.currentTimeMillis()\nreturn super.beginExecution(parameters)\n}\noverride fun instrumentDataFetcher(dataFetcher: DataFetcher&lt;*&gt;, parameters: InstrumentationFieldFetchParameters): DataFetcher&lt;*&gt; {\n// We only care about user code\nif(parameters.isTrivialDataFetcher) {\nreturn dataFetcher\n}\nval dataFetcherName = findDatafetcherTag(parameters)\nreturn DataFetcher { environment -&gt;\nval startTime = System.currentTimeMillis()\nval result = dataFetcher.get(environment)\nif(result is CompletableFuture&lt;*&gt;) {\nresult.whenComplete { _,_ -&gt;\nval totalTime = System.currentTimeMillis() - startTime\nlogger.info(\"Async datafetcher '$dataFetcherName' took ${totalTime}ms\")\n}\n} else {\nval totalTime = System.currentTimeMillis() - startTime\nlogger.info(\"Datafetcher '$dataFetcherName': ${totalTime}ms\")\n}\nresult\n}\n}\noverride fun instrumentExecutionResult(executionResult: ExecutionResult, parameters: InstrumentationExecutionParameters): CompletableFuture&lt;ExecutionResult&gt; {\nval state: TraceState = parameters.getInstrumentationState()\nval totalTime = System.currentTimeMillis() - state.traceStartTime\nlogger.info(\"Total execution time: ${totalTime}ms\")\nreturn super.instrumentExecutionResult(executionResult, parameters)\n}\nprivate fun findDatafetcherTag(parameters: InstrumentationFieldFetchParameters): String {\nval type = parameters.executionStepInfo.parent.type\nval parentType = if (type is GraphQLNonNull) {\ntype.wrappedType as GraphQLObjectType\n} else {\ntype as GraphQLObjectType\n}\nreturn \"${parentType.name}.${parameters.executionStepInfo.path.segmentName}\"\n}\ndata class TraceState(var traceStartTime: Long = 0): InstrumentationState\n}\n</code></pre> <p>Datafetcher 'Query.shows': 0ms</p> <p>Total execution time: 3ms</p>"},{"location":"advanced/instrumentation/#enabling-apollo-tracing","title":"Enabling Apollo Tracing","text":"<p>If you want to leverage Apollo Tracing, as supported by <code>graphql-java</code>, you can create a bean of type {@link TracingInstrumentation}.  In this example, we added a conditional property on the bean to enable/disable the Apollo Tracing. This property is enabled by default, but you can turn it off by setting <code>graphql.tracing.enabled=false</code> in your application properties.</p> <pre><code>import graphql.execution.instrumentation.tracing.TracingInstrumentation;\n@SpringBootApplication\npublic class ReviewsDgs {\n@Bean\n@ConditionalOnProperty( prefix = \"graphql.tracing\", name = \"enabled\", matchIfMissing = true)\npublic Instrumentation tracingInstrumentation(){\nreturn new TracingInstrumentation();\n}\n}\n</code></pre> <p>For federated tracing, you will need to use the instrumentation provided by Apollo's jvm federation library <pre><code>import com.apollographql.federation.graphqljava.tracing.FederatedTracingInstrumentation;\n@SpringBootApplication\npublic class ReviewsDgs {\n@Bean\n@ConditionalOnProperty( prefix = \"graphql.tracing\", name = \"enabled\", matchIfMissing = true)\npublic Instrumentation tracingInstrumentation(){\nreturn new FederatedTracingInstrumentation();\n}\n}\n</code></pre></p> <p>It's important to note that the default behavior in Apollo's JVM federation library is to trace requests, even if the federated gateway does not request it (i.e. the gateway does not add <code>FEDERATED_TRACING_HEADER_NAME</code> when forwarding the request). Including the following component in your DGS project will explicitly ask Apollo's JVM federation library to not trace a request in the event that the gateway does not request it.</p> <pre><code>@Component\npublic class ApolloFederatedTracingHeaderForwarder implements GraphQLContextContributor {\n@Override\npublic void contribute(@NotNull GraphQLContext.Builder builder, @Nullable Map&lt;String, ?&gt; extensions, @Nullable DgsRequestData dgsRequestData) {\nif (dgsRequestData == null || dgsRequestData.getHeaders() == null) {\nreturn;\n}\nfinal HttpHeaders headers = dgsRequestData.getHeaders();\n// if the header exists, we should just forward it.\nif (headers.containsKey(FederatedTracingInstrumentation.FEDERATED_TRACING_HEADER_NAME)) {\nbuilder.put(\nFederatedTracingInstrumentation.FEDERATED_TRACING_HEADER_NAME,\nheaders\n.get(FederatedTracingInstrumentation.FEDERATED_TRACING_HEADER_NAME)\n.stream()\n.findFirst()\n.get()\n);\n}  else {\n//otherwise, place a value != \"ftv1\" so when it gets checked for == ftv1 it fails\n// and trace does not happen.\nbuilder.put(FederatedTracingInstrumentation.FEDERATED_TRACING_HEADER_NAME, \"DO_NOT_TRACE\");\n}\n}\n}\n</code></pre>"},{"location":"advanced/instrumentation/#metrics-out-of-the-box","title":"Metrics Out of The Box","text":"<p>tl;dr</p> <ul> <li>Supported via the opt-in <code>graphql-dgs-spring-boot-micrometer</code> module.</li> <li>Provides specific GraphQL metrics such as <code>gql.query</code>, <code>gql.error</code>, and <code>gql.dataLoader</code>.</li> <li>Supports several backend implementations since it's implemented via Micrometer.</li> </ul> Gradle GroovyGradle KotlinMaven <pre><code>dependencies {\nimplementation 'com.netflix.graphql.dgs:graphql-dgs-spring-boot-micrometer'\n}\n</code></pre> <pre><code>dependencies {\nimplementation(\"com.netflix.graphql.dgs:graphql-dgs-spring-boot-micrometer\")\n}\n</code></pre> <pre><code>&lt;dependencies&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.netflix.graphql.dgs&lt;/groupId&gt;\n&lt;artifactId&gt;graphql-dgs-spring-boot-micrometer&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre> <p>Hint</p> <p>Note that the version is missing since we assume you are using the latest BOM. We recommend you use the DGS Platform BOM to handle such versions.</p>"},{"location":"advanced/instrumentation/#shared-tags","title":"Shared Tags","text":"<p>The following are tags shared across most of the meters.</p> <p>Tags:</p> tag name values description <code>gql.operation</code> QUERY, MUTATION, SUBSCRIPTION are the possible values. These represent the GraphQL operation that is executed. <code>gql.operation.name</code> GraphQL operation name if any, else <code>anonymous</code>. Since the cardinality of the value is high it will be limited. <code>gql.query.complexity</code> one in [5, 10, 20, 50, 100, 200, 500, 1000] The total number of nodes in the query. Refer to Query Complexity section. for additional information. <code>gql.query.sig.hash</code> Query Signature Hash of the query that was executed. Since the cardinality of the value is high it will be limited."},{"location":"advanced/instrumentation/#graphql-query-complexity","title":"GraphQL Query Complexity","text":"<p>The <code>gql.query.complexity</code> is typically calculated as 1 + Child's Complexity. The query complexity is valuable to calculate the cost of a query as this can vary based on input arguments to the query. The computed value is represented as one of the bucketed values to reduce the cardinality of the metric.</p> <p>Example Query:</p> <pre><code>query {\n  viewer {\n    repositories(first: 50) {\n      edges {\n        repository:node {\n          name\n\n          issues(first: 10) {\n            totalCount\n            edges {\n              node {\n                title\n                bodyHTML\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Example Calculation:</p> <pre><code>50          = 50 repositories\n+\n50 x 10     = 500 repository issues\n\n            = 550 total nodes\n</code></pre>"},{"location":"advanced/instrumentation/#graphql-query-signature-hash","title":"GraphQL Query Signature Hash","text":"<p>The Query Signature is defined as the tuple of the GraphQL AST Signature of the GraphQL Document and the GraphQL AST Signature Hash. The GraphQL AST Signature of a GraphQL Document is defined as follows:</p> <p>A canonical AST which removes excess operations, removes any field aliases, hides literal values and sorts the result into a canonical query Ref graphql-java</p> <p>The GraphQL AST Signature Hash is the Hex 256 SHA string produced by encoding the AST Signature. While we can't tag a metric by its signature, due its length, we can use the hash, as now expressed by the <code>gql.query.sig.hash</code> tag.</p> <p>There are a few configuration parameters that can change the behavior of the <code>gql.query.sig.hash</code> tag.</p> <ul> <li><code>management.metrics.dgs-graphql.query-signature.enabled</code>:    Defaulting to <code>true</code>, it enables the calculation of the  GQL Query Signature. The <code>gql.query.sig.hash</code> will express the GQL Query Signature Hash.</li> <li><code>management.metrics.dgs-graphql.query-signature.caching.enabled</code>:    Defaulting to <code>true</code>, it will cache the GQL Query Signature. If set to <code>false</code> it will just disable the cache but will    not turn the calculation of the signature off. If you want to turn such calculation off use the    <code>management.metrics.dgs-graphql.query-signature.enabled</code> property.</li> </ul>"},{"location":"advanced/instrumentation/#cardinality-limiter","title":"Cardinality Limiter","text":"<p>The cardinality of a given tag, the number of different values that a tag can express, can be problematic to servers supporting metrics. In order to prevent the cardinality of some of the tags supported out of the box there are some limiters by default. The limited tag values will only see the first 100 different values by default, from there new values will be expressed as <code>--others--</code>.</p> <p>You can change the limiter via the following configuration:</p> <ul> <li><code>management.metrics.dgs-graphql.tags.limiter.limit</code>: Defaults to<code>100</code>, sets the number of different values expressed per limited tag.</li> </ul> <p>Not all tags are limited, currently, only following are:</p> <ul> <li><code>gql.operation.name</code></li> <li><code>gql.query.sig.hash</code></li> </ul>"},{"location":"advanced/instrumentation/#query-timer-gqlquery","title":"Query Timer: gql.query","text":"<p>Captures the elapsed time that a given GraphQL query, or mutation, takes.</p> <p>Name: <code>gql.query</code></p> <p>Tags:</p> tag name values description <code>outcome</code> <code>success</code> or <code>failure</code> Result of the operation, as defined by the ExecutionResult."},{"location":"advanced/instrumentation/#error-counter-gqlerror","title":"Error Counter: gql.error","text":"<p>Captures the number of GraphQL errors encountered during query execution of a query or mutation. Remember that one graphql request can have multiple errors.</p> <p>Name: <code>gql.error</code></p> <p>Tags:</p> tag name description <code>gql.errorCode</code> The GraphQL error code, such as <code>VALIDATION</code>, <code>INTERNAL</code>, etc. <code>gql.path</code> The sanitized query path that resulted in the error. <code>gql.errorDetail</code> Optional flag containing additional details, if present."},{"location":"advanced/instrumentation/#data-loader-timer-gqldataloader","title":"Data Loader Timer: gql.dataLoader","text":"<p>Captures the elapsed time for a data loader invocation for a batch of queries. This is useful if you want to find data loaders that might be responsible for poor query performance.</p> <p>Name: <code>gql.dataLoader</code></p> <p>Tags:</p> tag name description <code>gql.loaderName</code> The name of the data loader, may or may not be the same as the type of entity. <code>gql.loaderBatchSize</code> The number of queries executed in the batch."},{"location":"advanced/instrumentation/#data-fetcher-timer-gqlresolver","title":"Data Fetcher Timer: gql.resolver","text":"<p>Captures the elapsed time of each data fetcher invocation. This is useful if you want to find data fetchers that might be responsible for poor query performance. That said, there might be times where you want to remove a data fetcher from being measured/included in this meter. You can do so by annotating the method with <code>@DgsEnableDataFetcherInstrumentation(false)</code>.</p> <p>Info</p> <p>This metric is not available if:</p> <ul> <li>The data is resolved via a Batch Loader.</li> <li>The method is annotated with <code>@DgsEnableDataFetcherInstrumentation(false)</code>.</li> <li>The DataFetcher is TrivialDataFetcher. A trivial DataFetcher is one that simply maps data from an object to a field.   This is defined directly in <code>graphql-java</code>.</li> </ul> <p>Name: <code>gql.resolver</code></p> <p>Tags:</p> tag name description <code>gql.field</code> Name of the data fetcher. This has the <code>${parentType}.${field}</code> format as specified in the <code>@DgsData</code> annotation."},{"location":"advanced/instrumentation/#data-fetcher-timer-as-a-counter","title":"Data Fetcher Timer as a Counter","text":"<p>The data fetcher, or resolver, timer can be used as a counter as well. If used in this manner it will reflect the number of invocations of each data fetcher. This is useful if you want to find out which data fetchers are used often.</p>"},{"location":"advanced/instrumentation/#further-tag-customization","title":"Further Tag Customization","text":"<p>You can customize the tags applied to the metrics above by providing beans that implement the following functional interfaces.</p> Interface Description <code>DgsContextualTagCustomizer</code> Used to add common, contextual tags. Example of these could be used to describe the deployment environment, application profile, application version, etc <code>DgsExecutionTagCustomizer</code> Used to add tags specific to the ExecutionResult of the query. The SimpleGqlOutcomeTagCustomizer is an example of this. <code>DgsFieldFetchTagCustomizer</code> Used to add tags specific to the execution of data fetchers. The SimpleGqlOutcomeTagCustomizer is an example of this as well."},{"location":"advanced/instrumentation/#additional-metrics-configuration","title":"Additional Metrics Configuration","text":"<ul> <li><code>management.metrics.dgs-graphql.enabled</code>: Enables the metrics provided out of the box; defaults to <code>true</code>.</li> <li><code>management.metrics.dgs-graphql.tag-customizers.outcome.enabled</code>: Enables the tag customizer that will label the <code>gql.query</code> and <code>gql.resolver</code> timers with an <code>outcome</code> reflecting    the result of the GraphQL outcome, either <code>success</code> or <code>failure</code>; defaults to <code>true</code>.</li> <li><code>management.metrics.dgs-graphql.data-loader-instrumentation.enabled</code>: Enables instrumentation of data loaders; defaults to <code>true</code>.</li> </ul>"},{"location":"advanced/java-client/","title":"Java GraphQL Client","text":""},{"location":"advanced/java-client/#usage","title":"Usage","text":"<p>The DGS framework provides a GraphQL client that can be used to retrieve data from a GraphQL endpoint. The client is also useful for integration testing a DGS. The client has two components, each usable by itself, or in combination together.</p> <ul> <li>GraphQL Client - A HTTP client wrapper that provides easy parsing of GraphQL responses</li> <li>Query API codegen - Generate type-safe Query builders</li> </ul> <p>The client has multiple interfaces and implementations for different needs. The interfaces are the following:</p> <ul> <li>GraphQLClient - Client interface for blocking implementations. Only recommended when using a blocking HTTP client.</li> <li>MonoGraphQLClient - The same as GraphQLClient, but based on the reactive <code>Mono</code> interface meant for non-blocking implementations. Comes with an out-of-the-box WebClient implementation: <code>MonoGraphQLClient.createWithWebClient(...)</code>.</li> <li>ReactiveGraphQLClient - Client interface for streaming responses such as Subscriptions, where multiple results are expected. Based on the reactive <code>Flux</code> interface. Implemented by <code>SSESubscriptionGraphQLClient</code> and <code>WebSocketGraphQLClient</code>.</li> </ul>"},{"location":"advanced/java-client/#graphql-client-with-webclient","title":"GraphQL Client with WebClient","text":"<p>The easiest way to use the DGS GraphQL Client is to use the WebClient implementation. WebClient is the recommended HTTP client in Spring, and is the best choice for most use cases, unless you have a specific reason to use a different HTTP client. Because WebClient is Reactive, the client returns a <code>Mono</code> for all operations.</p> Java <pre><code>    //Configure a WebClient for your needs, e.g. including authentication headers and TLS.\nWebClient webClient = WebClient.create(\"http://localhost:8080/graphql\");\nWebClientGraphQLClient client = MonoGraphQLClient.createWithWebClient(webClient);\n//The GraphQLResponse contains data and errors.\nMono&lt;GraphQLResponse&gt; graphQLResponseMono = graphQLClient.reactiveExecuteQuery(query);\n//GraphQLResponse has convenience methods to extract fields using JsonPath.\nMono&lt;String&gt; somefield = graphQLResponseMono.map(r -&gt; r.extractValue(\"somefield\"));\n//Don't forget to subscribe! The request won't be executed otherwise.\nsomefield.subscribe();\n</code></pre> Kotlin <pre><code>    //Configure a WebClient for your needs, e.g. including authentication headers and TLS.\nval client = MonoGraphQLClient.createWithWebClient(WebClient.create(\"http://localhost:8080/graphql\"))\n//Executing the query returns a Mono of GraphQLResponse.\nval result = client.reactiveExecuteQuery(\"{hello}\").map { r -&gt; r.extractValue&lt;String&gt;(\"hello\") }\n//Don't forget to subscribe! The request won't be executed otherwise.\nsomefield.subscribe();\n</code></pre> <p>The <code>reactiveExecuteQuery</code> method takes a query String as input, and optionally a Map of variables and an operation name. Instead of using a query String, you can use code generation to create a type-safe query builder API.</p> <p>The <code>GraphQLResponse</code> provides methods to parse and retrieve data and errors in a variety of ways. Refer to the <code>GraphQLResponse</code> JavaDoc for the complete list of supported methods.</p> method description example getData Get the data as a Map <code>Map&lt;String,Object&gt; data = response.getData()</code> dataAsObject Parse data as the provided class, using the Jackson Object Mapper <code>TickResponse data = response.dataAsObject(TicksResponse.class)</code> extractValue Extract values given a JsonPath. The return type will be whatever type you expect, but depends on the JSON shape. For JSON objects, a Map is returned. Although this looks type safe, it really isn't. It's mostly useful for \"simple\" types like String, Int etc., and Lists of those types. <code>List&lt;String&gt; name = response.extractValue(\"movies[*].originalTitle\")</code> extractValueAsObject Extract values given a JsonPath and deserialize into the given class <code>Ticks ticks = response.extractValueAsObject(\"ticks\", Ticks.class)</code> extractValueAsObject Extract values given a JsonPath and deserialize into the given TypeRef. Useful for Maps and Lists of a certain class. <code>List&lt;Route&gt; routes = response.extractValueAsObject(\"ticks.edges[*].node.route\", new TypeRef&lt;List&lt;Route&gt;&gt;(){})</code> getRequestDetails Extract a <code>RequestDetails</code> object. This only works if requestDetails was requested in the query, and against the Gateway. RequestDetails requestDetails = <code>response.getRequestDetails()</code> getParsed Get the parsed <code>DocumentContext</code> for further JsonPath processing <code>response.getDocumentContext()</code> <p>The client can be used against any GraphQL endpoint (it doesn't have to be implemented with the DGS framework), but provides extra conveniences for parsing Gateway and DGS responses. This includes support for the Errors Spec.</p>"},{"location":"advanced/java-client/#headers","title":"Headers","text":"<p>HTTP headers can easily be added to the request.</p> <pre><code>WebClientGraphQLClient client = MonoGraphQLClient.createWithWebClient(webClient, headers -&gt; headers.add(\"myheader\", \"test\"));\n</code></pre> <p>By default, the client already sets the <code>Content-type</code> and <code>Accept</code> headers.</p>"},{"location":"advanced/java-client/#errors","title":"Errors","text":"<p>The GraphQLClient checks both for HTTP level errors (based on the response status code) and the <code>errors</code> block in a GraphQL response. The GraphQLClient is compatible with the Errors Spec used by the Gateway and DGS, and makes it easy to extract error information such as the ErrorType.</p> <p>For example, for following GraphQL response the GraphQLClient lets you easily get the ErrorType and ErrorDetail fields. Note that the <code>ErrorType</code> is an enum as specified by the Errors Spec.</p> <pre><code>{\n  \"errors\": [\n    {\n      \"message\": \"java.lang.RuntimeException: test\",\n      \"locations\": [],\n      \"path\": [\n        \"hello\"\n      ],\n      \"extensions\": {\n        \"errorType\": \"BAD_REQUEST\",\n        \"errorDetail\": \"FIELD_NOT_FOUND\"\n      }\n    }\n  ],\n  \"data\": {\n    \"hello\": null\n  }\n}\n</code></pre> <pre><code>assertThat(graphQLResponse.errors.get(0).extensions.errorType).isEqualTo(ErrorType.BAD_REQUEST)\nassertThat(graphQLResponse.errors.get(0).extensions.errorDetail).isEqualTo(\"FIELD_NOT_FOUND\")\n</code></pre>"},{"location":"advanced/java-client/#plug-in-your-own-http-client","title":"Plug in your own HTTP client","text":"<p>Instead of using WebClient, you can also plug in your own HTTP client. This is useful if you already have a configured client for your backend with authN/authZ, TLS, etc. In this case you are responsible for making the actual request, but the GraphQL client wraps the HTTP client and provides easy parsing of GraphQL responses.</p> <p>There are two interfaces that you can pick from:</p> <ul> <li>GraphQLClient: For blocking HTTP clients</li> <li>MonoGraphQLClient: For non-blocking HTTP clients</li> </ul> <p>Both interfaces return a <code>GraphQLResponse</code> for each query execution, but <code>MonoGraphQLClient</code> wraps the result in a <code>Mono</code>, making it a better fit for non-blocking clients. Create an instance by using the factory method on the interface. This returns an instance of <code>CustomGraphQLClient</code> or <code>CustomMonoGraphQLClient</code>. The implementations are named <code>Custom*</code> to indicate you need to provide handling of the actual HTTP request.</p> JavaKotlin <pre><code>CustomGraphQLClient client = GraphQLClient.createCustom(url,  (url, headers, body) -&gt; {\nHttpHeaders httpHeaders = new HttpHeaders();\nheaders.forEach(httpHeaders::addAll);\nResponseEntity&lt;String&gt; exchange = restTemplate.exchange(url, HttpMethod.POST, new HttpEntity&lt;&gt;(body, httpHeaders),String.class);\nreturn new HttpResponse(exchange.getStatusCodeValue(), exchange.getBody());\n});\nGraphQLResponse graphQLResponse = client.executeQuery(query, emptyMap(), \"SubmitReview\");\nString submittedBy = graphQLResponse.extractValueAsObject(\"submitReview.submittedBy\", String.class);\n</code></pre> <pre><code>//Configure your HTTP client\nval restTemplate = RestTemplate();\nval client = GraphQLClient.createCustom(\"http://localhost:8080/graphql\") { url, headers, body -&gt;\n//Prepare the request, e.g. set up headers.\nval httpHeaders = HttpHeaders()\nheaders.forEach { httpHeaders.addAll(it.key, it.value) }\n//Use your HTTP client to send the request to the server.\nval exchange = restTemplate.exchange(url, HttpMethod.POST, HttpEntity(body, httpHeaders), String::class.java)\n//Transform the response into a HttpResponse\nHttpResponse(exchange.statusCodeValue, exchange.body)\n}\n//Send a query and extract a value out of the result.\nval result = client.executeQuery(\"{hello}\").extractValue&lt;String&gt;(\"hello\")\n</code></pre> <p>Alternatively, use <code>MonoGraphQLClient.createCustomReactive(...)</code> to create the reactive equivalent. The provided <code>RequestExecutor</code> must now return <code>Mono&lt;HttpResponse&gt;</code>.</p> <pre><code>CustomMonoGraphQLClient client = MonoGraphQLClient.createCustomReactive(url, (requestUrl, headers, body) -&gt; {\nHttpHeaders httpHeaders = new HttpHeaders();\nheaders.forEach(httpHeaders::addAll);\nResponseEntity&lt;String&gt; exchange = restTemplate.exchange(url, HttpMethod.POST, new HttpEntity&lt;&gt;(body, httpHeaders),String.class);\nreturn Mono.just(new HttpResponse(exchange.getStatusCodeValue(), exchange.getBody(), exchange.getHeaders()));\n});\nMono&lt;GraphQLResponse&gt; graphQLResponse = client.reactiveExecuteQuery(query, emptyMap(), \"SubmitReview\");\nString submittedBy = graphQLResponse.map(r -&gt; r.extractValueAsObject(\"submitReview.submittedBy\", String.class)).block();\n</code></pre> <p>Note that in this example we just use <code>Mono.just</code> to create a Mono. This doesn't make the call non-blocking.</p>"},{"location":"advanced/java-client/#migrating-from-defaultgraphqlclient","title":"Migrating from DefaultGraphQLClient","text":"<p>In previous versions of the framework we provided the <code>DefaultGraphQLClient</code> class. This has been deprecated for the following reasons:</p> <ul> <li>The \"Default\" in the name suggested that it should be the implementation for most use cases. However, the new WebClient implementation is a much better option now. Naming things is hard.</li> <li>The API required you to pass in the <code>RequestExecutor</code> for each query execution. This wasn't ergonomic for the new WebClient implementation, because no <code>RequestExecutor</code> is required.</li> </ul> <p>If you want to migrate existing usage of <code>DefaultGraphQLClient</code> you can either use the WebClient implementation and get rid of your <code>RequestExecutor</code> entirely, or alternatively use <code>CustomGraphQLClient</code> / <code>CustomMonoGraphQLClient</code> which has almost the same API. To migrate to <code>CustomGraphQLClient</code> you pass in your existing <code>RequestExecutor</code> to the <code>GraphQLClient.createCustom(url, requestExecutor)</code> factory method, and remove it from the <code>executeQuery</code> methods.</p> <p>We plan to eventually remove the <code>DefaultGraphQLClient</code>, because its API is confusing.</p>"},{"location":"advanced/java-client/#type-safe-query-api","title":"Type safe Query API","text":"<p>Based on a GraphQL schema a type safe query API can be generated for Java/Kotlin. The generated API is a builder style API that lets you build a GraphQL query, and it's projection (field selection). Because the code gets re-generated when the schema changes, it helps catch errors in the query. It's arguably also more readable, although multiline String support in Java and Kotlin do mitigate that issue as well.</p> <p>If you own a DGS and want to generate a client for this DGS (e.g. for testing purposes) the client generation is just an extra property on the Codegen configuration. Specify the following in your <code>build.gradle</code>.</p> <pre><code>// Using plugins DSL\nplugins {\nid \"com.netflix.dgs.codegen\" version \"[REPLACE_WITH_CODEGEN_PLUGIN_VERSION]\"\n}\ngenerateJava{\npackageName = 'com.example.packagename' // The package name to use to generate sources\ngenerateClient = true\n}\n</code></pre> <p>Code will be generated on build, and the generated code will be under <code>builder/generated</code>, which is added to the classpath by the plugin.</p> <p>With codegen configured correctly, a builder style API will be generated when building the project. Using the same query example as above, the query can be build using the generated builder API.</p> <pre><code>GraphQLQueryRequest graphQLQueryRequest =\n                new GraphQLQueryRequest(\n                    new TicksGraphQLQuery.Builder()\n                        .first(first)\n                        .after(after)\n                        .build(),\n                    new TicksConnectionProjectionRoot()\n                        .edges()\n                            .node()\n                                .date()\n                                .route()\n                                    .name()\n                                    .votes()\n                                        .starRating()\n                                        .parent()\n                                    .grade());\n\nString query = graphQLQueryRequest.serialize();\n</code></pre> <p>The <code>GraphQLQueryRequest</code> is a class made available by the Codegen plugin, specifically the <code>graphql-dgs-codegen-shared-core</code> module. Such module will be added as an implementation dependency if the plugin is applied to the project. The <code>TicksGraphQLQuery</code> and <code>TicksConnectionProjectionRoot</code> are generated. After building the query, it can be serialized to a String, and executed using the <code>GraphQLClient</code>.</p> <p>Note that the <code>edges</code> and <code>node</code> fields are because the example schema is using Relay pagination.</p>"},{"location":"advanced/java-client/#scalars-in-dgs-client","title":"Scalars in DGS Client","text":"<p>Custom scalars can be used in input types in GraphQL. Let's take the example of a <code>DateTimeScalar</code> (created in Adding Custom Scalars). In Java, we want to represent this as a <code>LocalDateTime</code> class. When sending the query, we somehow have to serialize this. There are many ways to represent a date, so how do we make sure that we use the same representation as the server expects?</p> <p>In this release we added an optional <code>scalars</code> argument to the <code>GraphQLQueryRequest</code> constructor. This is a <code>Map&lt;Class&lt;?&gt;, Coercing&lt;?,?&gt;&gt;</code> that maps the Java class representing the input to an actual Scalar implementation. We will generate the query API with <code>DateTimeScalar</code> as follows:</p> <pre><code>Map&lt;Class&lt;?&gt;, Coercing&lt;?, ?&gt;&gt; scalars = new HashMap&lt;&gt;();\nscalars.put(java.time.LocalDateTime.class, new DateTimeScalar());\nnew GraphQLQueryRequest(\nReviewsGraphQLQuery.newRequest().dateRange(new DateRange(LocalDate.of(2020, 1, 1), LocalDate.now())).build(),\nnew ReviewsProjectionRoot().submittedDate().starScore(), scalars);\n</code></pre> <p>This way you can re-use exactly the same serialization code that you already have for your scalar implementation or one of the existing ones from - for example - the <code>graphql-dgs-extended-scalars</code> module.</p>"},{"location":"advanced/java-client/#interface-projections","title":"Interface projections","text":"<p>When a field returns an interface, fields on the concrete types are specified using a fragment.</p> <pre><code>type Query @extends {\n    script(name: String): Script\n}\n\ninterface Script {\n    title: String\n    director: String\n    actors: [Actor]\n}\n\ntype MovieScript implements Script {\n    title: String\n    director: String\n    length: Int\n}\n\ntype ShowScript implements Script {\n    title: String\n    director: String\n    episodes: Int\n}\n</code></pre> <pre><code>query {\n    script(name: \"Top Secret\") {\n        title\n        ... on MovieScript {\n            length\n        }\n    }\n}\n</code></pre> <p>This syntax is supported by the Query builder as well.</p> <pre><code> GraphQLQueryRequest graphQLQueryRequest =\nnew GraphQLQueryRequest(\nnew ScriptGraphQLQuery.Builder()\n.name(\"Top Secret\")\n.build(),\nnew ScriptProjectionRoot()\n.title()\n.onMovieScript()\n.length();\n);\n</code></pre>"},{"location":"advanced/java-client/#building-federated-queries","title":"Building Federated Queries","text":"<p>You can use <code>GraphQLQueryRequest</code> along with <code>EntitiesGraphQLQuery</code> to generate federated queries. The API provides a type-safe way to construct the _entities query with the associated <code>representations</code> based on the input schema. The <code>representations</code> are passed in as a map of variables. Each representation class is generated based on the <code>key</code> fields defined  on the entity in your schema, along with the <code>__typename</code>. The <code>EntitiesProjectionRoot</code> is used to select query fields on the specified type.</p> <p>For example, let us look at a schema that extends a <code>Movie</code> type:</p> <pre><code>type Movie @key(fields: \"movieId\") @extends {\n    movieId: Int @external\n    script: MovieScript\n}\n\ntype MovieScript  {\n    title: String\n    director: String\n    actors: [Actor]\n}\n\ntype Actor {\n    name: String\n    gender: String\n    age: Int\n}\n</code></pre> <p>With client code generation, you will now have a <code>MovieRepresentation</code> containing the key field, i.e., <code>movieId</code>, and the <code>__typename</code> field already set to type <code>Movie</code>. Now you can add each representation to the <code>EntitiesGraphQLQuery</code> as a <code>representations</code> variable. You will also have a <code>EntitiesProjectionRoot</code> with <code>onMovie()</code> to select fields on <code>Movie</code> from. Finally, you put them all together as a <code>GraphQLQueryRequest</code>, which you serialize into the final query string. The map of <code>representations</code> variables is available via <code>getVariables</code> on the <code>EntitiesGraphQLQuery</code>.</p> <p>Here is an example for the schema shown earlier: <pre><code>        EntitiesGraphQLQuery entitiesQuery = new EntitiesGraphQLQuery.Builder()\n.addRepresentationAsVariable(\nMovieRepresentation.newBuilder().movieId(1122).build()\n)\n.build();\nGraphQLQueryRequest request = new GraphQLQueryRequest(\nentitiesQuery,\nnew EntitiesProjectionRoot().onMovie().movieId().script().title()\n);\nString query  = request.serialize();\nMap&lt;String, Object&gt; representations = entitiesQuery.getVariables();\n</code></pre></p>"},{"location":"advanced/java-client/#subscriptions","title":"Subscriptions","text":"<p>Subscriptions are supported through the <code>ReactiveGraphQLClient</code> interface. The interface has two implementations:</p> <ul> <li><code>WebSocketGraphQLClient</code>: For subscriptions over WebSockets</li> <li><code>SSESubscriptionGraphQLClient</code>: For subscriptions over Server Sent Events (SSE)</li> </ul> <p>Both implementations require the use of WebClient, and cannot be used with other HTTP clients (in contrast to the \"normal\" DGS client). The clients return a <code>Flux</code> of <code>GraphQLResponse</code>. Each <code>GraphQLResponse</code> represents a message pushed from the subscription, and contains <code>data</code> and <code>errors</code>. It also offers convenience methods to parse data using JsonPath.</p> JavaKotlin <pre><code>    WebClient webClient = WebClient.create(\"http://localhost:\" + port);\nSSESubscriptionGraphQLClient client = new SSESubscriptionGraphQLClient(\"/subscriptions\", webClient);\nFlux&lt;GraphQLResponse&gt; numbers = client.reactiveExecuteQuery(\"subscription {numbers}\", Collections.emptyMap());\nnumbers\n.mapNotNull(r -&gt; r.extractValue(\"data.numbers\"))\n.log()\n.subscribe();\n</code></pre> <pre><code>val webClient = WebClient.create(\"http://localhost:$port\")\nval client = SSESubscriptionGraphQLClient(\"/subscriptions\", webClient)\nval reactiveExecuteQuery = client.reactiveExecuteQuery(\"subscription {numbers}\", emptyMap())\nreactiveExecuteQuery\n.mapNotNull { r -&gt; r.data[\"numbers\"] }\n.log()\n.subscribe()\n</code></pre> <p>In case the connection fails to set up, either because of a connection error, or because of an invalid query, a <code>WebClientResponseException</code> will be thrown. Errors later on in the process will be errors in the stream.</p> <p>Don't forget to <code>subscribe()</code> to the stream, otherwise the connection doesn't get started!</p> <p>An example of using the client to write subscription integration tests is available here.</p>"},{"location":"advanced/mocking/","title":"Mocking","text":"<p>This guide is about how to provide mock data for data fetchers. There are two primary reasons to do so:</p> <ol> <li>Provide example data that UI teams can use while the data fetcher is under development.    This is useful during schema design.</li> <li>Provide stable test data for UI teams to write their tests against.</li> </ol> <p>An argument can be made that this type of mock data should live in the UI code. It\u2019s for their tests after all. However, by pulling it into the DGS, the owners of the data can provide test data that can be used by many teams. The two approaches are also not mutually exclusive.</p>"},{"location":"advanced/mocking/#graphql-mocking","title":"[GraphQL] Mocking","text":"<p>The library in the DGS framework supports:</p> <ol> <li>returning static data from mocks</li> <li>returning generated data for simple types (like String fields)</li> </ol> <p>The library is modular, so you can use it for a variety of workflows and use cases.</p> <p>The mocking framework is already part of the DGS framework. All you need to provide is one or more <code>MockProvider</code> implementations. <code>MockProvider</code> is an interface with a <code>Map&lt;String, Object&gt; provide()</code> method. Each key in the <code>Map</code> is a field in the [GraphQL] schema, which can be several levels deep. The value in the <code>Map</code> is whatever mock data you want to return for this key.</p>"},{"location":"advanced/mocking/#example","title":"Example","text":"<p>Create a <code>MockProvider</code> that provides mock data for the <code>hello</code> field you created in the getting started tutorial:</p> <pre><code>@Component\npublic class HelloMockProvider implements MockProvider {\n@NotNull\n@Override\npublic Map&lt;String, Object&gt; provide() {\nMap&lt;String, Object&gt; mock = new HashMap&lt;&gt;();\nmock.put(\"hello\", \"Mocked hello response\");\nreturn mock;\n}\n}\n</code></pre> <p>If you run the application again and test the <code>hello</code> query, you will see that it now returns the mock data.</p>"},{"location":"advanced/operation-caching/","title":"Operation Caching","text":"<p>Before operations (queries, mutations, and subscriptions) can be executed, their request string needs to be parsed and validated. Performing these two steps can be expensive.</p> <p>The GraphQL Java library opens up a special <code>PreparsedDocumentProvider</code> interface which intercepts these two steps and allows library consumers to cache, or modify the resulting operation.</p> <p>The DGS Framework supports injecting a <code>PreparsedDocumentProvider</code> by defining a bean of the same type.</p> <p>The following example uses Caffeine to cache the 2500 most recent operations for a maximum of 1 hour.</p> <pre><code>@Component // Resolved by Spring\npublic class CachingPreparsedDocumentProvider implements PreparsedDocumentProvider {\nprivate final Cache&lt;String, PreparsedDocumentEntry&gt; cache = Caffeine\n.newBuilder()\n.maximumSize(2500)\n.expireAfterAccess(Duration.ofHours(1))\n.build();\n@Override\npublic PreparsedDocumentEntry getDocument(ExecutionInput executionInput,\nFunction&lt;ExecutionInput, PreparsedDocumentEntry&gt; parseAndValidateFunction) {\nreturn cache.get(executionInput.getQuery(), operationString -&gt; parseAndValidateFunction.apply(executionInput));\n}\n}\n</code></pre> <p>The bean can also be injected using an annotated <code>@Bean</code> method:</p> <pre><code>@Configuration\npublic class MyDgsConfiguration {\n@Bean\npublic PreparsedDocumentEntry getDocument(ExecutionInput executionInput,\nFunction&lt;ExecutionInput, PreparsedDocumentEntry&gt; parseAndValidateFunction) {\nreturn new CachingPreparsedDocumentProvider();\n}\n}\n</code></pre>"},{"location":"advanced/operation-caching/#using-operation-variables","title":"Using operation variables","text":"<p>When using <code>PreparsedDocumentProvider</code> this way, it is important that you use operation variables in your operation. Otherwise, your cache may fill up with operations that are used only once, or contain personal information.</p> <p>This means that operations like the following:</p> <pre><code>query DgsPersonQuery {\n     person(id: \"123\") {\n        id\n        firstName\n     }\n}\n</code></pre> <p>Should be written as:</p> <pre><code>query DgsPersonQuery($personId: String!) {\n     person(id: $personId) {\n        id\n        firstName\n     }\n}\n</code></pre> <p>With the <code>personId</code> variable set to <code>\"123\"</code> in your specific client implementation.</p>"},{"location":"advanced/platform-bom/","title":"Using the Platform BOM","text":"<p>Using the Platform Bill of Materials (BOM)</p> <p>Both Gradle1 and Maven2 define a mechanism that developers can leverage to align the versions of dependencies that belong to the same framework, or an umbrella of dependencies that need to be aligned to work well together. Using them will prevent version conflicts and aide you figure out which dependency versions work well with each other.</p> <p>Let's go through a scenario, and assume you are using both the <code>graphql-dgs-spring-boot-starter</code> and the <code>graphql-dgs-subscriptions-websockets-autoconfigure</code>. Without using the platform/BOM you will have to define a version for each; unless the versions are explicitly maintained there is a chance that in the future they diverge. Manually aligning the versions of the dependencies becomes harder if your have a multi-module project where each module is using different dependencies of the DGS Framework, for example, the <code>graphql-dgs-client</code>. If you are using the platform/BOM you define the version of the DGS Framework in one place only, it will make sure that all other DGS Framework dependencies are using the same version.</p> <p>In the case of the DGS Framework we have two different BOM definitions, the <code>graphql-dgs-platform-dependencies</code> and the <code>graphql-dgs-platform</code>. The latter only defines version alignment for the DGS modules while the first also defines versions for the dependencies of the DGS framework, such as Spring, Jackson, and Kotlin.</p>"},{"location":"advanced/platform-bom/#using-the-platformbom","title":"Using the Platform/BOM","text":"<p>Warning</p> <p>If you are using the Spring Boot Dependency Management Plugin, please use the following syntax, such that the DGS BOM provides the version of <code>graphql-java</code>, and other managed dependencies. <pre><code>dependencyManagement {\nimports {\n// We need to define the DGS BOM as follows such that the\n// io.spring.dependency-management plugin respects the versions expressed in the DGS BOM, e.g. graphql-java\nmavenBom(\"com.netflix.graphql.dgs:graphql-dgs-platform-dependencies:latest.release\")\n}\n}\n</code></pre></p> <p>If the DGS BOM is not expressed in the <code>dependencyManagement</code> block as an imported Maven BOM, the Spring Boot Dependency Management Plugin will force the <code>graphql-java</code> version to a lower version. This will cause conflicts with what the DGS framework, and other artifacts such as the federation library, expect.</p> <p>Let's go through an example and assume that we want to use the DGS Framework 3.10.2...</p> GradleGradle KotlinMaven <pre><code>repositories {\nmavenCentral()\n}\ndependencies {\n//DGS BOM/platform dependency. This is the only place you set version of DGS\nimplementation(platform('com.netflix.graphql.dgs:graphql-dgs-platform-dependencies:3.10.2'))\n//DGS dependencies. We don't have to specify a version here!\nimplementation 'com.netflix.graphql.dgs:graphql-dgs-spring-boot-starter'\nimplementation 'com.netflix.graphql.dgs:graphql-dgs-subscriptions-websockets-autoconfigure'\n//Additional Jackson dependency. We don't need to specify a version, because Jackson is part of the BOM/platform definition.\nimplementation 'com.fasterxml.jackson.datatype:jackson-datatype-joda'\n//Other dependencies...\n}\n</code></pre> <pre><code>repositories {\nmavenCentral()\n}\ndependencies {\n//DGS BOM/platform dependency. This is the only place you set version of DGS\nimplementation(platform(\"com.netflix.graphql.dgs:graphql-dgs-platform-dependencies:3.10.2\"))\n//DGS dependencies. We don't have to specify a version here!\nimplementation(\"com.netflix.graphql.dgs:graphql-dgs-spring-boot-starter\")\nimplementation(\"com.netflix.graphql.dgs:graphql-dgs-subscriptions-websockets-autoconfigure\")\n//Additional Jackson dependency. We don't need to specify a version, because Jackson is part of the BOM/platform definition.\nimplementation(\"com.fasterxml.jackson.datatype:jackson-datatype-joda\")\n//Other dependencies...\n}\n</code></pre> <pre><code>&lt;dependencyManagement&gt;\n&lt;dependencies&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.netflix.graphql.dgs&lt;/groupId&gt;\n&lt;artifactId&gt;graphql-dgs-platform-dependencies&lt;/artifactId&gt;\n&lt;!-- The DGS BOM/platform dependency. This is the only place you set version of DGS --&gt;\n&lt;version&gt;3.10.2&lt;/version&gt;\n&lt;type&gt;pom&lt;/type&gt;\n&lt;scope&gt;import&lt;/scope&gt;\n&lt;/dependency&gt;\n&lt;/dependencies&gt;\n&lt;/dependencyManagement&gt;\n&lt;dependencies&gt;\n&lt;!-- DGS dependencies. We don't have to specify a version here! --&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.netflix.graphql.dgs&lt;/groupId&gt;\n&lt;artifactId&gt;graphql-dgs-spring-boot-starter&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.netflix.graphql.dgs&lt;/groupId&gt;\n&lt;artifactId&gt;graphql-dgs-subscriptions-websockets-autoconfigure&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;!-- Additional Jackson dependency. We don't need to specify a version, because Jackson is part of the BOM/platform definition. --&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.fasterxml.jackson.datatype&lt;/groupId&gt;\n&lt;artifactId&gt;jackson-datatype-joda&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;!-- Other dependencies --&gt;\n&lt;/dependencies&gt;\n</code></pre> <p>Notice that the version is only specified on the platform dependency, and not on the <code>graphql-dgs-spring-boot-starter</code> and <code>graphql-dgs-subscriptions-websockets-autoconfigure</code>. The BOM will make sure that all DGS dependencies are aligned, in other words, using the same version. In addition, since we are using the <code>graphql-dgs-platform-dependencies</code>, we can use the DGS chosen version of some dependencies as well, such as Jackson.</p> <p>Note</p> <p>Versions in the platform are recommendations. The versions can be overridden by the user, or by other platforms you might be using.</p> <ol> <li> <p>Gradle supports this via the Java Platform, checkout the section that describes how to consume a Java platform.\u00a0\u21a9</p> </li> <li> <p>Maven supports this via the BOM. Note that the BOM will be consumed via the <code>dependencyManagement</code> block.\u00a0\u21a9</p> </li> </ol>"},{"location":"advanced/relay-pagination/","title":"Relay Pagination","text":""},{"location":"advanced/relay-pagination/#relay-pagination","title":"Relay Pagination","text":"<p>The DGS framework supports dynamic generation of schema types for cursor based pagination based on the relay spec. When a type in the graphql schema is annotated with the <code>@connection</code> directive, the framework generates the corresponding <code>Connection</code> and <code>Edge</code> types, along with the common <code>PageInfo</code>.</p> <p>This avoids boilerplate code around defining related Connection and Edge types in the schema for every type that needs to be paginated. </p> <p>Note</p> <p>The <code>@connection</code> directive only works for DGSs that are not required to register the static schema file with an external service (since the relay types are dynamically generated). For example, in a federated architecture involving a gateway, some gateway implementations may or may not recognize the <code>@connection</code> directive when working with a static schema file.</p>"},{"location":"advanced/relay-pagination/#set-up","title":"Set up","text":"<p>To enable the use of <code>@connection</code> directive for generating the schema for pagination, add the following module to dependencies in your build.gradle:</p> <pre><code>dependencies {\n    implementation 'com.netflix.graphql.dgs:graphql-dgs-pagination'\n}\n</code></pre> <p>Next, add the directive on the type you want to paginate.</p> <pre><code>type Query {\n      hello: MessageConnection\n}\n\ntype Message @connection {\n    name: String\n}\n</code></pre> <p>Note that the <code>@connection</code> directive is defined automatically by the framework, so there is no need to add it to your schema file.</p> <p>This results in the following relay types dynamically generated and added to the schema:</p> <pre><code>\"MessageConnection\"\ntype MessageConnection {\n  \"Field edges\"\n  edges: [MessageEdge]\n  \"Field pageInfo\"\n  pageInfo: PageInfo\n}\n\n\"MessageEdge\"\ntype MessageEdge {\n    \"Field node\"\n    node: Message\n    \"Field cursor\"\n    cursor: String\n}\n\n\"PageInfo\"\ntype PageInfo {\n    \"Field hasPreviousPage\"\n    hasPreviousPage: Boolean!\n    \"Field hasNextPage\"\n    hasNextPage: Boolean!\n    \"Field startCursor\"\n    startCursor: String\n    \"Field endCursor\"\n    endCursor: String\n}\n</code></pre> <p>You can now use the corresponding <code>graphql.relay</code> types for <code>Connection&lt;T&gt;</code>, <code>Edge&lt;T&gt;</code> and <code>PageInfo</code> to set up your datafetcher as shown here:</p> <pre><code>@DgsData(parentType = \"Query\", field = \"hello\")\npublic Connection&lt;Message&gt; hello(DataFetchingEnvironment env) {\nreturn new SimpleListConnection&lt;&gt;(Collections.singletonList(new Message(\"This is a generated connection\"))).get(env);\n}\n</code></pre> <p>If your schema references a pagination type in a nested type, and you are using the code generation plugin, you will need some additional configuration, as described in the next section.</p>"},{"location":"advanced/relay-pagination/#configuring-code-generation","title":"Configuring Code Generation","text":"<p>If you are using the DGS Codegen Plugin for generating your data model, you will need to also add a type mapping for the relay types. The code generation plugin does not process the <code>@connection</code> directive and therefore needs to be configured so the generated classes can refer to the mapped type.</p> <p>For example, <pre><code>generateJava{\n  ...\n  typeMapping = [\"MessageConnection\": \"graphql.relay.SimpleListConnection&lt;Message&gt;\"]\n}\n</code></pre></p>"},{"location":"advanced/response-instrumentation/","title":"Response Instrumentation","text":"<p>The DGS framework internally uses [GraphQL Java]. GraphQL Java supports the concept of <code>instrumentation</code> that can be used to perform additional operations (tracing, logging), and instrument the graphql query and response as needed. The DGS framework makes it easy to add one or more instrumentation classes by implementing the <code>graphql.execution.instrumentation.Instrumentation</code> interface and registering the class as <code>@Component</code>. The most common method  to implement an <code>Instrumentation</code> interface is to extend <code>graphql.execution.instrumentation.SimpleInstrumentation</code>.</p> <p>Leveraging the concept of an instrumentation class, the framework offers the ability to add response headers to the outgoing response via setting the extensions field in the <code>ExecutionResult</code> for MVC.  This provides a hook to update the response headers based on the result of the <code>graphql</code> query execution. The instrumentation class provides access to <code>graphql</code> specific state related to the query. Simply add a map of headers using <code>DgsRestController.DGS_RESPONSE_HEADERS_KEY</code> to the extensions field of the result, and the framework handles adding those to the outgoing response.  This is a special key and is consumed by the framework, and thus will not appear in the final <code>extensions</code> field as part of the result.</p>"},{"location":"advanced/response-instrumentation/#example","title":"Example:","text":"<pre><code>@Component\npublic class MyInstrumentation extends SimpleInstrumentation {\n@Override\npublic CompletableFuture&lt;ExecutionResult&gt; instrumentExecutionResult(ExecutionResult executionResult, InstrumentationExecutionParameters parameters) {\nHashMap&lt;Object, Object&gt; extensions = new HashMap&lt;&gt;();\nif(executionResult.getExtensions() != null) {\nextensions.putAll(executionResult.getExtensions());\n}\nMap&lt;String, String&gt; responseHeaders = new HashMap&lt;&gt;();\nresponseHeaders.put(\"myHeader\", \"hello\");\nextensions.put(DgsRestController.DGS_RESPONSE_HEADERS_KEY, responseHeaders);\nreturn super.instrumentExecutionResult(new ExecutionResultImpl(executionResult.getData(), executionResult.getErrors(), extensions), parameters);\n}\n}\n</code></pre>"},{"location":"advanced/schema-reloading/","title":"Hot reloading schemas","text":"<p>The DGS framework is designed to work well with tools such as JRebel. In large Spring Boot codebases with many dependencies, it can take some time to restart the application during development. Waiting for the application to start can be disruptive to the development workflow.</p>"},{"location":"advanced/schema-reloading/#enabling-development-mode-for-hot-reloading","title":"Enabling development mode for hot reloading","text":"<p>Tools like JRebel allow for hot-reloading code. You make code changes compile, and without restarting the application, see the changes in the running application. Actively developing a DGS often includes making schema changes and wiring datafetchers. Some initialization needs to happen to pick up such changes. Out-of-the-box the DGS framework caches this initialization to be as efficient as possible in production, so the initialization only happens during startup.</p> <p>We can configure the DGS framework to run in development mode during development, which re-initializes the schema on each request. You can enable development mode in three ways:</p> <ol> <li>Set the <code>dgs.reload</code> configuration property to <code>true</code> (e.g. in <code>application.yml</code>)</li> <li>Enable the <code>laptop</code> profile</li> <li>Implement your own <code>ReloadIndicator</code> bean to be fully in control over when to reload. This is useful when working with fully dynamic schemas.</li> </ol>"},{"location":"advanced/security/","title":"Security","text":""},{"location":"advanced/security/#fine-grained-access-control-with-secured","title":"Fine-grained Access Control with @Secured","text":"<p>The DGS Framework integrates with Spring Security using the well known <code>@Secured</code> annotation. Spring Security itself can be configured in many ways, which goes beyond the scope of this documentation. Once Spring Security is set up however, you can apply <code>@Secured</code> to your data fetchers, very similarly to how you apply it to a REST Controller in Spring MVC.</p> <pre><code>@DgsComponent\npublic class SecurityExampleFetchers {\n@DgsData(parentType = \"Query\", field = \"hello\")\npublic String hello() {\nreturn \"Hello to everyone\";\n}      @Secured(\"admin\")\n@DgsData(parentType = \"Query\", field = \"secureGroup\")\npublic String secureGroup() {\nreturn \"Hello to admins only\";\n}\n}\n</code></pre>"},{"location":"advanced/subscriptions/","title":"Subscriptions","text":"<p>GraphQL Subscriptions enable a client to receive updates for a query from the server over time. Pushing update notifications from the server is a good example.</p> <p>The DGS framework supports subscriptions out of the box.</p>"},{"location":"advanced/subscriptions/#the-server-side-programming-model","title":"The Server Side Programming Model","text":"<p>In the DGS framework a Subscription is implemented as a data fetcher with the <code>@DgsSubscription</code> annotation. The <code>@DgsSubscription</code> is just short-hand for <code>@DgsData(parentType = \"Subscription\")</code>. The difference with a normal data fetcher is that a subscription must return a <code>org.reactivestreams.Publisher</code>.</p> <pre><code>import reactor.core.publisher.Flux;\nimport org.reactivestreams.Publisher;\n@DgsSubscription\npublic Publisher&lt;Stock&gt; stocks() {\n//Create a never-ending Flux that emits an item every second\nreturn Flux.interval(Duration.ofSeconds(1)).map({ t -&gt; Stock(\"NFLX\", 500 + t) })\n}\n</code></pre> <p>The <code>Publisher</code> interface is from Reactive Streams. The Spring Framework comes with the Reactor library to work with Reactive Streams.</p> <p>A complete example can be found in <code>SubscriptionDatafetcher.java</code>.</p>"},{"location":"advanced/subscriptions/#websockets","title":"WebSockets","text":"<p>The GraphQL specification doesn't specify a transport protocol. WebSockets are the most popular transport protocol however, and are supported by the DGS Framework.</p> <p>The framework now supports the <code>graphql-transport-ws</code> sub-protocol for websockets for both webmvc and webflux stacks. Apollo now supports the client for the newer protocol as well. </p> <p>Note</p> <p>The deprecated <code>graphql-ws</code> sub-protocol is functional for backwards compatibility. However, this implementation will no longer be actively maintained in the framework and we will be dropping support in a future release.</p> <p>To enable WebSockets support for the WebMVC stack, add the following module to your <code>build.gradle</code>:</p> <pre><code>implementation 'com.netflix.graphql.dgs:graphql-dgs-subscriptions-websockets-autoconfigure:latest.release'\n</code></pre> <p>For WebFlux, the starter already comes with support for websocket subscriptions, so no additional configuration is required.</p> <pre><code>implementation 'com.netflix.graphql.dgs:graphql-dgs-webflux-starter:latest.release'\n</code></pre> <p>The subscription endpoint is on <code>/subscriptions</code>. Normal GraphQL queries can be sent to <code>/graphql</code>, while subscription requests go to <code>/subscriptions</code>. Apollo client supports WebSockets through a link. Typically, you want to configure Apollo Client with both an HTTP link and a WS link, and split between them based on the query type.</p> <p>A simple example of using the Apollo client can be found in the example project of the DGS Framework repository.</p>"},{"location":"advanced/subscriptions/#unit-testing-subscriptions","title":"Unit Testing Subscriptions","text":"<p>Similar to a \"normal\" data fetcher test, you use the <code>DgsQueryExecutor</code> to execute a query. Just like a normal query, this results in a <code>ExecutionResult</code>. Instead of returning a result directly in the <code>getData()</code> method, a subscription query returns a <code>Publisher</code>. A <code>Publisher</code> can be asserted using the testing capabilities from Reactor. Each <code>onNext</code> of the <code>Publisher</code> is another <code>ExecutionResult</code>. This <code>ExecutionResult</code> contains the actual data!</p> <p>It might take a minute to wrap your head around the concept of this nested <code>ExecutionResult</code>, but it gives an excellent way to test Subscriptions, including corner cases.</p> <p>The following is a simple example of such a test. The example tests the <code>stocks</code> subscription from above. The <code>stocks</code> subscription produces a result every second, so the test uses <code>VirtualTime</code> to fast-forward time, without needing to wait in the test.</p> <p>Also note that the emitted <code>ExecutionResult</code> returns a <code>Map&lt;String, Object&gt;</code>, and not the Java type that your data fetcher returns. Use the <code>Jackson Objectmapper</code> to convert the map to a Java object.</p> <pre><code>@SpringBootTest(classes = {DgsAutoConfiguration.class, SubscriptionDataFetcher.class})\nclass SubscriptionDataFetcherTest {\n\n    @Autowired\n    DgsQueryExecutor queryExecutor;\n\n    ObjectMapper objectMapper = new ObjectMapper();\n\n    @Test\n    void stocks() {\n        ExecutionResult executionResult = queryExecutor.execute(\"subscription Stocks { stocks { name, price } }\");\n        Publisher&lt;ExecutionResult&gt; publisher = executionResult.getData();\n\n        VirtualTimeScheduler virtualTimeScheduler = VirtualTimeScheduler.create();\n        StepVerifier.withVirtualTime(() -&gt; publisher, 3)\n                .expectSubscription()\n                .thenRequest(3)\n                .assertNext(result -&gt; assertThat(toStock(result).getPrice()).isEqualTo(500))\n                .assertNext(result -&gt; assertThat(toStock(result).getPrice()).isEqualTo(501))\n                .assertNext(result -&gt; assertThat(toStock(result).getPrice()).isEqualTo(502))\n                .thenCancel()\n                .verify();\n    }\n\n    private Stock toStock(ExecutionResult result) {\n        Map&lt;String, Object&gt; data = result.getData();\n        return objectMapper.convertValue(data.get(\"stocks\"), Stock.class);\n    }\n}\n</code></pre> <p>In this example the subscription works in isolation; it just emits a result every second. In other scenarios a subscription could depend on something else happening in the system, such as the processing of a mutation. Such scenarios are easy to set up in a unit test, simply run multiple queries/mutations in your test to see it all work together.</p> <p>Notice that the unit tests really only test your code. It doesn't care about transport protocols. This is exactly what you need for your tests, because your tests should focus on testing your code, not the framework code.</p>"},{"location":"advanced/subscriptions/#integration-testing-subscriptions","title":"Integration testing subscriptions","text":"<p>Although most subscription logic should be tested in unit tests, it can be useful to test end-to-end with a client. This can be achieved with the DGS client, and works well in a <code>@SpringBootTest</code> with a random port. The example below starts a subscription, and sends to mutations that should result in updates on the subscription. The example uses Websockets, but the same can be done for SSE. The code for this example can be found in the example project.</p> <pre><code>@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT)\npublic class ReviewSubscriptionIntegrationTest {\n@LocalServerPort\nprivate Integer port;\nprivate WebSocketGraphQLClient webSocketGraphQLClient;\nprivate MonoGraphQLClient graphQLClient;\nprivate MonoRequestExecutor requestExecutor = (url, headers, body) -&gt; WebClient.create(url)\n.post()\n.bodyValue(body)\n.headers(consumer -&gt; headers.forEach(consumer::addAll))\n.exchangeToMono(r -&gt; r.bodyToMono(String.class).map(responseBody -&gt; new HttpResponse(r.rawStatusCode(), responseBody, r.headers().asHttpHeaders())));\n@BeforeEach\npublic void setup() {\nwebSocketGraphQLClient = new WebSocketGraphQLClient(\"ws://localhost:\" + port + \"/subscriptions\", new ReactorNettyWebSocketClient());\ngraphQLClient = new DefaultGraphQLClient(\"http://localhost:\" + port + \"/graphql\");\n}\n@Test\npublic void testWebSocketSubscription() {\nGraphQLQueryRequest subscriptionRequest = new GraphQLQueryRequest(\nReviewAddedGraphQLQuery.newRequest().showId(1).build(),\nnew ReviewAddedProjectionRoot().starScore()\n);\nGraphQLQueryRequest addReviewMutation1 = new GraphQLQueryRequest(\nAddReviewGraphQLQuery.newRequest().review(SubmittedReview.newBuilder().showId(1).starScore(5).username(\"DGS User\").build()).build(),\nnew AddReviewProjectionRoot().starScore()\n);\nGraphQLQueryRequest addReviewMutation2 = new GraphQLQueryRequest(\nAddReviewGraphQLQuery.newRequest().review(SubmittedReview.newBuilder().showId(1).starScore(3).username(\"DGS User\").build()).build(),\nnew AddReviewProjectionRoot().starScore()\n);\nFlux&lt;Integer&gt; starScore = webSocketGraphQLClient.reactiveExecuteQuery(subscriptionRequest.serialize(), Collections.emptyMap()).map(r -&gt; r.extractValue(\"reviewAdded.starScore\"));\nStepVerifier.create(starScore)\n.thenAwait(Duration.ofSeconds(1)) //This await is necessary because of issue [#657](https://github.com/Netflix/dgs-framework/issues/657)\n.then(() -&gt; {\ngraphQLClient.reactiveExecuteQuery(addReviewMutation1.serialize(), Collections.emptyMap(), requestExecutor).block();\n})\n.then(() -&gt;\ngraphQLClient.reactiveExecuteQuery(addReviewMutation2.serialize(), Collections.emptyMap(), requestExecutor).block())\n.expectNext(5)\n.expectNext(3)\n.thenCancel()\n.verify();\n}\n}\n</code></pre>"},{"location":"advanced/type-resolvers-for-abstract-types/","title":"Interfaces and Unions","text":"<p>You must register type resolvers whenever you use interface types or union types in your schema. Interface types and union types are explained in the GraphQL documentation.</p> <p>As an example, the following schema defines a <code>Movie</code> interface type with two different concrete object type implementations.</p> <pre><code>type Query {\n    movies: [Movie]\n}\n\ninterface Movie {\n    title: String\n}\n\ntype ScaryMovie implements Movie {\n    title: String\n    gory: Boolean\n    scareFactor: Int\n}\n\ntype ActionMovie implements Movie {\n    title: String\n    nrOfExplosions: Int\n}\n</code></pre> <p>The following data fetcher is registered to return a list of movies. The data fetcher returns a combination <code>Movie</code> types.</p> <pre><code>@DgsComponent\npublic class MovieDataFetcher {\n@DgsData(parentType = \"Query\", field = \"movies\")\npublic List&lt;Movie&gt; movies() {\nreturn Lists.newArrayList(\nnew ActionMovie(\"Crouching Tiger\", 0),\nnew ActionMovie(\"Black hawk down\", 10),\nnew ScaryMovie(\"American Horror Story\", true, 10),\nnew ScaryMovie(\"Love Death + Robots\", false, 4)\n);\n}\n}\n</code></pre> <p>The GraphQL runtime needs to know that a Java instance of <code>ActionMovie</code> represents the <code>ActionMovie</code> GraphQL type. This mapping is the responsibility of a <code>TypeResolver</code>.</p>  Tip:     If your Java type names and GraphQL type names are the same, the DGS framework creates a `TypeResolver` automatically.      No code needs to be added!"},{"location":"advanced/type-resolvers-for-abstract-types/#registering-a-type-resolver","title":"Registering a Type Resolver","text":"<p>If the name of your Java type and GraphQL type don't match, you need to provide a <code>TypeResolver</code>. A type resolver helps the framework map from concrete Java types to the correct object type in the schema.</p> <p>Use the <code>@DgsTypeResolver</code> annotation to register a type resolver. The annotation has a <code>name</code> property; set this to the name of the interface type or union type in the [GraphQL] schema. The resolver takes an object of the Java interface type, and returns a String which is the concrete object type of the instance as defined in the schema. The following is a type resolver for the <code>Movie</code> interface type introduced above:</p> <pre><code>@DgsTypeResolver(name = \"Movie\")\npublic String resolveMovie(Movie movie) {\nif(movie instanceof ScaryMovie) {\nreturn \"ScaryMovie\";\n} else if(movie instanceof ActionMovie) {\nreturn \"ActionMovie\";\n} else {\nthrow new RuntimeException(\"Invalid type: \" + movie.getClass().getName() + \" found in MovieTypeResolver\");\n}\n}\n</code></pre> <p>You can add the <code>@DgsTypeResolver</code> annotation to any <code>@DgsComponent</code> class. This means you can either keep the type resolver in the same class as the data fetcher responsible for returning the data for this type, or you can create a separate class for it.</p>"}]}