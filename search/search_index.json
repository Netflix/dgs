{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started","text":""},{"location":"#introduction","title":"Introduction","text":"<p>The DGS framework makes it easy to create GraphQL services with Spring Boot. The framework provides an easy-to-use annotation based programming model, and all the advanced features needed to build and run GraphQL services at scale.</p> <p>The DGS framework is primarily maintained by Netflix, surrounded by an active community. At Netflix we build our GraphQL architecture on the DGS framework.</p>"},{"location":"#create-a-new-spring-boot-application","title":"Create a new Spring Boot application","text":"<p>The DGS framework is now based on Spring Boot 3</p> <p>The easiest way to create a DGS project is to use the Spring Initializr.  You'll need to following dependencies:</p> <ol> <li>Netflix DGS</li> <li>Spring Web or Spring Reactive Web (WebFlux)</li> <li>(Optional) GraphQL DGS Code Generation</li> </ol> <p></p> <p>You can use either Gradle or Maven with Java 17 or Kotlin. We do recommend Gradle because we have a really cool code generation plugin for it!</p> <p>Open the project in an IDE (Intellij recommended).</p>"},{"location":"#requirements","title":"Requirements","text":"<p>The DGS framework requires Spring Boot 3 and JDK 17 for your project. The last version compatible with Spring Boot 2 and JDK 8 was 5.x, which is no longer maintained.</p>"},{"location":"#adding-the-dgs-framework-dependency-with-spring-for-graphql","title":"Adding the DGS Framework dependency with Spring for GraphQL","text":"<p>If you used the Spring Initializr to generate your DGS project, these steps are not necessary, your project is ready to go!</p> <ol> <li> <p>Add the platform BOM to your Gradle or Maven configuration.    The <code>com.netflix.graphql.dgs:graphql-dgs-platform-dependencies</code> dependency is a platform/BOM dependency, which aligns the versions of the individual modules and transitive dependencies of the framework.</p> </li> <li> <p>Add the DGS starter.    The <code>com.netflix.graphql.dgs:dgs-starter</code> is a Spring Boot starter that includes everything you need to get started building a DGS that uses Spring GraphQL.</p> </li> <li> <p>Add the relevant Spring Boot starter for the web flavor you want to use.    This would one of <code>org.springframework.boot:spring-boot-starter-web</code> or <code>org.springframework.boot:spring-boot-starter-webflux</code> depending on the stack you are using.</p> </li> </ol>"},{"location":"#creating-a-schema","title":"Creating a Schema","text":"<p>The DGS framework is designed for schema first development. The framework picks up any schema files in the <code>src/main/resources/schema</code> folder. Create a schema file in: <code>src/main/resources/schema/schema.graphqls</code>.</p> <pre><code>type Query {\n    shows(titleFilter: String): [Show]\n}\n\ntype Show {\n    title: String\n    releaseYear: Int\n}\n</code></pre> <p>This schema allows querying for a list of shows, optionally filtering by title.</p>"},{"location":"#implement-a-data-fetcher","title":"Implement a Data Fetcher","text":"<p>Data fetchers are responsible for returning data for a query.</p> <p>With the new Spring-GraphQL integration, it is technically possible to mix and match the DGS/Spring-GraphQL programming models. However, to maintain consistency in your codebase and to take full advantage of DGS features, we recommend sticking with the DGS programming model. Not all DGS features are applicable to Spring-GraphQL data fetchers in the current integration and would therefore not work as expected. Refer to our Known Gaps and Limitations section for more details.</p> <p>Create two new classes <code>example.ShowsDataFetcher</code> and <code>Show</code> and add the following code.</p> <p>Note that we have a Codegen plugin that can do this automatically, but in this guide we'll manually write the classes.</p> Java <pre><code>import java.util.List;\nimport java.util.stream.Collectors;\n\nimport com.netflix.graphql.dgs.DgsComponent;\nimport com.netflix.graphql.dgs.DgsQuery;\nimport com.netflix.graphql.dgs.InputArgument;\n\n@DgsComponent\npublic class ShowsDataFetcher {\n\n  private final List&lt;Show&gt; shows = List.of(\n          new Show(\"Stranger Things\", 2016),\n          new Show(\"Ozark\", 2017),\n          new Show(\"The Crown\", 2016),\n          new Show(\"Dead to Me\", 2019),\n          new Show(\"Orange is the New Black\", 2013)\n  );\n\n  @DgsQuery\n  public List&lt;Show&gt; shows(@InputArgument String titleFilter) {\n      if(titleFilter == null) {\n          return shows;\n      }\n\n      return shows.stream().filter(s -&gt; s.title().contains(titleFilter)).collect(Collectors.toList());\n  }\n}\n\nrecord Show(String title, int releaseYear) {}\n</code></pre> Kotlin <pre><code>import java.util.List;\nimport java.util.stream.Collectors;\n\nimport com.netflix.graphql.dgs.DgsComponent;\nimport com.netflix.graphql.dgs.DgsQuery;\nimport com.netflix.graphql.dgs.InputArgument;\n\n@DgsComponent\nclass ShowsDataFetcher {\n  private val shows = listOf(\n      Show(\"Stranger Things\", 2016),\n      Show(\"Ozark\", 2017),\n      Show(\"The Crown\", 2016),\n      Show(\"Dead to Me\", 2019),\n      Show(\"Orange is the New Black\", 2013))\n\n  @DgsQuery\n  fun shows(@InputArgument titleFilter : String?): List&lt;Show&gt; {\n      return if(titleFilter != null) {\n          shows.filter { it.title.contains(titleFilter) }\n      } else {\n          shows\n      }\n  }\n\n  data class Show(val title: String, val releaseYear: Int)\n}\n</code></pre> <p>That's all the code needed, the application is ready to be tested!</p>"},{"location":"#test-the-app-with-graphiql","title":"Test the app with GraphiQL","text":"<p>Start the application and open a browser to http://localhost:8080/graphiql. GraphiQL is a query editor that comes out of the box with the DGS framework. Write the following query and tests the result.</p> <pre><code>gradle bootRun\n</code></pre> <pre><code>mvn spring-boot:run\n</code></pre> <pre><code>{\n    shows {\n        title\n        releaseYear\n    }\n}\n</code></pre> <p>Note that unlike with REST, you have to specifically list which fields you want to get returned from your query. This is where a lot of the power from GraphQL comes from, but a surprise to many developers new to GraphQL.</p> <p>The GraphiQL editor is really just a UI that uses the <code>/graphql</code> endpoint of your service. You could now connect a UI to your backend as well, for example using React and the Apollo Client.</p>"},{"location":"#install-the-intellij-plugin","title":"Install the Intellij plugin","text":"<p>If you are an Intellij user, there is a plugin available for DGS. The plugin supports navigation between schema files and code and many hints and quick fixes. You can install the plugin from the Jetbrains plugin repository here.</p> <p></p>"},{"location":"#next-steps","title":"Next steps","text":"<p>Now that you have a first GraphQL service running, we recommend improving this further by doing the following:</p> <ul> <li>Use the DGS Platform BOM to align DGS Framework dependencies.</li> <li>Learn more about datafetchers</li> <li>Use the Gradle CodeGen plugin - this will generate the data types for you.</li> <li>Write query tests in JUnit</li> <li>Look at example projects</li> </ul>"},{"location":"announcements/","title":"New and Noteworthy","text":""},{"location":"announcements/#dgs-framework-1000-released-dec-17-2024","title":"DGS Framework 10.0.0 released! (Dec 17, 2024)","text":"<p>DGS 10.0.0 removes all the legacy code in favor of our integration with Spring for GraphQL. In March 2024 we released deep integration with Spring for GraphQL after working closely with the Spring team. This integration makes it possible to mix and match features from DGS and Spring for GraphQL, and leverages the web transports provided by Spring for GraphQL. With the March released we declared the \"old\" DGS starter, and the implementation code legacy, with the plan to remove this code end of 2024. The community has adopted the DGS/Spring for GraphQL integration really well, in most cases without any required code changes. At Netflix we migrated all our services to use the new integration, again mostly without any code changes. Performance is critical for our services, and after all the performance optimization that went into the March release and some patch releases after, we see the same performance with the Spring for GraphQL integration as what we had previously.</p> <p>DGS 10.0.0 finalizes the integration work by removing all the legacy modules and code. This greatly reduces the footprint of the codebase, which will speed up feature development into the future!</p> <p>Although the list of changes is large, you probably won't notice the difference for your applications! Just make sure to use the (new) <code>netflix.graphql.dgs:dgs-starter</code> AKA <code>netflix.graphql.dgs:graphql-dgs-spring-graphql-starter</code> starter!</p>"},{"location":"announcements/#detailed-list-of-changes","title":"Detailed list of changes","text":"<p>New modules:</p> <ul> <li><code>netflix.graphql.dgs:dgs-starter</code> as a nicer/shorter name for <code>netflix.graphql.dgs:graphql-dgs-spring-graphql-starter</code>.</li> </ul> <p>Deleted modules:</p> <ul> <li><code>graphql-dgs-spring-boot-oss-autoconfigure</code> (replaced by Spring for GraphQL)</li> <li><code>graphql-dgs-spring-webmvc</code> (replaced by Spring for GraphQL)</li> <li><code>graphql-dgs-spring-webmvc-autoconfigure</code> (replaced by Spring for GraphQL)</li> <li><code>graphql-dgs-spring-boot-starter</code> (replaced by <code>netflix.graphql.dgs:dgs-starter</code>)</li> <li><code>graphql-dgs-example-java</code> (legacy example, no longer relevant)</li> <li><code>graphql-dgs-example-java-webflux</code> (legacy example, no longer relevant)</li> <li><code>graphql-dgs-mocking</code> (old feature that wasn't used much)</li> <li><code>graphql-dgs-subscriptions-websockets</code> (replaced by Spring for GraphQL)</li> <li><code>graphql-dgs-subscriptions-websockets-autoconfigure</code> (replaced by Spring for GraphQL)</li> <li><code>graphql-dgs-subscriptions-graphql-sse</code> (replaced by Spring for GraphQL)</li> <li><code>graphql-dgs-subscriptions-graphql-sse-autoconfigure</code> (replaced by Spring for GraphQL)</li> <li><code>graphql-dgs-subscriptions-sse</code> (replaced by Spring for GraphQL)</li> <li><code>graphql-dgs-subscriptions-sse-autoconfigure</code> (replaced by Spring for GraphQL)</li> <li><code>graphql-dgs-spring-webflux-autoconfigure</code> (replaced by Spring for GraphQL)</li> <li><code>graphql-dgs-webflux-starter</code> (replaced by <code>netflix.graphql.dgs:dgs-starter</code>)</li> </ul> <p>Deleted classes:</p> <ul> <li><code>DgsAutoConfiguration</code>: Autoconfiguration classes have moved. This may break tests that are using @SpringBootTest(classes = {DgsAutoConfiguration.class, ...}, and should use @EnableDgsTest instead.</li> <li><code>DefaultGraphQLClient</code>: This is a long deprecated class that has been replaced by [CustomGraphQLClient, CustomReactiveGraphQLClient and WebClientGraphQLClient.</li> <li><code>DefaultDgsReactiveQueryExecutor</code>: There should be no user impact because its interface should be used instead.</li> <li><code>DefaultDgsQueryExecutor</code>: There should be no user impact because its interface should be used instead.</li> </ul> <p>Other changes:</p> <ul> <li>We're now using the K2 compiler and Kotlin 2.1.</li> <li>Subscriptions over SSE in Spring GraphQL are using the newer, more official GraphQL over SSE RFC spec. The old spec is no longer supported by DGS.</li> <li>If you are using SSESubscriptionGraphQLClient in tests to test your DGS server, switch to GraphqlSSESubscriptionGraphQLClient, which has the same interface, but uses the new protocol.</li> <li>SSESubscriptionGraphQLClient can still be used to call into other services using the old protocol, but this client is now deprecated for removal in the future.</li> <li>Note that Spring GraphQL serves subscriptions on the /graphql endpoint, not on the /subscriptions endpoint like DGS used to do.</li> </ul>"},{"location":"announcements/#dgs-framework-850-with-spring-graphql-released-march-292024","title":"DGS Framework 8.5.0 with Spring GraphQL released! (March 29,2024)","text":"<p>The DGS and Spring-GraphQL teams are super excited to introduce deep integration between the DGS framework and Spring-GraphQL.  This will bring the community together, and we can continue building the best possible GraphQL framework for Spring Boot in the future. We have been actively working on this integration over the past several months and are preparing for a release in the coming weeks.</p> <p>Check out our release notes for all the details! Additional details are available in here</p>"},{"location":"announcements/#dgs-framework-800","title":"DGS Framework 8.0.0","text":"<p>This release updates the graphql-java version to 21.2.  The main breaking change affects the usage of the already deprecated DefaultExceptionHandler::onException method.  If you have defined your own custom exception handlers, you will need to switch to using handleException instead of onException.</p>"},{"location":"announcements/#dgs-framework-700-may-15-2023","title":"DGS Framework 7.0.0 (May 15, 2023)","text":"<p>The latest 7.0.0 release updates the version of graphql-java from graphql-java-19.5 -&gt; graphql-java 20.2. Graphql-java-20.0 introduces breaking changes. Refer to the notes here.</p> <p>Other dependencies updated include :</p> <ul> <li>graphql-java-extended-scalars: 19.1 -&gt; 20.2</li> <li>graphql-java-extended-validation: 19.1 -&gt; 20.0</li> <li>federation-graphql-java-support: 2.1.0 -&gt; 3.0.0</li> </ul>"},{"location":"announcements/#dgs-framework-600-now-on-spring-boot-300-january-17-2023","title":"DGS Framework 6.0.0 now on Spring Boot 3.0.0! (January 17, 2023)","text":"<p>The 6.0.0 release now supports Spring Boot 3.0.0.  This is a breaking change and requires the application to be using Spring Boot 3.0.0 and JDK 17. We will continue to maintain a separate 5.x.x release train for supporting Spring Boot 2.7 for the near future for any minor bug fixes and improvements.</p> <p>The following versions are updated: * Spring Boot 3.0.0 * Spring Framework 6.0.3 * Spring Security 6.0.1 * Spring Cloud 2022.0.0 * JDK target 17</p>"},{"location":"announcements/#other-breaking-changes","title":"Other Breaking Changes","text":""},{"location":"announcements/#use-graphqlcontext-instead-of-dgscontext-for-dataloaders","title":"Use GraphQLContext instead of DgsContext for dataloaders","text":"<p>Previously, the DGS framework passed DgsContext to dataloaders as context.  CustomContext is contained in DgsContext, and is generally retrieved with a static helper. <pre><code>MyContext context = DgsContext.getCustomContext(environment);\n</code></pre> The helper DgsContext::getCustomContext is able to pull MyContext from GraphQLContext, so this is non-breaking for users that employ the recommended helper method. This is potentially a breaking change for any user code that coerces dataloader context to DgsContext manually. Updating to using the recommended <code>getCustomContext</code> should fix any resulting issues. <pre><code>MyContext context = (DgsContext)environment.context;\n</code></pre></p>"},{"location":"announcements/#upcoming-release-of-the-dgs-framework-for-spring-boot-30-january-10-2023","title":"Upcoming Release of the DGS Framework for Spring Boot 3.0 (January 10, 2023)","text":"<p>We plan to release a new version 6.x of the DGS Framework supporting Spring Boot 3.0 by end of January 2023.  There are no known additional changes required to use the new version of the DGs framework. We will continue to maintain a separate release train for the DGS framework (5.x.x) on Spring Boot 2.7 till the end of 2023. Only patches and minor features will be available on the Spring Boot 2.7 compatible releases. </p>"},{"location":"configuration/","title":"Configuration","text":""},{"location":"configuration/#configuration","title":"Configuration","text":""},{"location":"configuration/#core-properties","title":"Core Properties","text":"Name Type Default Description dgs.graphql.path String <code>\"/graphql\"</code> Path to the endpoint that will serve GraphQL requests. dgs.graphql.introspection.enabled Boolean <code>true</code> Enables graphql introspection functionality. dgs.graphql.introspection.show-sdl-comments Boolean <code>true</code> Toggles showing graphql SDL comments in introspection query results. dgs.graphql.schema-json.enabled Boolean <code>true</code> Enables schema-json endpoint functionality.  If you are using Spring GraphQL, use spring.graphql.schema.printer.enabled instead. dgs.graphql.schema-json.path String <code>\"/schema.json\"</code> Path to the schema-json endpoint without trailing slash. dgs.graphql.schema-locations [String] <code>\"classpath*:schema/**/*.graphql*\"</code> Location of the GraphQL schema files. dgs.graphql.graphiql.enabled Boolean <code>true</code> Enables GraphiQL functionality. dgs.graphql.graphiql.path String <code>\"/graphiql\"</code> Path to the GraphiQL endpoint without trailing slash. dgs.graphql.graphiql.title String <code>\"Simple GraphiQL Example\"</code> Title of the GraphiQL page dgs.graphql.enable-entity-fetcher-custom-scalar-parsing Boolean <code>false</code> Enables the bug fix for entity fetcher custom scalar parsing. This will eventually be enabled by default. dgs.graphql.dataloader.ticker-mode-enabled Boolean <code>false</code> Enables the ticker mode for scheduling data loader dispatches. dgs.graphql.dataloader.schedule-duration String <code>10ms</code> Set the schedule for scheduling dispatch predicate checks. dgs.graphql.preparsed-document-provider.enabled Boolean <code>false</code> Enables a Caffiene-cache backed implementation of a PreparsedDocumentProvider. dgs.graphql.preparsed-document-provider.maximum-cache-size Long <code>2000</code> Sets the maximum size of the PreparsedDocumentProvider Caffiene-cache. dgs.graphql.preparsed-document-provider.cache-validity-duration String <code>PT1H</code> How long a cached entry in the PreparsedDocumentProvider Caffiene-cache is valid for. Specified as an ISO-8601 duration string."},{"location":"configuration/#example-configure-the-location-of-the-graphql-schema-files","title":"Example: Configure the location of the GraphQL Schema Files","text":"<p>You can configure the location of your GraphQL schema files via the <code>dgs.graphql.schema-locations</code> property. By default it will attempt to load them from the <code>schema</code> directory via the Classpath, i.e. using <code>classpath*:schema/**/*.graphql*</code>. Let's go through an example, let's say you want to change the directory from being <code>schema</code> to <code>graphql-schemas</code>, you would define your configuration as follows:</p> <pre><code>dgs:\n  graphql:\n    schema-locations:\n      - classpath*:graphql-schemas/**/*.graphql*\n</code></pre> <p>Now, if you want to add additional locations to look for the GraphQL Schema files you add them to the list. For example, let's say we want to also look into your <code>graphql-experimental-schemas</code>:</p> <pre><code>dgs:\n  graphql:\n    schema-locations:\n      - classpath*:graphql-schemas/**/*.graphql*\n      - classpath*:graphql-experimental-schemas/**/*.graphql*\n</code></pre>"},{"location":"configuration/#dgs-extended-scalars-graphql-dgs-extended-scalars","title":"DGS Extended Scalars: graphql-dgs-extended-scalars","text":"Name Type Default Description dgs.graphql.extensions.scalars.enabled Boolean <code>true</code> Registered the Scalar Extensions available in graphql-java-extended-scalars for the DGS Framework. dgs.graphql.extensions.scalars.chars.enabled Boolean <code>true</code> Will register the Char scalar extension. dgs.graphql.extensions.scalars.numbers.enabled Boolean <code>true</code> Will register all numeric scalar extensions (PositiveInt, NegativeInt, NonPositiveInt, NonNegativeInt, PositiveFloat, NegativeFloat, NonPositiveFloat, NonNegativeFloat, Long, Short, Byte, BigDecimal, BigInteger). dgs.graphql.extensions.scalars.objects.enabled Boolean <code>true</code> Will register the Object, Json, Url, and Locale scalar extensions. dgs.graphql.extensions.scalars.time-dates.enabled Boolean <code>true</code> Will register the DateTime, Date, Time and LocalTime scalar extensions. dgs.graphql.extensions.scalars.ids.enabled Boolean <code>true</code> Will register the UUID scalar extension. dgs.graphql.extensions.scalars.country Boolean <code>true</code> Will register the CountryCode scalar extension. dgs.graphql.extensions.scalars.currency Boolean <code>true</code> Will register the Currency scalar extension."},{"location":"configuration/#dgs-extended-validation-graphql-dgs-extended-validation","title":"DGS Extended Validation: graphql-dgs-extended-validation","text":"Name Type Default Description dgs.graphql.extensions.validation.enabled Boolean <code>true</code> Registered the Validation Schema Directive Extensions available in graphql-java-extended-validation for the DGS Framework."},{"location":"configuration/#dgs-metrics-graphql-dgs-spring-boot-micrometer","title":"DGS Metrics: graphql-dgs-spring-boot-micrometer","text":"Name Type Default Description management.metrics.dgs-graphql.enabled Boolean <code>true</code> Enables DGS' GraphQL metrics, via micrometer. management.metrics.dgs-graphql.instrumentation.enabled Boolean <code>true</code> Enables DGS' GraphQL's base instrumentation; emits <code>gql.query</code>, <code>gql.resolver</code>, and <code>gql.error</code> meters. management.metrics.dgs-graphql.data-loader-instrumentation.enabled Boolean <code>true</code> Enables DGS' instrumentation for DataLoader; emits <code>gql.dataLoader</code> meters. management.metrics.dgs-graphql.tag-customizers.outcome.enabled Boolean <code>true</code> Enables DGS' GraphQL Outcome tag customizer. This adds an OUTCOME tag that is ether SUCCESS or ERROR to the emitted gql meters. management.metrics.dgs-graphql.query-signature.enabled Boolean <code>true</code> Enables DGS' <code>QuerySignatureRepository</code>; if available metrics will be tagged with the <code>gql.query.sig.hash</code>. management.metrics.dgs-graphql.query-signature.caching.enabled Boolean <code>true</code> Enables DGS' <code>QuerySignature</code> caching; if set to false the signature will always be calculated on each request. management.metrics.dgs-graphql.tags.limiter.limit Integer 100 The limit that will apply for this tag. The interpretation of this limit depends on the cardinality limiter itself. management.metrics.dgs-graphql.autotime.percentiles [Double] [] DGS Micrometer Timers percentiles, e.g. <code>[0.95, 0.99, 0.50]</code>. <sup>1</sup> management.metrics.dgs-graphql.autotime.percentiles-histogram Boolean <code>false</code> Enables publishing percentile histograms for the DGS Micrometer Timers. <sup>1</sup> <p>Hint</p> <p>You can configure percentiles, and enable percentile histograms, directly via the per-meter customizations available out of the box in Spring Boot. For example, to enable percentile histograms for all <code>gql.*</code> meters you can set the following property:</p> <pre><code>management.metrics.distribution.percentiles-histogram.gql=true\n</code></pre> <p>For more information please refer to Spring Boot's Per Meter Properties.</p> <ol> <li> <p>Spring Boot's Per Meter Properties can be used to configure percentiles, and histograms, out of the box.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"data-loaders/","title":"Data Loaders (N+1)","text":"<p>Data loaders solve the N+1 problem while loading data.</p>"},{"location":"data-loaders/#the-n1-problem-explained","title":"The N+1 Problem Explained","text":"<p>Say you query for a list of movies, and each movie includes some data about the director of the movie. Also assume that the Movie and Director entities are owned by two different services. In a na\u00efve implementation, to load 50 movies, you would have to call the Director service 50 times: once for each movie. This totals 51 queries: one query to get the list of movies, and 50 queries to get the director data for each movie. This obviously wouldn\u2019t perform very well.</p> <p>It would be much more efficient to create a list of directors to load, and load all of them at once in a single call. This first of all must be supported by the Director service, because that service needs to provide a way to load a list of Directors. The data fetchers in the Movie service need to be smart as well, to take care of batching the requests to the Directors service.</p> <p>This is where data loaders come in.</p>"},{"location":"data-loaders/#what-if-my-service-doesnt-support-loading-in-batches","title":"What If My Service Doesn\u2019t Support Loading in Batches?","text":"<p>What if (in this example) <code>DirectorServiceClient</code> doesn\u2019t provide a method to load a list of directors? What if it only provides a method to load a single director by ID? The same problem applies to REST services as well: what if there\u2019s no endpoint to load multiple directors? Similarly, to load from a database directly, you must write a query to load multiple directors. If such methods are unavailable, the providing service needs to fix this!</p>"},{"location":"data-loaders/#implementing-a-data-loader","title":"Implementing a Data Loader","text":"<p>The easiest way for you to register a data loader is for you to create a class that implements the <code>org.dataloader.BatchLoader</code> or <code>org.dataloader.MappedBatchLoader</code> interface. This interface is parameterized; it requires a type for the key and result of the <code>BatchLoader</code>. For example, if the identifiers for a Director are of type <code>String</code>, you could have a <code>org.dataloader.BatchLoader&lt;String, Director&gt;</code>. You must annotate the class with <code>@DgsDataLoader</code> so that the framework will register the data loader it represents.</p> <p>In order to implement the <code>BatchLoader</code> interface you must implement a <code>CompletionStage&lt;List&lt;V&gt;&gt; load(List&lt;K&gt; keys)</code> method.</p> <p>The following example is a data loader that loads data from an imaginary Director service:</p> <pre><code>package com.netflix.graphql.dgs.example.dataLoader;\n\nimport com.netflix.graphql.dgs.DgsDataLoader;\nimport org.dataloader.BatchLoader;\n\nimport java.util.List;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.CompletionStage;\nimport java.util.stream.Collectors;\n\n@DgsDataLoader(name = \"directors\")\npublic class DirectorsDataLoader implements BatchLoader&lt;String, Director&gt; {\n\n    @Autowired\n    DirectorServiceClient directorServiceClient;\n\n    @Override\n    public CompletionStage&lt;List&lt;Director&gt;&gt; load(List&lt;String&gt; keys) {\n        return CompletableFuture.supplyAsync(() -&gt; directorServiceClient.loadDirectors(keys));\n    }\n}\n</code></pre> <p>The data loader is responsible for loading data for a given list of keys. In this example, it just passes on the list of keys to the backend that owns <code>Director</code> (this could for example be a [gRPC] service). However, you might also write such a service so that it loads data from a database. Although this example registers a data loader, nobody will use that data loader until you implement a data fetcher that uses it.</p>"},{"location":"data-loaders/#implementing-a-data-loader-with-try","title":"Implementing a Data Loader With Try","text":"<p>If you want to handle exceptions during fetching of partial results, you can return a list of <code>Try</code> objects from the loader.  The query result will contain partial results for the successful calls and  an error for the exception case.</p> <pre><code>package com.netflix.graphql.dgs.example.dataLoader;\n\nimport com.netflix.graphql.dgs.DgsDataLoader;\nimport org.dataloader.BatchLoader;\nimport org.dataloader.Try;\nimport org.springframework.beans.factory.annotation.Autowired;\n\nimport java.util.List;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.CompletionStage;\nimport java.util.stream.Collectors;\n\n@DgsDataLoader(name = \"directors\")\npublic class DirectorsDataLoader implements BatchLoader&lt;String, Try&lt;Director&gt;&gt; {\n\n    @Autowired\n    private DirectorServiceClient directorServiceClient;\n\n    @Override\n    public CompletionStage&lt;List&lt;Try&lt;Director&gt;&gt;&gt; load(List&lt;String&gt; keys) {\n        return CompletableFuture.supplyAsync(() -&gt; keys.stream()\n                .map(key -&gt; Try.tryCall(() -&gt; directorServiceClient.loadDirectors(keys)))\n                .collect(Collectors.toList()));\n    }\n\n}\n</code></pre>"},{"location":"data-loaders/#provide-as-lambda","title":"Provide as Lambda","text":"<p>Because <code>BatchLoader</code> is a functional interface (an interface with only a single method), you can also provide it as a lambda expression. Technically this is exactly the same as providing a class; it\u2019s really just another way of writing it:</p> <pre><code>@DgsComponent\npublic class ExampleBatchLoaderFromField {\n    @DgsDataLoader(name = \"directors\")\n    public BatchLoader&lt;String, Director&gt; directorBatchLoader = keys -&gt; CompletableFuture.supplyAsync(() -&gt; directorServiceClient.loadDirectors(keys));\n}\n</code></pre>"},{"location":"data-loaders/#mappedbatchloader","title":"MappedBatchLoader","text":"<p>The <code>BatchLoader</code> interface creates a <code>List</code> of values for a <code>List</code> of keys. You can also use the <code>MappedBatchLoader</code> which creates a <code>Map</code> of key/values for a <code>Set</code> of values. The latter is a better choice if you do not expect all keys to have a value. You register a <code>MappedBatchLoader</code> in the same way as you register a <code>BatchLoader</code>:</p> <pre><code>@DgsDataLoader(name = \"directors\")\npublic class DirectorsDataLoader implements MappedBatchLoader&lt;String, Director&gt; {\n\n    @Autowired\n    DirectorServiceClient directorServiceClient;\n\n    @Override\n    public CompletionStage&lt;Map&lt;String, Director&gt;&gt; load(Set&lt;String&gt; keys) {\n        return CompletableFuture.supplyAsync(() -&gt; directorServiceClient.loadDirectors(keys));\n    }\n}\n</code></pre>"},{"location":"data-loaders/#using-a-data-loader","title":"Using a Data Loader","text":"<p>The following is an example of a data fetcher that uses a data loader:</p> <pre><code>@DgsComponent\npublic class DirectorDataFetcher {\n\n    @DgsData(parentType = \"Movie\", field = \"director\")\n    public CompletableFuture&lt;Director&gt; director(DataFetchingEnvironment dfe) {\n\n        DataLoader&lt;String, Director&gt; dataLoader = dfe.getDataLoader(\"directors\");\n        String id = dfe.getArgument(\"directorId\");\n\n        return dataLoader.load(id);\n    }\n}\n</code></pre> <p>The code above is mostly just a regular data fetcher. However, instead of actually loading the data from another service or database, it uses the data loader to do so. You can retrieve a data loader from the <code>DataFetchingEnvironment</code> with its <code>getDataLoader()</code> method. This requires  you to pass the name of the data loader as a string. The other change to the data fetcher is that it returns a <code>CompletableFuture</code> instead of the actual type you\u2019re loading. This enables the framework to do work asynchronously, and is a requirement for batching.</p>"},{"location":"data-loaders/#using-the-dgsdatafetchingenvironment","title":"Using the DgsDataFetchingEnvironment","text":"<p>You can also get the data loader in a type-safe way by using our custom <code>DgsDataFetchingEnvironment</code>, which is an enhanced version of <code>DataFetchingEnvironment</code> in <code>graphql-java</code>, and provides <code>getDataLoader()</code> using the classname.</p> <pre><code>@DgsComponent\npublic class DirectorDataFetcher {\n\n    @DgsData(parentType = \"Movie\", field = \"director\")\n    public CompletableFuture&lt;Director&gt; director(DgsDataFetchingEnvironment dfe) {\n\n        DataLoader&lt;String, Director&gt; dataLoader = dfe.getDataLoader(DirectorsDataLoader.class);\n        String id = dfe.getArgument(\"directorId\");\n\n        return dataLoader.load(id);\n    }\n}\n</code></pre> <p>The same works if you have <code>@DgsDataLoader</code> defined as a lambda instead of on a class as shown here. If you have multiple <code>@DgsDataLoader</code> lambdas defined as fields in the same class, you won't be able to use this feature. It is recommended that you use <code>getDataLoader()</code> with the loader name passed as a string in such cases.</p> <p>Note that there is no logic present about how batching works exactly; this is all handled by the framework! The framework will recognize that many directors need to be loaded when many movies are loaded, batch up all the calls to the data loader, and call the data loader with a list of IDs instead of a single ID. The data loader implemented above already knows how to handle a list of IDs, and that way it avoids the N+1 problem.</p>"},{"location":"data-loaders/#using-spring-features-such-as-securitycontextholder-inside-a-completablefuture","title":"Using Spring Features such as SecurityContextHolder inside a CompletableFuture","text":"<p>All the <code>*async*</code> methods on <code>CompletableFuture</code> allow you to provide an <code>Executor</code>. If provided, that stage of the <code>CompletableFuture</code> will run on a Thread represented by that <code>Executor</code>. If no <code>Executor</code> is provided, the stage runs on a Thread from the fork/join pool.</p> <p>In Spring WebMVC it's common to store request information such as SSO data on <code>ThreadLocal</code>, which doesn't automatically become available if you run on a different thread, in this case a thread from the <code>Executor</code>. Spring Security uses <code>ThreadLocal</code> for example to store its <code>SecurityContext</code>.</p> <p>While creating and extending an <code>Executor</code> with the correct context propagation goes beyond the scope of this documentation, a good place to start is the <code>DelegatingSecurityContextExecutor</code> from Spring Security. A relevant howto guide is available here. </p>"},{"location":"data-loaders/#scheduled-data-loaders-with-dispatch-predicates","title":"Scheduled Data Loaders with Dispatch Predicates","text":"<p>The framework now supports setting up a Dispatch Predicate on a per data loader basis.  This allows you to configure when the batch is dispatched based on queue depth or time.  Note that the predicate will be applied for the data loader that you set the predicate up for and not across all data loaders. Here is how you can set up a <code>DispatchPredicate</code> for an example data loader: <pre><code>@DgsDataLoader(name = \"messagesWithScheduledDispatch\")\npublic class MessageDataLoaderWithDispatchPredicate implements BatchLoader&lt;String, String&gt; {\n    @DgsDispatchPredicate\n    DispatchPredicate pred = DispatchPredicate.dispatchIfLongerThan(Duration.ofSeconds(2));\n\n    @Override\n    public CompletionStage&lt;List&lt;String&gt;&gt; load(List&lt;String&gt; keys) {\n        return CompletableFuture.supplyAsync(() -&gt; keys.stream().map(key -&gt; \"hello, \" + key + \"!\").collect(Collectors.toList()));\n    }\n}\n</code></pre></p> <p>In addition to defining the <code>load</code> method for the data loader class, you can specify a DispatchPredicate annotated with <code>@DgsDispatchPredicate</code> to apply that for the specific data loader.</p>"},{"location":"data-loaders/#chaining-data-loaders","title":"Chaining Data Loaders","text":"<p>Often times, you may want to chain data loaders as shown in the following example: <pre><code> @DgsData(parentType = \"Query\", field = \"messageFromBatchLoader\")\n    public CompletableFuture&lt;String&gt; getMessage(DataFetchingEnvironment env) {\n        DataLoader&lt;String, String&gt; dataLoader = env.getDataLoader(\"messages\");\n        DataLoader&lt;String, String&gt; dataLoaderB = env.getDataLoader(\"greetings\");\n        return dataLoader.load(\"a\").thenCompose(key -&gt; {\n            CompletableFuture&lt;String&gt; loadA = dataLoaderB.load(key);\n            return loadA;\n        });\n    }\n</code></pre></p> <p>The above implementation will hang indefinitely in the call to the second data loader in the chain. The <code>java-dataloader</code> library already handles dispatching of data loader calls at each level of the query.  However, when there is yet another load call to the same or different dataloader, who would call dispatch on this data loader? Since these return Completable Futures, it is not easy to determine when the dispatch should be triggered. To handle this, we need to explicitly call dispatch on the second data loader (shown below) , since there is nothing else to trigger the dispatch when load is invoked on <code>dataloaderB</code>. <pre><code> @DgsData(parentType = \"Query\", field = \"messageFromBatchLoader\")\n    public CompletableFuture&lt;String&gt; getMessage(DataFetchingEnvironment env) {\n        DataLoader&lt;String, String&gt; dataLoader = env.getDataLoader(\"messages\");\n        DataLoader&lt;String, String&gt; dataLoaderB = env.getDataLoader(\"greetings\");\n        return dataLoader.load(\"a\").thenCompose(key -&gt; {\n            CompletableFuture&lt;String&gt; loadA = dataLoaderB.load(key);\n            // Manually call dispatch\n            dataLoaderB.dispatch();\n            return loadA;\n        });\n    }\n</code></pre> This is expected according to the documented behavior here. However, this can result in suboptimal batching for <code>dataloaderB</code>, with a batch size of 1.</p> <p>The <code>v8.1.0</code> release introduces a new feature to enable ticker mode available in the <code>java-dataloader</code> library This allows you to schedule the dispatch checks instead of manually calling dispatch in your data loaders. By default, the checks will occur every 10ms but can be configured via <code>dgs.graphql.dataloader.scheduleDuration</code>. To enable ticker mode in the DGS framework, you can set <code>dgs.graphql.dataloader.ticker-mode-enabled</code> to true. In addition, you can also specify dispatch predicates per dataloader vi a<code>@DgsDispatchPredicate</code>to better control the batching.</p> <p>With ticker mode enabled, you can eliminate calls to manually dispatch and rely on the scheduler to periodically check and dispatch any batches as needed. This should result in better batching behavior overall. Thus the following implementation that would hang earlier will now work as expected without the additional call to dispatch: <pre><code> @DgsData(parentType = \"Query\", field = \"messageFromBatchLoader\")\n    public CompletableFuture&lt;String&gt; getMessage(DataFetchingEnvironment env) {\n        DataLoader&lt;String, String&gt; dataLoader = env.getDataLoader(\"messages\");\n        DataLoader&lt;String, String&gt; dataLoaderB = env.getDataLoader(\"greetings\");\n        return dataLoader.load(\"a\").thenCompose(key -&gt; {\n            CompletableFuture&lt;String&gt; loadA = dataLoaderB.load(key);\n            return loadA;\n        });\n    }\n</code></pre></p>"},{"location":"data-loaders/#thread-pool-optimization","title":"Thread Pool Optimization","text":"<p>Using <code>supplyAsync()</code> without a second argument will cause the main work of a data loader to run on a common thread pool  shared with most other async operations in your application. If that data loader is responsible for calling a slow service and/or subject to heavy load, the common thread pool could become fully saturated. In the worst case, this could result in application \"freezes\" as each data loader in the application awaits a free thread from the fixed-size common pool. </p> <p>To account for this, IO-bound data loaders should instead maintain their own dedicated thread pool rather than use the common pool. When choosing a thread pool, it's recommended to review the options under the Executors Javadoc, but a safe default for IO bound workloads is usually <code>Executors.newCachedThreadPool()</code>. As opposed to the fixed-size common thread pool, <code>Executors.newCachedThreadPool()</code> will  create new threads on-demand if all previously-created threads are saturated, but still prefers thread re-use when possible.</p> <pre><code>@Configuration\npublic class SlowDataLoaderConfiguration {\n    @Bean(name = \"SlowDataLoaderThreadPool\")\n    Executor slowDataLoaderExecutor() {\n        return Executors.newCachedThreadPool();\n    }\n}\n</code></pre> <p>Individual Bean names combined with <code>@Qualifier</code> annotations will result in the proper executor being autowired to the corresponding data loader.</p> <pre><code>@DgsDataLoader(name = \"slow\")\npublic class SlowDataLoader implements MappedBatchLoader&lt;String, Snail&gt; {\n\n    @Autowired\n    @Qualifier(\"SlowDataLoaderThreadPool\")\n    Executor dedicatedExecutor;\n\n    @Autowired\n    DirectorServiceClient directorServiceClient;\n\n    @Override\n    public CompletionStage&lt;Map&lt;String, Snail&gt;&gt; load(Set&lt;String&gt; keys) {\n        // This slow operation will now run on a dedicated thread pool instead of the common pool\n        return CompletableFuture.supplyAsync(() -&gt; directorServiceClient.loadSlowData(keys), dedicatedExecutor);\n    }\n}\n</code></pre> <p>Note that a custom executor will not carry Spring Security context automatically.  Further documentation on passing Spring Security context between threads can be found in the Spring Security Concurrency docs.</p>"},{"location":"data-loaders/#caching","title":"Caching","text":"<p>Batching is the most important aspect of preventing N+1 problems. Data loaders also support caching, however. If the same key is loaded multiple times, it will only be loaded once. For example, if a list of movies is loaded, and some movies are directed by the same director, the director data will only be retrieved once.</p> <p>Caching is Disabled by Default in DGS 1</p> <p>Version 1 of the DGS framework disables caching by default, but you can switch it on in the <code>@DgsDataLoader</code> annotation:</p> <pre><code>@DgsDataLoader(name = \"directors\", caching=true)\nclass DirectorsBatchLoader implements BatchLoader&lt;String, Director&gt; {}\n</code></pre> <p>You do not need to make this change in version 2 of the DGS framework, because that version enables caching by default.</p>"},{"location":"data-loaders/#batch-size","title":"Batch Size","text":"<p>Sometimes it\u2019s possible to load multiple items at once, but to a certain limit. When loading from a database for example, an <code>IN</code> query could be used, but maybe with the limitation of a maximum number of IDs to provide. The <code>@DgsDataLoader</code> has a <code>maxBatchSize</code> annotation that you can use to configure this behavior. By default it does not specify a maximum batch size.</p>"},{"location":"data-loaders/#data-loader-scope","title":"Data Loader Scope","text":"<p>Data loaders are wired up to only span a single request. This is what most use cases require. Spanning multiple requests can introduce difficult-to-debug issues.</p>"},{"location":"datafetching/","title":"Data fetching","text":"<p>In the getting started guide we introduced the <code>@DgsQuery</code> annotation, which you use to create a data fetcher. In this section, we look at some of the finer details of datafetchers.</p>"},{"location":"datafetching/#the-dgsdata-dgsquery-dgsmutation-and-dgssubscription-annotations","title":"The @DgsData, @DgsQuery, @DgsMutation and @DgsSubscription Annotations","text":"<p>You use the <code>@DgsData</code> annotation on a Java/Kotlin method to make that method a datafetcher. The method must be in a <code>@DgsComponent</code> class. The <code>@DgsData</code> annotation has two parameters:</p> Parameter Description <code>parentType</code> This is the type that contains the field. <code>field</code> The field that the datafetcher is responsible for <p>For example, we have the following schema.</p> <p><pre><code>type Query {\n   shows: [Show]\n}\n\ntype Show {\n  title: String\n  actors: [Actor]\n}\n</code></pre> We can implement this schema with a single datafetcher.</p> <pre><code>@DgsComponent\npublic class ShowDataFetcher {\n\n   @DgsData(parentType = \"Query\", field = \"shows\")\n   public List&lt;Show&gt; shows() {\n\n       //Load shows from a database and return the list of Show objects\n       return shows;\n   }\n}\n</code></pre> <p>If the <code>field</code> parameter is not set, the method name will be used as the field name. The <code>@DgsQuery</code>, <code>@DgsMutation</code> and <code>@DgsSubscription</code> annotations are shorthands to define datafetchers on the <code>Query</code>, <code>Mutation</code> and <code>Subscription</code> types. The following definitions are all equivalent.</p> <pre><code>@DgsData(parentType = \"Query\", field = \"shows\")\npublic List&lt;Show&gt; shows() { .... }\n\n// The \"field\" argument is omitted. It uses the method name as the field name.\n@DgsData(parentType = \"Query\")\npublic List&lt;Show&gt; shows() { .... }\n\n// The parentType is \"Query\", the field name is derived from the method name.\n@DgsQuery\npublic List&lt;Show&gt; shows() { .... }\n\n// The parentType is \"Query\", the field name is explicitly specified.\n@DgsQuery(field = \"shows\")\npublic List&lt;Show&gt; shows() { .... }\n</code></pre> <p>Notice how a datafetcher can return complex objects or lists of objects. You don't have to create a separate datafetcher for each field. The framework will take care of only returning the fields that are specified in the query. For example, if a user queries:</p> <p><pre><code>{\n    shows {\n        title\n    }\n}\n</code></pre> Although we're returning Show objects, which in the example contains both a <code>title</code> and an <code>actors</code> field, the <code>actors</code> field gets stripped off before the response gets sent back.</p>"},{"location":"datafetching/#child-datafetchers","title":"Child Datafetchers","text":"<p>The previous example assumed that you could load a list of <code>Show</code> objects from your database with a single query. It wouldn't matter which fields the user included in the GraphQL query; the cost of loading the shows would be the same. What if there is an extra cost to specific fields? For example, what if loading actors for a show requires an extra query? It would be wasteful to run the extra query to load actors if the <code>actors</code> field doesn't get returned to the user.</p> <p>In such scenarios, it's better to create a separate datafetcher for the expensive field.</p> <p><pre><code>@DgsQuery\npublic List&lt;Show&gt; shows() {\n\n    //Load shows, which doesn't include \"actors\"\n    return shows;\n}\n\n@DgsData(parentType = \"Show\", field = \"actors\")\npublic List&lt;Actor&gt; actors(DgsDataFetchingEnvironment dfe) {\n\n   Show show = dfe.getSource();\n\n   return actorsService.forShow(show.getId());\n}\n</code></pre> The <code>actors</code> datafetcher only gets executed when the <code>actors</code> field is included in the query. The <code>actors</code> datafetcher also introduces a new concept; the <code>DgsDataFetchingEnvironment</code>. The <code>DgsDataFetchingEnvironment</code> gives access to the <code>context</code>, the query itself, data loaders, and the <code>source</code> object. The source object is the object that contains the field. For this example, the source is the <code>Show</code> object, which you can use to get the show's identifier to use in the query for actors.</p> <p>Do note that the <code>shows</code> datafetcher is returning a list of <code>Show</code>, while the <code>actors</code> datafetcher fetches the actors for a single show. The framework executes the <code>actors</code> datafetcher for each <code>Show</code> returned by the <code>shows</code> datafetcher. If the actors get loaded from a database, this would now cause an N+1 problem. To solve the N+1 problem, you use data loaders.</p> <p>Note: There are more complex scenarios with nested datafetchers, and ways to pass context between related datafetchers. See the nested datafetchers guide for more advanced use-cases.</p>"},{"location":"datafetching/#multiple-dgsdata-annotations-via-dgsdatalist","title":"Multiple @DgsData Annotations Via @DgsData.List","text":"<p>Since v4.6.0, methods can be annotated with multiple <code>@DgsData</code> annotations. Effectively you can resolve multiple GraphQL Type fields via the same method implementation. To do so you will have to leverage the <code>@DgsData.List</code> annotation, for example:</p> JavaKotlin <pre><code>@DgsData.List({\n    @DgsData(parentType = \"Query\", field = \"movies\"),\n    @DgsData(parentType = \"Query\", field = \"shows\")\n})\n</code></pre> <pre><code>@DgsData.List(\n    DgsData(parentType = \"Query\", field = \"movies\"),\n    DgsData(parentType = \"Query\", field = \"shows\")\n)\n</code></pre> <p>Tip</p> <p>Both <code>@DgsQuery</code> and <code>@DgsMutation</code> can't be defined multiple times in a single method. Please use <code>@DgsData</code> instead and explicitly define the <code>parentType</code> to match either <code>Query</code> or <code>Mutation</code>.</p>"},{"location":"datafetching/#using-inputargument","title":"Using @InputArgument","text":"<p>It's very common for GraphQL queries to have one or more input arguments. According to the GraphQL specification, an input argument can be:</p> <ul> <li>An input type</li> <li>A scalar</li> <li>An enum</li> </ul> <p>Other types, such as output types, unions and interfaces, are not allowed as input arguments.</p> <p>You can get input arguments as method arguments in a datafetcher method using the <code>@InputArgument</code> annotation.</p> <pre><code>type Query {\n    shows(title: String, filter: ShowFilter): [Show]\n}\n\ninput ShowFilter {\n   director: String\n   genre: ShowGenre\n}\n\nenum ShowGenre {\n   commedy, action, horror\n}\n</code></pre> <p>We can write a datafetcher with the following signature: <pre><code>@DgsQuery\npublic List&lt;Show&gt; shows(@InputArgument String title, @InputArgument ShowFilter filter)\n</code></pre></p> <p>The <code>@InputArgument</code> annotation will use the name of the method argument to match it with the name of an input argument sent in the query. Optionally we can specify the <code>name</code> argument in the <code>@InputArgument</code> annotation, if the argument name doesn't match the method argument name.</p> <p>The framework converts input arguments to Java/Kotlin types. The first step for converting input arguments is <code>graphql-java</code> using scalar implementations to convert raw string input into whatever type the scalar represents. A GraphQL <code>Int</code> becomes an <code>Integer</code> in Java, a formatted date string becomes a <code>LocalDateTime</code> (depending on the scalars you're using!), lists become an instance of <code>java.util.ArrayList</code>. Input objects are represented as a <code>Map&lt;String, Object&gt;</code> in <code>graphql-java</code>.</p> <p>The next step is the DGS Framework converting the <code>Map&lt;String, Object&gt;</code> to the Java/Kotlin classes that you use for the <code>@InputArgument</code>. For Java classes, the framework creates a new instance of the class using the no-arg constructor. This implies that a no-arg constructor is required. It then sets each field of the instance to the input argument values.</p> <p>For Kotlin Data classes, the instance can only be created by passing in all arguments in the constructor. This means you have to make sure to make fields optional in the data class when the fields are optional in the GraphQL schema!</p> <p>If you're using the Codegen plugin (you really should!), the input types will work perfectly out of the box.</p> <p>Input argument conversion isn't JSON</p> <p>It's easy to confuse the conversion described above with JSON deserialization as you are familiar with in libraries such as Jackson. Although it looks similar, the mechanisms are completely unrelated.  Input arguments aren't JSON, and the Scalar mechanism is really the core of how conversion works. This also means that Jackson annotations on Java/Kotlin types are not used at all. </p> <p>Defining scalars, and scalars in codegen</p> <p>@InputArgument is designed to work well with scalars. More information about defining custom scalars in the framework can be found here. For a scalar you typically either create a class representing the value, or use an existing type.  Such types need to be mapped in Codegen configuration so that they don't (incorrectly) get generated.</p>"},{"location":"datafetching/#nullability-in-kotlin-for-input-arguments","title":"Nullability in Kotlin for Input Arguments","text":"<p>If you're using Kotlin you must consider if an input type is nullable. If the schema defines an input argument as nullable, the code must reflect this by using a nullable type. If a non-nullable type receives a null value, Kotlin will throw an exception.</p> <p>For example:</p> <p><pre><code># name is a nullable input argument\nhello(name: String): String\n</code></pre> You must write the datafetcher function as: <pre><code>fun hello(@InputArgument hello: String?)\n</code></pre></p> <p>In Java you don't have to worry about this, types can always be null assuming you are using boxed types.  Generally, we recommend using boxed types instead of unboxed if you are expecting null input. You do need to null check in your datafetching code for handling possible null inputs. If using unboxed types, null input will result in exceptions.</p>"},{"location":"datafetching/#using-inputargument-with-lists","title":"Using @InputArgument with lists","text":"<p>An input argument can also be a list. If the list type is an input type, you must specify the type explicitly in the <code>@InputArgument</code> annotation.</p> <pre><code>type Query {\n    hello(people:[Person]): String\n}\n</code></pre> <pre><code>public String hello(@InputArgument(collectionType = Person.class) List&lt;Person&gt; people)\n</code></pre>"},{"location":"datafetching/#using-optional-with-inputargument","title":"Using Optional with @InputArgument","text":"<p>Input arguments are often defined as optional in schemas. Your datafetcher code needs to null-check arguments to check if they were provided. Instead of null-checks you can wrap an input argument in an Optional.</p> <pre><code>public List&lt;Show&gt; shows(@InputArgument(collectionType = ShowFilter.class) Optional&lt;ShowFilter&gt; filter)\n</code></pre> <p>You do need to provide the type in the <code>collectionType</code> argument when using complex types, similar to using lists. If the argument is not provided, the value will be <code>Optional.empty()</code>. It's a matter of preference to use <code>Optional</code> or not.</p>"},{"location":"datafetching/#codegen-constants","title":"Codegen Constants","text":"<p>In the examples of <code>@DgsData</code> so far, we used string values for the <code>parentType</code> and <code>field</code> arguments. If you are using code generation you can instead use generated constants. Codegen creates a <code>DgsConstants</code> class with constants for each type and field in your schema. Using this we can write a datafetcher as follows:</p> <pre><code>type Query {\n    shows: [Show]\n}\n</code></pre> <pre><code>@DgsData(parentType = DgsConstants.QUERY_TYPE, field = DgsConstants.QUERY.Shows)\npublic List&lt;Show&gt; shows() {}\n</code></pre> <p>The benefit of using constants is that you can detect issues between your schema and datafetchers at compile time.</p>"},{"location":"datafetching/#requestheader-requestparam-and-cookievalue","title":"@RequestHeader, @RequestParam and @CookieValue","text":"<p>Sometimes you need to evaluate HTTP headers, or other elements of the request, in a datafetcher. You can easily get an HTTP header value by using the <code>@RequestHeader</code> annotation. The <code>@RequestHeader</code> annotation is the same annotation as used in Spring WebMVC. Note that you will need to ensure you are using the webmvc stack for this functionality.</p> <pre><code>@DgsQuery\npublic String hello(@RequestHeader String host)\n</code></pre> <p>Technically, headers are lists of values. If multiple values are set, you can retrieve them as a list by using a List as your argument type. Otherwise, the values are concatenated to a single String. Similar to <code>@InputArgument</code> it's possible to wrap a header or parameter in an <code>Optional</code>.</p> <p>Similarly, you can get request parameters using <code>@RequestParam</code>. Both <code>@RequestHeader</code> and <code>@RequestParam</code> support a <code>defaultValue</code> and <code>required</code> argument. If a <code>@RequestHeader</code> or <code>@RequestParam</code> is <code>required</code>, doesn't have a <code>defaultValue</code> and isn't provided, a <code>DgsInvalidInputArgumentException</code> is thrown.</p> <p>To easily get access to cookie values you can use Spring's <code>@CookieValue</code> annotation.</p> <pre><code>@DgsQuery\npublic String usingCookieWithDefault(@CookieValue(defaultValue = \"defaultvalue\") myCookie: String) {\n    return myCookie\n}\n</code></pre> <p><code>@CookieValue</code> supports a <code>defaultValue</code> and the <code>required</code> argument. You can also use an <code>Optional&lt;String&gt;</code> for a <code>@CookieValue</code> if it's not required.</p>"},{"location":"datafetching/#using-dgsrequestdata-to-get-access-to-the-request-object","title":"Using DgsRequestData to get access to the request object","text":"<p>You can get access to the request object, representing the HTTP request itself, as well. It's stored on the <code>DgsContext</code> object in the <code>DgsDataFetchingEnvironment</code>. </p> <p>Because Spring WebMVC and Spring Webflux use different types to represent the request, the <code>DgsRequestData</code> is different depending on what environment (WebMVC/WebFlux) you're running in. The <code>DgsRequestData</code> interface only gives access to the request headers and the <code>extensions</code>. To get the actual request object, you need to cast the <code>DgsRequestData</code> to the correct implementation. This is either <code>DgsWebMvcRequestData</code> or <code>DgsReactiveRequestData</code>. Let's use this in an example to set a cookie, which is done through the response object.</p> <p>Let's look at a WebMVC example first. From the <code>DgsWebMvcRequestData</code> you can get the <code>WebRequest</code>, which can be further cast to a <code>ServletWebRequest</code>.</p> <pre><code>@DgsQuery\n@DgsMutation\npublic String updateCookie(@InputArgument String value, DgsDataFetchingEnvironment dfe) {\n    DgsWebMvcRequestData requestData = (DgsWebMvcRequestData) dfe.getDgsContext().getRequestData();\n    ServletWebRequest webRequest = (ServletWebRequest) requestData.getWebRequest();\n    javax.servlet.http.Cookie cookie = new javax.servlet.http.Cookie(\"mydgscookie\", value);\n    webRequest.getResponse().addCookie(cookie);\n\n    return value;\n}\n</code></pre> <p>Now let's try the same with WebFlux. <code>DgsRequestData</code> is now an instance of <code>DgsReactiveRequestData</code>, which gives access to the <code>ServerRequest</code>.</p> <pre><code>@DgsMutation\npublic String updateCookie(@InputArgument String value, DgsDataFetchingEnvironment dfe) {\n    DgsReactiveRequestData requestData = (DgsReactiveRequestData) dfe.getDgsContext().getRequestData();\n    ServerRequest serverRequest = requestData.getServerRequest();\n\n    serverRequest.exchange().getResponse()\n            .addCookie(ResponseCookie.from(\"mydgscookie\", \"webfluxupdated\").build());\n    return value;\n}\n</code></pre>"},{"location":"datafetching/#using-data-fetcher-context","title":"Using data fetcher context","text":"<p>The <code>DgsRequestData</code> object described in the previous section is part of the data fetching context. The DGS Framework adds the <code>DgsRequestData</code> to the data fetching context. You can also add your own data to the context, for use in data fetchers. The context is initialized per request, before query execution starts.</p> <p>You can customize the context and add your own data by creating a <code>DgsCustomContextBuilder</code>.</p> <pre><code>@Component\npublic class MyContextBuilder implements DgsCustomContextBuilder&lt;MyContext&gt; {\n    @Override\n    public MyContext build() {\n        return new MyContext();\n    }\n}\n\npublic class MyContext {\n    private final String customState = \"Custom state!\";\n\n    public String getCustomState() {\n        return customState;\n    }\n}\n</code></pre> <p>If you require access to the request, e.g. to read HTTP headers, you can implement the <code>DgsCustomContextBuilderWithRequest</code> interface instead.</p> <pre><code>@Component\npublic class MyContextBuilder implements DgsCustomContextBuilderWithRequest&lt;MyContext&gt; {\n    @Override\n    public MyContext build(Map&lt;String, Object&gt; extensions, HttpHeaders headers, WebRequest webRequest) {\n        //e.g. you can now read headers to set up context\n        return new MyContext();\n    }\n}\n</code></pre> <p>A data fetcher can now retrieve the context by calling the <code>getCustomContext()</code> method:</p> <pre><code>@DgsData(parentType = \"Query\", field = \"withContext\")\npublic String withContext(DataFetchingEnvironment dfe) {\n    MyContext customContext = DgsContext.getCustomContext(dfe);\n    return customContext.getCustomState();\n}\n</code></pre> <p>Similarly, custom context can be used in a DataLoader.</p> <pre><code>@DgsDataLoader(name = \"exampleLoaderWithContext\")\npublic class ExampleLoaderWithContext implements BatchLoaderWithContext&lt;String, String&gt; {\n    @Override\n    public CompletionStage&lt;List&lt;String&gt;&gt; load(List&lt;String&gt; keys, BatchLoaderEnvironment environment) {\n\n        MyContext context = DgsContext.getCustomContext(environment);\n\n        return CompletableFuture.supplyAsync(() -&gt; keys.stream().map(key -&gt; context.getCustomState() + \" \" + key).collect(Collectors.toList()));\n    }\n}\n</code></pre>"},{"location":"datafetching/#data-fetcher-threading-model","title":"Data fetcher threading model","text":"<p>The threading model for data fetchers, concurrent behavior and the use of JDK 21 Virtual threads is further explained here.</p>"},{"location":"error-handling/","title":"Error Handling","text":"<p>It is common in GraphQL to support error reporting by adding an <code>errors</code> block to a response. Responses can contain both data and errors, for example when some fields where resolved successfully, but other fields had errors. A field with an error is set to null, and an error is added to the <code>errors</code> block.</p> <p>The DGS framework has an exception handler out-of-the-box that works according to the specification described in the <code>Error Specification</code> section on this page. This exception handler handles exceptions from data fetchers. Any <code>RuntimeException</code> is translated to a <code>GraphQLError</code> of type <code>INTERNAL</code>. For some specific exception types, a more specific GraphQL error type is used.</p> Exception type GraphQL error type description <code>AccessDeniedException</code> <code>PERMISSION_DENIED</code> When a <code>@Secured</code> check fails <code>DgsEntityNotFoundException</code> <code>NOT_FOUND</code> Thrown by the developer when a requested entity (e.g. based on query parameters) isn't found"},{"location":"error-handling/#handling-exceptions","title":"Handling exceptions","text":"<p>It can be useful to map application specific exceptions to meaningful exceptions back to the client. The framework provides two different mechanisms to achieve this.</p>"},{"location":"error-handling/#mapping-custom-exceptions-with-controlleradvice","title":"Mapping custom exceptions with @ControllerAdvice","text":"<p>The easiest way to manually map exceptions is to use <code>@ControllerAdvice</code> from Spring for GraphQL. Annotate a class with <code>@ControllerAdvice</code>, and add a method annotated with <code>@GraphQlExceptionHandler</code> for each type of exception you want to handle. The method must take an argument of the type of exception it should handle, and must return <code>GraphQLError</code>. If no matching method is found, the framework falls back to the built-in error handling from the framework.</p> <pre><code>@ControllerAdvice\npublic class ControllerExceptionHandler {\n    @GraphQlExceptionHandler\n    public GraphQLError handle(IllegalArgumentException ex) {\n        return GraphQLError.newError().errorType(ErrorType.BAD_REQUEST).message(\"Handled an IllegalArgumentException!\").build();\n    }\n}\n</code></pre>"},{"location":"error-handling/#mapping-custom-exceptions-by-implementing-datafetcherexceptionhandler","title":"Mapping custom exceptions by implementing DataFetcherExceptionHandler","text":"<p>You can also register a component of type <code>DataFetcherExceptionHandler</code>, which is an interface from graphql-java. Make sure to delegate to the <code>DefaultDataFetcherExceptionHandler</code> class, this is the default exception handler of the framework. If you don't delegate to this class, you lose the framework's built-in exception handler.</p> <p>The following is an example of a custom exception handler implementation.</p> <pre><code>@Component\npublic class MyExceptionHandler implements DataFetcherExceptionHandler {\n\n   @Override\n   public CompletableFuture&lt;DataFetcherExceptionHandlerResult&gt; handleException(DataFetcherExceptionHandlerParameters handlerParameters) {\n      if (handlerParameters.getException() instanceof RuntimeException) {\n         Map&lt;String, Object&gt; debugInfo = new HashMap&lt;&gt;();\n         debugInfo.put(\"somefield\", \"somevalue\");\n\n         GraphQLError graphqlError = TypedGraphQLError.newInternalErrorBuilder()\n                 .message(\"This custom thing went wrong!\")\n                 .debugInfo(debugInfo)\n                 .path(handlerParameters.getPath()).build();\n\n         DataFetcherExceptionHandlerResult result = DataFetcherExceptionHandlerResult.newResult()\n                 .error(graphqlError)\n                 .build();\n\n         return CompletableFuture.completedFuture(result);\n      } else {\n         return new DefaultDataFetcherExceptionHandler().handleException(handlerParameters);\n      }\n   }\n}\n</code></pre> <p>The following data fetcher throws <code>MyException</code>.</p> <pre><code>@DgsComponent\npublic class HelloDataFetcher {\n    @DgsData(parentType = \"Query\", field = \"hello\")\n    @DgsEnableDataFetcherInstrumentation(false)\n    public String hello(DataFetchingEnvironment dfe) {\n\n        throw new MyException();\n    }\n}\n</code></pre> <p>Querying the <code>hello</code> field results in the following response.</p> <pre><code>{\n  \"errors\": [\n    {\n      \"message\": \"This custom thing went wrong!\",\n      \"locations\": [],\n      \"path\": [\n        \"hello\"\n      ],\n      \"extensions\": {\n        \"errorType\": \"INTERNAL\",\n        \"debugInfo\": {\n          \"somefield\": \"somevalue\"\n        }\n      }\n    }\n  ],\n  \"data\": {\n    \"hello\": null\n  }\n}\n</code></pre>"},{"location":"error-handling/#error-specification","title":"Error specification","text":"<p>There are two families of errors we typically encounter for GraphQL:</p> <ol> <li>Comprehensive Errors.    These are unexpected errors and do not represent a condition that the end user can be expected to fix.    Errors of this sort are generally applicable to many types and fields.    Such errors appear in the <code>errors</code> array in the GraphQL response.</li> <li>Errors as Data.    These are errors that are informative to the end user (for example: \u201cthis title is not available in your country\u201d or \u201cyour account has been suspended\u201d).    Errors of this sort are typically specific to a particular use case and apply only to certain fields or to a certain subset of fields.    These errors are part of the GraphQL schema.</li> </ol>"},{"location":"error-handling/#the-graphqlerror-interface","title":"The GraphQLError Interface","text":"<p>The GraphQL specification provides minimal guidance on the structure of an error. The only required field is a <code>message</code> String, which has no defined format. In Studio Edge we would like to have a stronger, more expressive contract. Here is the definition we are using:</p> field type description <code>message</code> (non-nullable) <code>String!</code> a string description of the error intended for the developer as a guide to understand and correct the error <code>locations</code> <code>[Location]</code> an array of code locations, where each location is a map with the keys <code>line</code> and <code>column</code>, both natural numbers starting from 1 that describe the beginning of an associated syntax element <code>path</code> <code>[String | Int]</code> if the error is associated with one or more particular fields in the response, this field of the error details the paths of those response fields that experienced the error (this allows clients to identify whether a <code>null</code> result is intentional or caused by a runtime error) <code>extensions</code> <code>TypedError</code> see \u201cThe TypedError Interface\u201d below <pre><code>\"\"\"\nError format as defined in GraphQL Spec\n\"\"\"\ninterface GraphQLError {\n    message: String! // Required by GraphQL Spec\n    locations: [Location] // See GraphQL Spec\n    path: [String | Int] // See GraphQL Spec\n    extensions: TypedError\n}\n</code></pre> <p>See the GraphQL specification: Errors for more information.</p>"},{"location":"error-handling/#the-typederror-interface","title":"The TypedError Interface","text":"<p>Studio Edge defines <code>TypedError</code> as follows:</p> field type description <code>errorType</code> (non-nullable) <code>ErrorType!</code> an enumerated error code that is meant as a fairly coarse characterization of an error, sufficient for client-side branching logic <code>errorDetail</code> <code>ErrorDetail</code> an enumeration that provides more detail about the error, including its specific cause (the elements of this enumeration are subject to change and are not documented here) <code>origin</code> <code>String</code> the name of the source that issued the error (for instance the name of a backend service, DGS, gateway, client library, or client app) <code>debugInfo</code> <code>DebugInfo</code> if the request included a flag indicating that it wanted debug information, this field contains that additional information (such as a stack trace or additional reporting from an upstream service) <code>debugUri</code> <code>String</code> the URI of a page that contains additional information that may be helpful in debugging the error (this could be a generic page for errors of this sort, or a specific page about the particular error instance) <pre><code>interface TypedError {\n    \"\"\"\n    An error code from the ErrorType enumeration.\n    An errorType is a fairly coarse characterization\n    of an error that should be sufficient for client\n    side branching logic.\n    \"\"\"\n    errorType: ErrorType!\n\n    \"\"\"\n    The ErrorDetail is an optional field which will\n    provide more fine grained information on the error\n    condition. This allows the ErrorType enumeration to\n    be small and mostly static so that application branching\n    logic can depend on it. The ErrorDetail provides a\n    more specific cause for the error. This enumeration\n    will be much larger and likely change/grow over time.\n    \"\"\"\n    errorDetail: ErrorDetail\n\n    \"\"\"\n    Indicates the source that issued the error. For example, could\n    be a backend service name, a domain graph service name, or a\n    gateway. In the case of client code throwing the error, this\n    may be a client library name, or the client app name.\n    \"\"\"\n    origin: String\n\n    \"\"\"\n    Optionally provided based on request flag\n    Could include e.g. stacktrace or info from\n    upstream service\n    \"\"\"\n    debugInfo: DebugInfo\n\n    \"\"\"\n    Http URI to a page detailing additional\n    information that could be used to debug\n    the error. This information may be general\n    to the class of error or specific to this\n    particular instance of the error.\n    \"\"\"\n    debugUri: String\n}\n</code></pre>"},{"location":"error-handling/#the-errortype-enumeration","title":"The ErrorType Enumeration","text":"<p>The following table shows the available <code>ErrorType</code> <code>enum</code> values:</p> type description HTTP analog <code>BAD_REQUEST</code> This indicates a problem with the request. Retrying the same request is not likely to succeed. An example would be a query or argument that cannot be deserialized. 400 Bad Request <code>FAILED_PRECONDITION</code> The operation was rejected because the system is not in a state required for the operation\u2019s execution. For example, the directory to be deleted is non-empty, an <code>rmdir</code> operation is applied to a non-directory, etc. Use <code>UNAVAILABLE</code> instead if the client can retry just the failing call without waiting for the system state to be explicitly fixed. 400 Bad Request, or 500 Internal Server Error <code>INTERNAL</code> This indicates that an unexpected internal error was encountered: some invariants expected by the underlying system have been broken. This error code is reserved for serious errors. 500 Internal Server Error <code>NOT_FOUND</code> This could apply to a resource that has never existed (e.g. bad resource id), or a resource that no longer exists (e.g. cache expired). Note to server developers: if a request is denied for an entire class of users, such as gradual feature rollout or undocumented allowlist, <code>NOT_FOUND</code> may be used. If a request is denied for some users within a class of users, such as user-based access control, <code>PERMISSION_DENIED</code> must be used. 404 Not Found <code>PERMISSION_DENIED</code> This indicates that the requester does not have permission to execute the specified operation. <code>PERMISSION_DENIED</code> must not be used for rejections caused by exhausting some resource or quota. <code>PERMISSION_DENIED</code> must not be used if the caller cannot be identified (use <code>UNAUTHENTICATED</code> instead for those errors). This error does not imply that the request is valid or the requested entity exists or satisfies other pre-conditions. 403 Forbidden <code>UNAUTHENTICATED</code> This indicates that the request does not have valid authentication credentials but the route requires authentication. 401 Unauthorized <code>UNAVAILABLE</code> This indicates that the service is currently unavailable. This is most likely a transient condition, which can be corrected by retrying with a backoff. 503 Unavailable <code>UNKNOWN</code> This error may be returned, for example, when an error code received from another address space belongs to an error space that is not known in this address space. Errors raised by APIs that do not return enough error information may also be converted to this error. If a client sees an <code>errorType</code> that is not known to it, it will be interpreted as <code>UNKNOWN</code>. Unknown errors must not trigger any special behavior. They may be treated by an implementation as being equivalent to <code>INTERNAL</code>. 520 Unknown Error  The HTTP analogs are only rough mappings that are given here to provide a quick conceptual explanation of the semantics of the error by showing their analogs in the HTTP specification."},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#common-patterns","title":"Common Patterns","text":"<p>We have several example applications that demonstrate the use of the DGS framework in a fully working example. Each example has detailed documentation in the corresponding GitHub repository, and is further explained in code comments.</p> <ul> <li>Java DGS example: An implementation of a typical GraphQL service</li> <li>Kotlin DGS example: The same example as above, but implemented in Kotlin</li> <li>Federation examples: A Federated GraphQL example, using Apollo Gateway</li> </ul>"},{"location":"examples/#community-contributions","title":"Community Contributions","text":"<p>We welcome contributions from the community. Check out some of these examples for more patterns.</p> <ul> <li>Authorization using custom directives: Authz using a custom @secured directive</li> </ul>"},{"location":"federation/","title":"Federation","text":"<p>Federation is based on the Federation spec.</p> <p>A DGS is federation-compatible out of the box with the ability to reference and extend federated types.</p> <p>There is more federation documentation available</p> <ul> <li>Read the Federation Spec.</li> <li>Check out Federated Testing to learn how to write tests for federated queries.</li> </ul>"},{"location":"federation/#federation-example-dgs","title":"Federation Example DGS","text":"<p>This is a DGS example that demonstrates how to implement a federated type, and test federated queries. The source code in this guide comes from the Federation example app. We highly recommend cloning the project and use the IDE while following this guide.</p> <p>The example project has the following set up:</p> <ol> <li>A federated gateway is set up using Apollo's federation gateway libraries.</li> <li>The Shows DGS defines and owns the <code>Show</code> type.</li> <li>The Reviews DGS adds a <code>reviews</code> field to the <code>Show</code> type.</li> </ol> <p>Info</p> <p>If you are completely new to the DGS framework, please take a look at the DGS Getting Started guide, which also contains an introduction video. The remainder of the guide on this page assumes basic GraphQL and DGS knowledge, and focuses on more advanced use cases.</p>"},{"location":"federation/#defining-a-federated-type","title":"Defining a federated type","text":"<p>The Shows DGS defines the <code>Show</code> type with fields id, title and releaseYear.  Note that the <code>id</code> field is marked as the key.  The example has one key, but you can have multiple keys as well <code>@key(fields:\"fieldA fieldB\")</code> This indicates to the gateway that the <code>id</code> field will be used for identifying the corresponding Show in the Shows DGS and must be specified for federated types. <pre><code>type Query {\n  shows(titleFilter: String): [Show]\n}\n\ntype Show @key(fields: \"id\") {\n  id: ID\n  title: String\n  releaseYear: Int\n}\n</code></pre></p>"},{"location":"federation/#extending-a-federated-type","title":"Extending a federated Type","text":"<p>To extend a type you redefine the type in your own schema, using directive <code>@extends</code> to instruct that it's a type extension. <code>@key</code> is required to indicate the field that the gateway will use to identify the original <code>Show</code> for a query. In this case, the key is the <code>id</code> field.</p> <p><pre><code>type Show @key(fields: \"id\") @extends {\n  id: ID @external\n  reviews: [Review]\n}\n\ntype Review {\n  starRating: Int\n}\n</code></pre> When redefining a type, only the id field, and the fields you're adding need to be listed. Other fields, such as <code>title</code> for <code>Show</code> type are provided by the Shows DGS and do not need to be specified unless you are using it in the schema. Federation makes sure the fields provided by all DGSs are combined into a single type for returning the results of a query.</p> <p>Info</p> <p>Don't forget to use the @external directive if you define a field that doesn't belong to your DGS, but you need to reference it.</p>"},{"location":"federation/#implementing-a-federated-type","title":"Implementing a Federated Type","text":"<p>The very first step to get started is to generate Java types that represent the schema. This is configured in <code>build.gradle</code> as described in the manual. When running <code>./gradlew build</code> the Java types are generated into the <code>build/generated</code> folder, which are then automatically added to the classpath.</p>"},{"location":"federation/#provide-an-entity-fetcher","title":"Provide an Entity Fetcher","text":"<p>Let's go through an example of the following query sent to the gateway: <pre><code>query {\n  shows {\n    title\n    reviews {\n      starRating\n    }\n  }\n}\n</code></pre></p> <p>The gateway first fetches the list of all the shows from the Shows DGS containing the title and id fields. <pre><code>query {\n  shows {\n    __typename\n    id\n    title\n  }\n}\n</code></pre></p> <p>Next, the gateway sends the following <code>_entities</code> query to the Reviews DGS using the list of <code>id</code>s from the first query: <pre><code>query($representations: [_Any!]!) {\n  _entities(representations: $representations) {\n    ... on Show {\n      reviews {\n        starRating\n      }\n    }\n  }\n}  \n</code></pre></p> <p>This query comes with the following variables: <pre><code>{\n  \"representations\": [    \n    {          \n      \"__typename\": \"Show\",\n      \"id\": 1\n    },\n    ,\n    {\n      \"__typename\": \"Show\",\n      \"id\": 2\n    },\n    {\n      \"__typename\": \"Show\",\n      \"id\": 3\n    },\n    {\n      \"__typename\": \"Show\",\n      \"id\": 4\n    },\n    {\n      \"__typename\": \"Show\",\n      \"id\": 5\n    }\n  ]        \n} \n</code></pre></p> <p>The Reviews DGS needs to implement an <code>entity fetcher</code> to handle this query. An entity fetcher is responsible for creating an instance of a <code>Show</code> based on the representation in the <code>_entities</code> query above. The DGS framework does most of the heavy lifting, and all we have to do is provide the following:</p> <p>Full code <pre><code>@DgsEntityFetcher(name = \"Show\")\npublic Show movie(Map&lt;String, Object&gt; values) {\n        return new Show((String) values.get(\"id\"), null);\n}\n</code></pre></p> <p>Tip</p> <p>Remember that the Show Java type here is generated by codegen. It's generated from the schema, so it only has the fields our schema specifies.</p> <p>Info</p> <p>Methods annotated using <code>@DgsEntityFetcher</code> are expected to return a concrete type (in this example: <code>Show</code>), <code>CompletionStage&lt;T&gt;</code> (e.g. <code>CompletableFuture&lt;T&gt;</code>), or Reactor <code>Mono&lt;T&gt;</code> instance.</p> <p>Instances of Reactor <code>Flux&lt;T&gt;</code> are not supported. When your scenario warrants returning a collection of concrete types, we suggest using <code>Flux#collectList</code>.</p>"},{"location":"federation/#providing-data-with-a-data-fetcher","title":"Providing Data with a Data Fetcher","text":"<p>Now the DGS knows how to create a Show instance when an <code>_entities</code> query is received, we can specify how to hydrate data for the reviews field.</p> <p>Full code <pre><code>@DgsData(parentType = \"Show\", field = \"reviews\")\npublic List&lt;Review&gt; reviews(DgsDataFetchingEnvironment dataFetchingEnvironment)  {\n    Show show = dataFetchingEnvironment.getSource();\n    return reviews.get(show.getId());\n}\n</code></pre></p>"},{"location":"federation/#testing-a-federated-query","title":"Testing a Federated Query","text":"<p>You can always manually test federated queries by running the gateway and your DGS locally.  You can also manually test a federated query against just your DGS, without the gateway, using the <code>_entities</code> query to replicate the call made to your DGS by the gateway.</p> <p>For automated tests, the QueryExecutor gives a way to run queries from unit tests, with very little startup overhead (in the order of 500ms). We can capture (or manually write) the <code>_entities</code> query that the gateway sends to the DGS. When running the query through the (locally running) gateway, the DGS will log the query that it receives. Simply copy this query in a <code>QueryExecutor</code> test, and that verifies the DGS in isolation.</p> <pre><code>@SpringBootTest(classes = {DgsAutoConfiguration.class, ReviewsDatafetcher.class})\nclass ReviewsDatafetcherTest {\n\n    @Autowired\n    DgsQueryExecutor dgsQueryExecutor;\n\n    @Test\n    void shows() {\n        Map&lt;String,Object&gt; representation = new HashMap&lt;&gt;();\n        representation.put(\"__typename\", \"Show\");\n        representation.put(\"id\", \"1\");\n        List&lt;Map&lt;String, Object&gt;&gt; representationsList = new ArrayList&lt;&gt;();\n        representationsList.add(representation);\n\n        Map&lt;String, Object&gt; variables = new HashMap&lt;&gt;();\n        variables.put(\"representations\", representationsList);\n        List&lt;Review&gt; reviewsList = dgsQueryExecutor.executeAndExtractJsonPathAsObject(\n                \"query ($representations:[_Any!]!) {\" +\n                        \"_entities(representations:$representations) {\" +\n                        \"... on Show {\" +\n                        \"   reviews {\" +\n                        \"       starRating\" +\n                        \"}}}}\",\n                \"data['_entities'][0].reviews\", variables, new TypeRef&lt;&gt;() {});\n\n        assertThat(reviewsList)\n                .isNotNull()\n                .hasSize(3);\n    }\n}\n</code></pre> <p>To help build the federated <code>_entities</code> query, you can also use the <code>EntitiesGraphQLQuery</code> available in <code>graphql-dgs-client</code> package along with code generation. Here is an example of the same test that uses the builder API:</p> <pre><code>@SpringBootTest(classes = {DgsAutoConfiguration.class, ReviewsDatafetcher.class})\nclass ReviewssDatafetcherTest {\n\n    @Autowired\n    DgsQueryExecutor dgsQueryExecutor;\n\n    @Test\n    void showsWithEntitiesQueryBuilder() {\n        EntitiesGraphQLQuery entitiesQuery = new EntitiesGraphQLQuery.Builder().addRepresentationAsVariable(ShowRepresentation.newBuilder().id(\"1\").build()).build();\n        GraphQLQueryRequest request = new GraphQLQueryRequest(entitiesQuery, new EntitiesProjectionRoot().onShow().reviews().starRating());\n        List&lt;Review&gt; reviewsList = dgsQueryExecutor.executeAndExtractJsonPathAsObject(\n                request.serialize(),\n                \"data['_entities'][0].reviews\", entitiesQuery.getVariables(), new TypeRef&lt;&gt;() {\n                });\n        assertThat(reviewsList).isNotNull();\n        assertThat(reviewsList.size()).isEqualTo(3);\n    }\n}\n</code></pre> <p>For more details on the API and how to set it up for tests, please refer to our documentation here.</p>"},{"location":"federation/#customizing-the-default-federation-resolver","title":"Customizing the Default Federation Resolver","text":"<p>In the example above the GraphQL <code>Show</code> type name maps to the Java <code>Show</code> type. There are also cases where the GraphQL and Java type names don't match, specially when working with existing code. If any of your class names do not match your schema type names, you need to provide this class with a way to map between them. To do this, return a map from the <code>typeMapping()</code> method in your own implementation of the <code>DefaultDgsFederationResolver</code>. In the following example we map the GraphQL <code>Show</code> type to a <code>ShowId</code> Java type.</p> <pre><code>@DgsComponent\npublic class FederationResolver extends DefaultDgsFederationResolver {\n    private final Map&lt;Class&lt;?&gt;, String&gt; types = new HashMap&lt;&gt;();\n\n    @PostConstruct\n    public void init() {\n        //The Show type is represented by the ShowId class.\n        types.put(ShowId.class, \"Show\");\n    }\n\n    @Override\n    public Map&lt;Class&lt;?&gt;, String&gt; typeMapping() {\n        return types;\n    }\n}\n</code></pre>"},{"location":"generating-code-from-schema/","title":"Code Generation","text":"<p>The DGS Code Generation plugin generates code during your project\u2019s build process based on your Domain Graph Service\u2019s GraphQL schema file. The plugin generates the following:</p> <ul> <li>Data types for types, input types, enums and interfaces.</li> <li>A <code>DgsConstants</code> class containing the names of types and fields</li> <li>A type safe query API that represents your queries</li> </ul>"},{"location":"generating-code-from-schema/#quick-start","title":"Quick Start","text":"<p>Code generation is typically integrated in the build. This project provides a Gradle plugin, and a Maven plugin was made available by the community, built on the same core.</p> <p>To apply the plugin, update your project\u2019s <code>build.gradle</code> file to include the following: <pre><code>// Using plugins DSL\nplugins {\n    id \"com.netflix.dgs.codegen\" version \"[REPLACE_WITH_CODEGEN_PLUGIN_VERSION]\"\n}\n</code></pre></p> <p>Alternatively, you can set up classpath dependencies in your buildscript: <pre><code>buildscript {\n   dependencies{\n      classpath 'com.netflix.graphql.dgs.codegen:graphql-dgs-codegen-gradle:[REPLACE_WITH_CODEGEN_PLUGIN_VERSION]'\n   }\n}\n\napply plugin: 'com.netflix.dgs.codegen'\n</code></pre></p> <p>Next, you need to add the task configuration as shown here:</p> <pre><code>generateJava{\n   schemaPaths = [\"${projectDir}/src/main/resources/schema\"] // List of directories containing schema files\n   packageName = 'com.example.packagename' // The package name to use to generate sources\n   generateClient = true // Enable generating the type safe query API\n}\n</code></pre>   NOTE: Please use the latest version of the plugin, available here <p>The plugin adds a <code>generateJava</code> Gradle task that runs as part of your project\u2019s build. <code>generateJava</code> generates the code in the project\u2019s <code>build/generated</code> directory. Note that on a Kotlin project, the <code>generateJava</code> task generates Kotlin code by default (yes the name is confusing). This folder is automatically added to the project's classpath. Types are available as part of the package specified by the <code>packageName.types</code>, where you specify the value of packageName as a configuration in your <code>build.gradle</code> file. Please ensure that your project\u2019s sources refer to the generated code using the specified package name.</p> <p>You can exclude parts of the schema from code-generation by placing them in a different schema directory that is not specified as part of the <code>schemaPaths</code> for the plugin.</p>"},{"location":"generating-code-from-schema/#using-the-generated-types","title":"Using the generated types","text":"<p>The generated types are POJOs with both a non-arg constructor, a constructor for all fields, and implementations for hashCode/equals/toString. You also get a builder class to easily create instances. The types are typically used used as return types for your datafetchers and input arguments for your datafetchers. </p> <p>The following are some examples of using generated types for the example schema below.</p> <pre><code>type Query {\n    events: [Event]\n}\n\ntype Event {\n    id: ID\n    name: String\n    location: String\n    keywords: [String]\n    website: String\n    date: Date\n}\n\ntype Mutation {\n    update(event: EventInput): String\n}\n\ninput EventInput {\n    id: ID\n    name: String\n    location: String\n    keywords: [String]\n    website: String\n    date: Date\n}\n</code></pre> <pre><code>@DgsQuery\npublic List&lt;Event&gt; events() {\n    return List.of(\n            Event.newBuilder()\n                    .name(\"JavaOne\")\n                    .location(\"Redwood City\")\n                    .build()\n    );\n}\n\n@DgsMutation\npublic String update(@InputArgument EventInput event) {\n    LOGGER.info(\"Storing event: {}\", event.getName());\n\n    return \"Stored event with id \" + event.getId();\n}\n</code></pre>"},{"location":"generating-code-from-schema/#sparse-updates","title":"Sparse updates","text":"<p>Use the <code>trackInputFieldSet</code> flag to enable tracking which fields are set on input types. This is useful for sparse updates; just sending the fields in a mutation that you want to update, and leave other fields untouched. Codegen creates the POJOs with each field value wrapped in an <code>Optional</code>, and a <code>has[FieldName]</code> method to check if the optional was set. In a mutation datafetcher you can check the input type for which fields were explicitly set.</p> <pre><code>generateJava {\n    trackInputFieldSet = true\n}\n</code></pre> <pre><code>@DgsMutation\npublic String update(@InputArgument EventInput event) {\n\n    LOGGER.info(\"Storing event: {}\", event.getName());\n\n    var updated = new HashSet&lt;String&gt;();\n\n    if(event.hasName()) {\n        LOGGER.info(\"Update name to: {}\", event.getName());\n        updated.add(\"name=\" + event.getName());\n    }\n\n    if(event.hasLocation()) {\n        LOGGER.info(\"Update location to: {}\", event.getLocation());\n        updated.add(\"location=\" + event.getLocation());\n    }\n\n    if(event.hasWebsite()) {\n        LOGGER.info(\"Update website to: {}\", event.getWebsite());\n        updated.add(\"website=\" + event.getWebsite());\n    }\n\n    if(event.hasDate()) {\n        LOGGER.info(\"Update date to: {}\", event.getDate());\n        updated.add(\"date=\" + event.getDate());\n    }\n\n    if(event.hasKeywords()) {\n        LOGGER.info(\"Update keywords to: {}\", event.getKeywords());\n        updated.add(\"keywords=\" + event.getKeywords());\n    }\n\n    return String.join(\", \", updated);\n\n}\n</code></pre>"},{"location":"generating-code-from-schema/#jspecify-null-safety-annotations","title":"JSpecify Null-Safety Annotations","text":"<p>You can generate JSpecify annotations for null-safety on Java types using the <code>generateJSpecifyAnnotations</code> flag.</p> <pre><code>generateJava {\n    generateJSpecifyAnnotations = true\n}\n</code></pre> <p>These annotations enable static analysis tools like NullAway to detect potential null pointer exceptions at compile time. To compile generated sources with these annotations and use them in your own code, add the following to your project's <code>build.gradle</code>:</p> <pre><code>dependencies {\n    implementation \"org.jspecify:jspecify:1.0.0\"\n}\n</code></pre> <p>Learn more</p> <p>Check out the Nullness User Guide for more information on using JSpecify annotations.</p> <p>When enabled, the following are added to the generated Java code:</p> <ul> <li><code>@NullMarked</code> annotations on classes and interfaces so generated types are non-null by default</li> <li><code>@Nullable</code> annotations on nullable fields, getters, setters, and constructor parameters</li> <li><code>Objects.requireNonNull()</code> validation in the generated builder's <code>build()</code> method for non-null fields</li> </ul> <p>Where is @NonNull?</p> <p>Due to the presence of <code>@NullMarked</code> at the type level, we can omit the use of <code>@NonNull</code> annotations.</p> <p>For example, given the following schema:</p> <pre><code>type Event {\n    id: ID!\n    name: String!\n    location: String\n    attendees: [Attendee!]\n    organizer: Person\n}\n\ninterface Person {\n    name: String!\n    email: String\n}\n\ntype Attendee implements Person {\n    name: String!\n    email: String\n    ticketNumber: String!\n}\n\ninput EventInput {\n    id: ID\n    name: String!\n    location: String\n    attendeeNames: [String]\n}\n</code></pre> <p>The generator produces the following (shown with <code>javaGenerateAllConstructor = true</code>):</p> <pre><code>@NullMarked\npublic class Event {\n    private String id;\n\n    private String name;\n\n    @Nullable\n    private String location;\n\n    @Nullable\n    private List&lt;Attendee&gt; attendees;\n\n    @Nullable\n    private Person organizer;\n\n    private Event() {\n    }\n\n    public Event(String id, String name, @Nullable String location,\n            @Nullable List&lt;Attendee&gt; attendees, @Nullable Person organizer) {\n        this.id = id;\n        this.name = name;\n        this.location = location;\n        this.attendees = attendees;\n        this.organizer = organizer;\n    }\n\n    public String getId() {\n        return id;\n    }\n\n    public void setId(String id) {\n        this.id = id;\n    }\n\n    public String getName() {\n        return name;\n    }\n\n    public void setName(String name) {\n        this.name = name;\n    }\n\n    @Nullable\n    public List&lt;Attendee&gt; getAttendees() {\n        return attendees;\n    }\n\n    public void setAttendees(@Nullable List&lt;Attendee&gt; attendees) {\n        this.attendees = attendees;\n    }\n\n    // Other methods omitted for brevity...\n\n    public static class Builder {\n        @Nullable\n        private String id;\n\n        @Nullable\n        private String name;\n\n        @Nullable\n        private String location;\n\n        @Nullable\n        private List&lt;Attendee&gt; attendees;\n\n        @Nullable\n        private Person organizer;\n\n        public Event build() {\n            Event result = new Event();\n            result.id = Objects.requireNonNull(this.id, \"id cannot be null\");\n            result.name = Objects.requireNonNull(this.name, \"name cannot be null\");\n            result.location = this.location;\n            result.attendees = this.attendees;\n            result.organizer = this.organizer;\n            return result;\n        }\n\n        public Builder id(String id) {\n            this.id = id;\n            return this;\n        }\n\n        public Builder name(String name) {\n            this.name = name;\n            return this;\n        }\n\n        public Builder location(@Nullable String location) {\n            this.location = location;\n            return this;\n        }\n\n        public Builder attendees(@Nullable List&lt;Attendee&gt; attendees) {\n            this.attendees = attendees;\n            return this;\n        }\n\n        public Builder organizer(@Nullable Person organizer) {\n            this.organizer = organizer;\n            return this;\n        }\n    }\n}\n</code></pre> <pre><code>@NullMarked\npublic interface Person {\n    String getName();\n\n    void setName(String name);\n\n    @Nullable\n    String getEmail();\n\n    void setEmail(@Nullable String email);\n}\n</code></pre> <pre><code>@NullMarked\npublic class Attendee implements Person {\n    private String name;\n\n    @Nullable\n    private String email;\n\n    private String ticketNumber;\n\n    private Attendee() {\n    }\n\n    public Attendee(String name, @Nullable String email, String ticketNumber) {\n        this.name = name;\n        this.email = email;\n        this.ticketNumber = ticketNumber;\n    }\n\n    public String getName() {\n        return name;\n    }\n\n    public void setName(String name) {\n        this.name = name;\n    }\n\n    @Nullable\n    public String getEmail() {\n        return email;\n    }\n\n    public void setEmail(@Nullable String email) {\n        this.email = email;\n    }\n\n    // Other methods omitted for brevity...\n\n    public static class Builder {\n        @Nullable\n        private String name;\n\n        @Nullable\n        private String email;\n\n        @Nullable\n        private String ticketNumber;\n\n        public Attendee build() {\n            Attendee result = new Attendee();\n            result.name = Objects.requireNonNull(this.name, \"name cannot be null\");\n            result.email = this.email;\n            result.ticketNumber = Objects.requireNonNull(this.ticketNumber, \"ticketNumber cannot be null\");\n            return result;\n        }\n\n        public Builder name(String name) {\n            this.name = name;\n            return this;\n        }\n\n        public Builder email(@Nullable String email) {\n            this.email = email;\n            return this;\n        }\n\n        public Builder ticketNumber(String ticketNumber) {\n            this.ticketNumber = ticketNumber;\n            return this;\n        }\n    }\n}\n</code></pre> <pre><code>@NullMarked\npublic class EventInput {\n    @Nullable\n    private String id;\n\n    private String name;\n\n    @Nullable\n    private String location;\n\n    @Nullable\n    private List&lt;@Nullable String&gt; attendeeNames;\n\n    private EventInput() {\n    }\n\n    public EventInput(@Nullable String id, String name, @Nullable String location,\n            @Nullable List&lt;@Nullable String&gt; attendeeNames) {\n        this.id = id;\n        this.name = name;\n        this.location = location;\n        this.attendeeNames = attendeeNames;\n    }\n\n    @Nullable\n    public String getId() {\n        return id;\n    }\n\n    public void setId(@Nullable String id) {\n        this.id = id;\n    }\n\n    public String getName() {\n        return name;\n    }\n\n    public void setName(String name) {\n        this.name = name;\n    }\n\n    // Other methods omitted for brevity...\n\n    public static class Builder {\n        @Nullable\n        private String id;\n\n        @Nullable\n        private String name;\n\n        @Nullable\n        private String location;\n\n        @Nullable\n        private List&lt;@Nullable String&gt; attendeeNames;\n\n        public EventInput build() {\n            EventInput result = new EventInput();\n            result.id = this.id;\n            result.name = Objects.requireNonNull(this.name, \"name cannot be null\");\n            result.location = this.location;\n            result.attendeeNames = this.attendeeNames;\n            return result;\n        }\n\n        public Builder id(@Nullable String id) {\n            this.id = id;\n            return this;\n        }\n\n        public Builder name(String name) {\n            this.name = name;\n            return this;\n        }\n\n        public Builder location(@Nullable String location) {\n            this.location = location;\n            return this;\n        }\n\n        public Builder attendeeNames(@Nullable List&lt;@Nullable String&gt; attendeeNames) {\n            this.attendeeNames = attendeeNames;\n            return this;\n        }\n    }\n}\n</code></pre> <p>Note</p> <p>When <code>generateJSpecifyAnnotations</code> is enabled, a <code>private</code> no-arg constructor is generated instead of a <code>public</code> one to prevent bypassing null-checks.</p> <p>Tip</p> <p>It is recommended to also set <code>javaGenerateAllConstructor = true</code> for direct instantiation using an all-args constructor.</p>"},{"location":"generating-code-from-schema/#generating-code-from-external-schemas-in-jars","title":"Generating code from external schemas in JARs","text":"<p>You can also specify external dependencies containing schemas to use for generation by declaring it as a dependency in the <code>dgsCodegen</code> configuration. The plugin will scan all <code>.graphql</code> and <code>.graphqls</code> files and generate those classes under the <code>build/generated</code> directory. This is useful if you have external dependencies containing some shared types that you want to add to your schema for code generation.  Not that this does NOT affect your project's schema, and is only for code generation.</p> <pre><code>dependencies {\n    // other dependencies\n    dgsCodegen 'com.netflix.graphql.dgs:example-schema:x.x.x'\n}\n</code></pre> <p>For libraries that are looking to export type mappings for their schemas (see next section), you can also add a <code>dgs.codegen.typemappings</code> file as a resource under META-INF. For a project that is consuming schemas from an external JAR, codegen will also scan <code>dgs.codegen.typemappings</code> to automatically map types to corresponding Java classes. This avoids the need to explicitly specify type mappings by every consumer of the JAR.</p>"},{"location":"generating-code-from-schema/#mapping-existing-types","title":"Mapping existing types","text":"<p>Codegen tries to generate a type for each type it finds in the schema, with a few exceptions.</p> <ol> <li>Basic scalar types - are mapped to corresponding Java/Kotlin types (String, Integer etc.)</li> <li>Date and time types - are mapped to corresponding <code>java.time</code> classes</li> <li>PageInfo and RelayPageInfo - are mapped to <code>graphql.relay</code> classes</li> <li>Types mapped with a <code>typeMapping</code> configuration</li> </ol> <p>When you have existing classes that you want to use instead of generating a class for a certain type, you can configure the plugin to do so using a <code>typeMapping</code>. The <code>typeMapping</code> configuration is a <code>Map</code> where each key is a GraphQL type and each value is a fully qualified Java/Kotlin type.</p> <pre><code>generateJava{\n   typeMapping = [\"MyGraphQLType\": \"com.mypackage.MyJavaType\"]\n}\n</code></pre>"},{"location":"generating-code-from-schema/#generating-client-apis","title":"Generating Client APIs","text":"<p>The code generator can also create client API classes. You can use these classes to query data from a GraphQL endpoint using Java, or in unit tests using the <code>DgsQueryExecutor</code>. The Java GraphQL Client is useful for server-to-server communication. A GraphQL Java Client is available as part of the framework.</p> <p>Code generation creates a <code>field-nameGraphQLQuery</code> for each Query and Mutation field. The <code>*GraphQLQuery</code> query class contains fields for each parameter of the field. For each type returned by a Query or Mutation, code generation creates a <code>*ProjectionRoot</code>. A projection is a builder class that specifies which fields get returned.</p> <p>The following is an example usage of a generated API.</p> <pre><code>type Query {\n    events(filter: EventFilter): [Event]\n}\n\ninput EventFilter {\n    name: String\n    location: String\n}\n\ntype Event {\n    id: ID\n    name(uppercase: Boolean): String\n    location: String\n    keywords: [String]\n    website: String\n}\n</code></pre> <pre><code>@SpringBootTest(classes = QueryDatafetcher.class)\n@EnableDgsTest\nclass QueryDatafetcherTest {\n\n    @Autowired\n    DgsQueryExecutor queryExecutor;\n\n    @Test\n    public void clientApi() {\n        var query = EventsGraphQLQuery.newRequest()\n                .queryName(\"ExampleQuery\")\n                .filter(EventFilter.newBuilder().name(\"JavaOne\").build())\n                .build();\n\n        var projection = new EventsProjectionRoot&lt;&gt;().name(true).parent().location();\n        var request = new GraphQLQueryRequest(query, projection);\n\n        var serializeQuery = request.serialize();\n        var result = queryExecutor.execute(serializeQuery);\n\n        System.out.println(serializeQuery);\n        assertThat(result.isDataPresent()).isTrue();\n    }\n\n}\n</code></pre> <p>Creating a query has three parts: 1. The query - <code>EventsGraphQLQuery</code> in this example, generated by Codegen.  2. The projection (the fields you want to retrieve) - <code>EventsProjectionRoot</code> in this example, generated by Codegen. 3. The <code>GraphQLQueryRequest</code> which is part of the DGS API.</p> <p>The <code>GraphQLQueryRequest</code> lets you <code>serialize()</code>, which gives the String representation of the request. For this example this results in the following request.</p> <pre><code>query ExampleQuery {\n    events(filter: {name : \"JavaOne\"}) {\n        name(uppercase: true)\n        location\n    }\n}\n</code></pre>"},{"location":"generating-code-from-schema/#using-query-variables","title":"Using query variables","text":"<p>In the previous examples the input arguments to the query (the  <code>filter</code>), and the input argument to the <code>name</code> field (<code>uppercase</code>) where provided in-line. This is the easiest way to write a query and has the benefit of the arguments being typed. However, creating queries this way can come with a downside. Advanced GraphQL features such as persisted queries require queries to be written with variables. This is a bit more cumbersome because you lose typing, but it's sometimes required. Codegen creates <code>[fieldName]Reference</code> and <code>[fieldName]WithVariableReferences</code> (for projections) for this purpose.</p> <p>The following is an example of the same query as above, but with using variables.</p> <pre><code>@SpringBootTest(classes = QueryDatafetcher.class)\n@EnableDgsTest\nclass QueryDatafetcherTest {\n\n    @Autowired\n    DgsQueryExecutor queryExecutor;\n\n    @Test\n    public void clientApi() {\n        var query = EventsGraphQLQuery.newRequest()\n                .queryName(\"ExampleQuery\")\n                .filterReference(\"eventFilter\")\n                .build();\n\n        var projection = new EventsProjectionRoot&lt;&gt;()\n                .nameWithVariableReferences(\"uppercase\").parent()\n                .location();\n\n        var request = new GraphQLQueryRequest(query, projection);\n\n        var serializeQuery = request.serialize();\n        var result = queryExecutor.execute(serializeQuery,\n                Map.of(\"eventFilter\", Map.of(\"name\", \"JavaOne\"),\n                        \"uppercase\", true));\n\n        System.out.println(serializeQuery);\n        assertThat(result.isDataPresent()).isTrue();\n    }\n\n}\n</code></pre>"},{"location":"generating-code-from-schema/#generating-query-apis-for-external-services","title":"Generating Query APIs for external services","text":"<p>Generating a Query API like above is very useful for testing your own DGS. The same type of API can also be useful when interacting with another GraphQL service, where your code is a client of that service. This is typically done using the DGS Client.</p> <p>When you use code generation both for your own schema, and an internal schema, you might want different code generation configuration for both. The recommendation is to create a separate module in your project containing the schema of the external service and the codegen configuration to just generate a Query API. The following is example configuration that only generates a Query API.</p> <pre><code>generateJava {\n    schemaPaths = [\"${projectDir}/composed-schema.graphqls\"]\n    packageName = \"some.other.service\"\n    generateClientv2 = true\n    generateDataTypes = false\n    skipEntityQueries = true\n    includeQueries = [\"hello\"]\n    includeMutations = [\"\"]\n    shortProjectionNames = true\n}\n</code></pre>"},{"location":"generating-code-from-schema/#limiting-generated-code-for-client-api","title":"Limiting generated code for Client API","text":"<p>If your schema is large or has a lot of cycles, it is not ideal to generate client APIs for the entire schema, since you will end up with a large number of projections. This can cause code generation to slow down significantly, or run out of memory depending on your schema. We have a few configuration parameters that help tune this so you can limit the generation of client API to only what is required.</p> <p><pre><code>generateJava {\n    ...\n    generateClientv2 = true\n    skipEntityQueries = true\n    includeQueries = [\"hello\"]\n    includeMutations = [\"\"]\n    includeSubscriptions = [\"\"]\n}\n</code></pre> Firstly, you can specify exactly which queries/mutation/subscriptions to generate for via <code>includeQueries</code>, <code>includeMutations</code>, and <code>includeSubscriptions</code>. <code>skipEntityQueries</code> is only used if you are constructing federated <code>_entities</code> queries for testing purposes, so you can also set that to restrict the amount of generated code.</p>"},{"location":"generating-code-from-schema/#generating-classes-with-custom-annotations","title":"Generating classes with Custom Annotations","text":"<p>This feature provides the ability to support any custom annotation on the generated POJOs using the @annotate directive in graphQL. The <code>@annotate</code> directive can be placed on type, input or fields in the graphQL. This feature is turned off by default and can be enabled by setting generateCustomAnnotation to true in build.gradle.</p> <p><pre><code>generateJava {\n    ...\n    generateCustomAnnotations = true\n}\n</code></pre> @annotate contains 4 fields:</p> <ul> <li>name - Mandatory field. Name of the annotation. Eg: ValidPerson. You can have the package along with the annotation name. eg: <code>com.test.ValidPerson</code>. The package value given with the annotation name takes precedence over the mapped package in build.gradle.</li> <li>type - Optional field. This variable is used to map the annotation package in build.gradle. The package if given with annotation name will take precedence over this value. But if neither are given an empty string is used.</li> <li>inputs - Optional field. Contains the inputs to the annotation in key-value pairs. Eg: <code>inputs: {types: [HUSBAND, WIFE]}</code>. Inputs can be of types: String, int, float, enums, list, map, class, etc. For class inputs, refer to Example with Class Object </li> <li>target - Optional field. Refers to the site targets for the annotations. Refer to use target site doc for the target site available values.</li> </ul> <p>@annotate definition in the graphQL: <pre><code>\"Custom Annotation\"\ndirective @annotate(\n    name: String!\n    type: String\n    inputs: JSON\n    target: String\n) repeatable on OBJECT | FIELD_DEFINITION | INPUT_OBJECT | INPUT_FIELD_DEFINITION\n</code></pre> Custom annotations specified in the schema will require corresponding implementations by the resolvers to avoid runtime errors. Some examples: <pre><code>type Person @annotate(name: \"ValidPerson\", type: \"validator\", inputs: {types: [HUSBAND, WIFE]}) {\n       name: String @annotate(name: \"com.test.anotherValidator.ValidName\")\n       type: String @annotate(name: \"ValidType\", type: \"personType\", inputs: {types: [PRIMARY, SECONDARY]}) \n}\n</code></pre> The package mapping for the annotation and enums can be provided in the build.gradle file. <pre><code>generateJava {\n    ...\n    generateCustomAnnotations = true\n    includeImports = [\"validator\": \"com.test.validator\"]\n    includeEnumImports = [\"ValidPerson\": [\"types\": \"com.enums\"]]\n}\n</code></pre> Generated POJO in Java. Please note that this feature is also available in Kotlin. <pre><code>package com.netflix.graphql.dgs.codegen.tests.generated.types;\n\nimport com.test.anotherValidator.ValidName;\nimport com.test.validator.ValidPerson;\nimport java.lang.Object;\nimport java.lang.Override;\nimport java.lang.String;\n\n@ValidPerson(\n    types = [com.enums.HUSBAND, com.enums.WIFE]\n)\npublic class Person {\n  @ValidName\n  private String name;\n\n  @ValidType(\n      types = [com.personType.enum.PRIMARY, com.personType.enum.SECONDARY]\n  )\n  private String type;\n\n  public Person() {\n  }\n\n  public Person(String name, String type) {\n    this.name = name;\n    this.type = type;\n  }\n\n  public String getName() {\n    return name;\n  }\n\n  public void setName(String name) {\n    this.name = name;\n  }\n\n  public String getType() {\n    return type;\n  }\n\n  public void setType(String type) {\n    this.type = type;\n  }\n\n  @Override\n  public String toString() {\n    return \"Person{\" + \"name='\" + name + \"',\" +\"type='\" + type + \"'\" +\"}\";\n  }\n\n  @Override\n  public boolean equals(Object o) {\n    if (this == o) return true;\n        if (o == null || getClass() != o.getClass()) return false;\n        Person that = (Person) o;\n        return java.util.Objects.equals(name, that.name) &amp;&amp;\n                            java.util.Objects.equals(type, that.type);\n  }\n\n  @Override\n  public int hashCode() {\n    return java.util.Objects.hash(name, type);\n  }\n}\n</code></pre></p> <p>Example with Class Object:</p> <p>Since GraphQL parser does not have built-in support for class objects, a class is represented as a string ending with \".class\" in the schema</p> <p><pre><code>type Person @annotate(name: \"ValidPerson\", type: \"validator\", inputs: {groups: \"BasicValidation.class\"}) {\n    name: String @annotate(name: \"com.test.anotherValidator.ValidName\")\n}\n</code></pre> The package mapping for the annotation and classes can be provided in the build.gradle file. If mapping is not provided, input will be treated as a string.</p> <p><pre><code>generateJava {\n    ...\n    generateCustomAnnotations = true,\n    includeImports = mapOf(Pair(\"validator\", \"com.test.validator\")),\n    includeClassImports = mapOf(\"ValidPerson\" to mapOf(Pair(\"BasicValidation\", \"com.test.validator.groups\")))\n}\n</code></pre> Generated POJO in Java. Note: In Kotlin, using the same schema above will generate <code>BasicValidation::class</code> <pre><code>package com.netflix.graphql.dgs.codegen.tests.generated.types;\n\nimport com.test.anotherValidator.ValidName;\nimport com.test.validator.ValidPerson;\nimport com.test.validator.groups.BasicValidation;\nimport java.lang.Object;\nimport java.lang.Override;\nimport java.lang.String;\n\n@ValidPerson(\n    groups = BasicValidation.class\n)\npublic class Person {\n  @ValidName\n  private String name;\n\n  public Person() {\n  }\n\n  public Person(String name) {\n    this.name = name;\n  }\n\n  public String getName() {\n    return name;\n  }\n\n  public void setName(String name) {\n    this.name = name;\n  }\n\n  @Override\n  public String toString() {\n    return \"Person{\" + \"name='\" + name + \"'\" +\"}\";\n  }\n\n  @Override\n  public boolean equals(Object o) {\n    if (this == o) return true;\n        if (o == null || getClass() != o.getClass()) return false;\n        Person that = (Person) o;\n        return java.util.Objects.equals(name, that.name);\n  }\n\n  @Override\n  public int hashCode() {\n    return java.util.Objects.hash(name);\n  }\n}\n</code></pre> While using <code>@deprecated</code>, the following configuration is needed to generate <code>@Deprecated</code> on the POJO in Java:</p> <pre><code>generateJava {\n    addDeprecatedAnnotation = true\n}\n</code></pre> <p>Example with target site: <pre><code>type Person @deprecated(reason: \"This is going bye bye\") @annotate(name: \"ValidPerson\", type: \"validator\", inputs: {types: [HUSBAND, WIFE]}) {\n    name: String @annotate(name: \"com.test.anotherValidator.ValidName\", target: \"field\") @annotate(name: \"com.test.nullValidator.NullValue\")\n}\n</code></pre> Generated POJO in Java. <pre><code>package com.netflix.graphql.dgs.codegen.tests.generated.types;\n\nimport com.test.anotherValidator.ValidName;\nimport com.test.nullValidator.NullValue;\nimport com.test.validator.ValidPerson;\nimport java.lang.Deprecated;\nimport java.lang.Object;\nimport java.lang.Override;\nimport java.lang.String;\n\n/**\n * This is going bye bye\n */\n@Deprecated\n@ValidPerson(\n    types = [com.enums.HUSBAND, com.enums.WIFE]\n)\npublic class Person {\n  @ValidName\n  @NullValue\n  private String name;\n\n  public Person() {\n  }\n\n  public Person(String name) {\n    this.name = name;\n  }\n\n  public String getName() {\n    return name;\n  }\n\n  public void setName(String name) {\n    this.name = name;\n  }\n\n  @Override\n  public String toString() {\n    return \"Person{\" + \"name='\" + name + \"'\" +\"}\";\n  }\n\n  @Override\n  public boolean equals(Object o) {\n    if (this == o) return true;\n        if (o == null || getClass() != o.getClass()) return false;\n        Person that = (Person) o;\n        return java.util.Objects.equals(name, that.name);\n  }\n\n  @Override\n  public int hashCode() {\n    return java.util.Objects.hash(name);\n  }\n}\n</code></pre></p>"},{"location":"generating-code-from-schema/#configuring-code-generation","title":"Configuring code generation","text":"<p>Code generation has many configuration switches. The following table shows the Gradle configuration options, but the same options are available command line and in Maven as well.</p>"},{"location":"generating-code-from-schema/#file-package-configuration","title":"File &amp; Package Configuration","text":"Configuration property Description Default value generatedSourcesDir Build directory for Gradle build schemaPaths List of files/directories containing schemas src/main/resources/schema packageName Base package name of generated code com.netflix.dgs.codegen.generated subPackageNameClient Sub package name for generated Query API client subPackageNameDatafetchers Sub package name for generated data fetchers datafetchers subPackageNameTypes Sub package name for generated data types types"},{"location":"generating-code-from-schema/#language-type-configuration","title":"Language &amp; Type Configuration","text":"Configuration property Description Default value language Target language: <code>java</code> or <code>kotlin</code> Autodetected from project typeMapping A Map where each key is a GraphQL type, and the value the FQN of a Java class generateBoxedTypes Always use boxed types (Integer, Boolean) for primitives false (boxed types are used only for nullable fields)"},{"location":"generating-code-from-schema/#client-generation","title":"Client Generation","text":"Configuration property Description Default value generateClient Generate a Query API. false generateClientv2 Generate a Query API. false"},{"location":"generating-code-from-schema/#data-type-generation","title":"Data Type Generation","text":"Configuration property Description Default value generateDataTypes Generate data types. Useful for only generating a Query API. Input types are still generated when <code>generateClientv2</code> is true. true generateInterfaces Generate interfaces for data classes. This is useful if you would like to extend the generated POJOs for more context and use interfaces instead of the data classes in your data fetchers. false generateInterfaceSetters Generate setter methods in interfaces for fields that are not interfaces true generateInterfaceMethodsForInterfaceFields Generate setter methods in interfaces for fields that are interfaces. Getter methods in interfaces for fields that are interfaces are generated by default even without this option. false"},{"location":"generating-code-from-schema/#java-specific-options","title":"Java-Specific Options","text":"Configuration property Description Default value generateIsGetterForPrimitiveBooleanFields Use \"is\" prefix for primitive boolean getters false generateDocs Generate JavaDoc from schema descriptions false implementSerializable Make generated classes implement Serializable false javaGenerateAllConstructor Generate all-args constructor in Java classes true"},{"location":"generating-code-from-schema/#kotlin-specific-options","title":"Kotlin-Specific Options","text":"Configuration property Description Default value generateKotlinNullableClasses Generate nullable types in Kotlin false generateKotlinClosureProjections Use closure syntax for projections in Kotlin false kotlinAllFieldsOptional Make all fields optional in Kotlin false"},{"location":"generating-code-from-schema/#querymutationsubscription-filtering","title":"Query/Mutation/Subscription Filtering","text":"Configuration property Description Default value includeQueries Generate Query API only for the given list of Query fields All queries defined in schema includeMutations Generate Query API only for the given list of Mutation fields All mutations defined in schema includeSubscriptions Generate Query API only for the given list of Subscription fields All subscriptions defined in schema skipEntityQueries Disable generating Entity queries for federated types false"},{"location":"generating-code-from-schema/#naming-conventions","title":"Naming Conventions","text":"Configuration property Description Default value shortProjectionNames Shorten class names of projection types. These types are not visible to the developer. false snakeCaseConstantNames Use snake_case for constant names false"},{"location":"generating-code-from-schema/#annotations","title":"Annotations","text":"Configuration property Description Default value addGeneratedAnnotation Add <code>jakarta.annotation.Generated</code> and application specific <code>@Generated</code> annotation to generated types false disableDatesInGeneratedAnnotation Don't add a date to the <code>jakarta.annotation.Generated</code> annotation false addDeprecatedAnnotation Add <code>@Deprecated</code> annotation for deprecated schema elements false generateCustomAnnotations Enable generation of custom annotations on generated types and fields using <code>@annotate</code> directive in schema false generateJSpecifyAnnotations Enable/disable generation of JSpecify annotations (<code>@NullMarked</code> for types, <code>@Nullable</code> for fields/getters/setters/parameters). false includeImports Maps the custom annotation type to the package, the annotations belong to. Only used when generateCustomAnnotations is enabled. includeEnumImports Maps the custom annotation and enum argument names to the enum packages. Only used when generateCustomAnnotations is enabled. includeClassImports Maps the custom annotation and class names to the class packages. Only used when generateCustomAnnotations is enabled."},{"location":"generating-code-from-schema/#additional-features","title":"Additional Features","text":"Configuration property Description Default value trackInputFieldSet Generate <code>has[FieldName]</code> methods keeping track of what fields are explicitly set on input types false"},{"location":"getting-started/","title":"Getting started","text":""},{"location":"mutations/","title":"Mutations","text":"<p>The DGS framework supports Mutations with the same constructs as data fetchers, using the <code>@DgsData</code> annotation. The following is a simple example of a mutation:</p> <pre><code>type Mutation {\n    addRating(title: String, stars: Int):Rating\n}\n\ntype Rating {\n    avgStars: Float\n}\n</code></pre> <pre><code>@DgsComponent\npublic class RatingMutation {\n    @DgsData(parentType = \"Mutation\", field = \"addRating\")\n    public Rating addRating(DataFetchingEnvironment dataFetchingEnvironment) {\n        int stars = dataFetchingEnvironment.getArgument(\"stars\");\n\n        if(stars &lt; 1) {\n            throw new IllegalArgumentException(\"Stars must be 1-5\");\n        }\n\n        String title = dataFetchingEnvironment.getArgument(\"title\");\n        System.out.println(\"Rated \" + title + \" with \" + stars + \" stars\") ;\n\n        return new Rating(stars);\n    }\n}\n</code></pre> <p>Note that the code above retrieves the input data for the Mutation by calling the <code>DataFetchingEnvironment.getArgument</code> method, just as data fetchers do for their arguments.</p>"},{"location":"mutations/#input-types","title":"Input Types","text":"<p>In the example above the input was two standard scalar types. You can also use complex types, and you should define these as <code>input</code> types in your schema. An <code>input</code> type is almost the same as a <code>type</code> in GraphQL, but with some extra rules.</p> <p>According to the GraphQL specification an input type should always be passed to the data fetcher as a <code>Map</code>. This means the <code>DataFetchingEnvironment.getArgument</code> for an input type is a <code>Map</code>, and not the Java/Kotlin representation that you might have. The framework has a convenience mechanism around this, which will be discussed next. Let's first look at an example that uses DataFetchingEnvironment directly.</p> <pre><code>type Mutation {\n    addRating(input: RatingInput):Rating\n}\n\ninput RatingInput {\n    title: String,\n    stars: Int\n}\n\ntype Rating {\n    avgStars: Float\n}\n</code></pre> <pre><code>@DgsComponent\npublic class RatingMutation {\n    @DgsData(parentType = \"Mutation\", field = \"addRating\")\n    public Rating addRating(DataFetchingEnvironment dataFetchingEnvironment) {\n\n        Map&lt;String,Object&gt; input = dataFetchingEnvironment.getArgument(\"input\");\n        RatingInput ratingInput = new ObjectMapper().convertValue(input, RatingInput.class);\n\n        System.out.println(\"Rated \" + ratingInput.getTitle() + \" with \" + ratingInput.getStars() + \" stars\") ;\n\n        return new Rating(ratingInput.getStars());\n    }\n}\n\nclass RatingInput {\n    private String title;\n    private int stars;\n\n    public String getTitle() {\n        return title;\n    }\n\n    public void setTitle(String title) {\n        this.title = title;\n    }\n\n    public int getStars() {\n        return stars;\n    }\n\n    public void setStars(int stars) {\n        this.stars = stars;\n    }\n}\n</code></pre>"},{"location":"mutations/#input-arguments-as-data-fetcher-method-parameters","title":"Input arguments as data fetcher method parameters","text":"<p>The framework makes it easier to get input arguments. You can specify arguments as method parameters of a data fetcher.</p> <pre><code>@DgsComponent\npublic class RatingMutation {\n    @DgsData(parentType = \"Mutation\", field = \"addRating\")\n    public Rating addRating(@InputArgument(\"input\") RatingInput ratingInput) {\n        //No need for custom parsing anymore!\n        System.out.println(\"Rated \" + ratingInput.getTitle() + \" with \" + ratingInput.getStars() + \" stars\") ;\n\n        return new Rating(ratingInput.getStars());\n    }\n}\n</code></pre> <p>The <code>@InputArgument</code> annotation is important to specify the name of the input argument, because arguments can be specified in any order. If no annotation is present, the framework tries to use the parameter name, but this is only possible if the code is compiled with specific compiler settings. Input type parameters can be combined with a <code>DataFetchingEnvironment</code> parameter.</p> <pre><code>@DgsComponent\npublic class RatingMutation {\n    @DgsData(parentType = \"Mutation\", field = \"addRating\")\n    public Rating addRating(@InputArgument(\"input\") RatingInput ratingInput, DataFetchingEnvironment dfe) {\n        //No need for custom parsing anymore!\n        System.out.println(\"Rated \" + ratingInput.getTitle() + \" with \" + ratingInput.getStars() + \" stars\") ;\n        System.out.println(\"DataFetchingEnvironment: \" + dfe.getArgument(ratingInput));\n\n        return new Rating(ratingInput.getStars());\n    }\n}\n</code></pre>"},{"location":"query-execution-testing/","title":"Testing","text":"<p>The DGS framework allows you to write lightweight tests that partially bootstrap the framework, just enough to run queries.</p>"},{"location":"query-execution-testing/#example","title":"Example","text":"<p>Before writing tests, make sure that JUnit is enabled. If you created a project with Spring Initializr this configuration should already be there.</p> GradleGradle KotlinMaven <pre><code>dependencies {\n    testImplementation 'org.springframework.boot:spring-boot-starter-test'\n    testImplementation 'com.netflix.graphql.dgs:dgs-starter-test'\n}\n\ntest {\n    useJUnitPlatform()\n}\n</code></pre> <pre><code>tasks.withType&lt;Test&gt; {\n    useJUnitPlatform()\n}\n</code></pre> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;\n    &lt;scope&gt;test&lt;/scope&gt;\n&lt;/dependency&gt;\n\n&lt;dependency&gt;\n    &lt;groupId&gt;com.netflix.graphql.dgs&lt;/groupId&gt;\n    &lt;artifactId&gt;dgs-starter-test&lt;/artifactId&gt;\n    &lt;scope&gt;test&lt;/scope&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Create a test class with the following contents to test the <code>ShowsDatafetcher</code> from the getting started example.</p> JavaKotlin <pre><code>import com.netflix.graphql.dgs.DgsQueryExecutor;\nimport org.junit.jupiter.api.Test;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.boot.test.context.SpringBootTest;\n\nimport java.util.List;\nimport com.netflix.graphql.dgs.test.EnableDgsTest;\n\nimport static org.assertj.core.api.Assertions.assertThat;\n\n@SpringBootTest(classes = {ShowsDatafetcher.class})\n@EnableDgsTest\nclass ShowsDatafetcherTest {\n\n    @Autowired\n    DgsQueryExecutor dgsQueryExecutor;\n\n    @Test\n    void shows() {\n        List&lt;String&gt; titles = dgsQueryExecutor.executeAndExtractJsonPath(\n                \" { shows { title releaseYear }}\",\n                \"data.shows[*].title\");\n\n        assertThat(titles).contains(\"Ozark\");\n    }\n}\n</code></pre> <pre><code>import com.netflix.graphql.dgs.DgsQueryExecutor\nimport com.netflix.graphql.dgs.test.EnableDgsTest\nimport org.assertj.core.api.Assertions.assertThat\nimport org.junit.jupiter.api.Test\nimport org.springframework.beans.factory.annotation.Autowired\nimport org.springframework.boot.test.context.SpringBootTest\n\n@SpringBootTest(classes = [ShowsDataFetcher::class])\n@EnableDgsTest\nclass ShowsDataFetcherTest {\n\n    @Autowired\n    lateinit var dgsQueryExecutor: DgsQueryExecutor\n\n    @Test\n    fun shows() {\n        val titles : List&lt;String&gt; = dgsQueryExecutor.executeAndExtractJsonPath(\"\"\"\n            {\n                shows {\n                    title\n                    releaseYear\n                }\n            }\n        \"\"\".trimIndent(), \"data.shows[*].title\")\n\n        assertThat(titles).contains(\"Ozark\")\n    }\n}\n</code></pre> <p>The <code>@SpringBootTest</code> annotation makes this a Spring test. If you do not specify <code>classes</code> explicitly, Spring will start all components on the classpath. For a small application this is fine, but for applications with components that are \"expensive\" to start we can speed up the test by only adding the classes we need for the test. In this example that's just the <code>ShowsDatafetcher</code>. You also need components from the DGS framework itself, so that you can run a real GraphQL query in your test. Add the <code>@EnableDgsTest</code> test slice annotation. This annotation effectively adds a list of autoconfiguration classes that start as part of the test.</p> <p>Testing data fetchers that use WebMVC annotations such as @RequestHeader</p> <p><code>@EnableDgsTest</code> is designed to not require a web stack, to keep tests as fast and simple as possible. In some scenarios you do need web functionality. In that case you should use the <code>@EnableDgsMockMvcTest</code>, which also sets up the components to use <code>MockMvc</code>.</p> <p>To execute queries, inject <code>DgsQueryExecutor</code> in the test. This interface has several methods to execute a query and get back the result. It executes the exact same code as a query on the <code>/graphql</code> endpoint would, but you won\u2019t have to deal with HTTP in your tests. The <code>DgsQueryExecutor</code> methods accept JSON paths, so that the methods can easily extract just the data from the response that you\u2019re interested in. The <code>DgsQueryExecutor</code> also includes methods (e.g. <code>executeAndExtractJsonPathAsObject</code>) to deserialize the result to a Java class, which uses Jackson under the hood. The JSON paths are supported by the open source JsonPath library.</p> <p>Write a few more tests, for example to verify the behavior with using the <code>titleFilter</code> of <code>ShowsDatafetcher</code>. You can run the tests from the IDE, or from Gradle/Maven, just like any JUnit test.</p>"},{"location":"query-execution-testing/#building-graphql-queries-for-tests","title":"Building GraphQL Queries for Tests","text":"<p>In the examples shown previously, we handcrafted the query string. This is simple enough for queries that are small and straightforward. However, constructing longer query strings can be tedious, specially in Java without support for multi-line Strings. For this, we can use the GraphQLQueryRequest to build the graphql request in combination with the code generation plugin to generate the classes needed to use the request builder. This provides a convenient type-safe way to build your queries.</p> <p>To set up code generation to generate the required classes to use for building your queries, follow the instructions here.</p> <p>Now we can write a test that uses <code>GraphQLQueryRequest</code> to build the query and extract the response using <code>GraphQLResponse</code>.</p> JavaKotlin <pre><code>@Test\npublic void showsWithQueryApi() {\n    GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest(\n            new ShowsGraphQLQuery.Builder().titleFilter(\"Oz\").build(),\n            new ShowsProjectionRoot().title()\n    );\n\n    List&lt;String&gt; titles = dgsQueryExecutor.executeAndExtractJsonPath(graphQLQueryRequest.serialize(), \"data.shows[*].title\");\n    assertThat(titles).containsExactly(\"Ozark\");\n}\n</code></pre> <pre><code>@Test\nfun showsWithQueryApi() {\n    val graphQLQueryRequest = GraphQLQueryRequest(\n        ShowsGraphQLQuery.Builder()\n            .titleFilter(\"Oz\")\n            .build(),\n        ShowsProjectionRoot().title())\n\n    val titles = dgsQueryExecutor.executeAndExtractJsonPath&lt;List&lt;String&gt;&gt;(graphQLQueryRequest.serialize(), \"data.shows[*].title\")\n    assertThat(titles).containsExactly(\"Ozark\")\n}\n</code></pre> <p>The <code>GraphQLQueryRequest</code> is available as part of the graphql-client module and is used to build the query string, and wrap the response respectively. You can also refer to the GraphQLClient JavaDoc for more details on the list of supported methods.</p>"},{"location":"query-execution-testing/#mocking-external-service-calls-in-tests","title":"Mocking External Service Calls in Tests","text":"<p>It\u2019s not uncommon for a data fetcher to talk to external systems such as a database or a gRPC service. If it does so within a test, this adds two problems:</p> <ol> <li>It adds latency; your tests are going to run slower when they make a lot of external calls.</li> <li>It adds flakiness: Did your code introduce a bug, or did something go wrong in the external system?</li> </ol> <p>In many cases it\u2019s better to mock these external services. Spring already has good support for doing so with the @Mockbean annotation, which you can leverage in your DGS tests.</p>"},{"location":"query-execution-testing/#example_1","title":"Example","text":"<p>Let's update the <code>Shows</code> example to load shows from an external data source, instead of just returning a fixed list. For the sake of the example we'll just move the fixed list of shows to a new class that we'll annotate <code>@Service</code>. The data fetcher is updated to use the injected <code>ShowsService</code>.</p> JavaKotlin <pre><code>public interface ShowsService {\n    List&lt;Show&gt; shows();\n}\n\n@Service\npublic class ShowsServiceImpl implements ShowsService {\n    @Override\n    public List&lt;Show&gt; shows() {\n        return List.of(\n            new Show(\"Stranger Things\", 2016),\n            new Show(\"Ozark\", 2017),\n            new Show(\"The Crown\", 2016),\n            new Show(\"Dead to Me\", 2019),\n            new Show(\"Orange is the New Black\", 2013)\n        );\n    }\n}\n</code></pre> <pre><code>interface ShowsService {\n    fun shows(): List&lt;ShowsDataFetcher.Show&gt;\n}\n\n@Service\nclass BasicShowsService : ShowsService {\n    override fun shows(): List&lt;ShowsDataFetcher.Show&gt; {\n        return listOf(\n            ShowsDataFetcher.Show(\"Stranger Things\", 2016),\n            ShowsDataFetcher.Show(\"Ozark\", 2017),\n            ShowsDataFetcher.Show(\"The Crown\", 2016),\n            ShowsDataFetcher.Show(\"Dead to Me\", 2019),\n            ShowsDataFetcher.Show(\"Orange is the New Black\", 2013)\n        )\n    }\n}\n\n@DgsComponent\nclass ShowsDataFetcher {\n    @Autowired\n    lateinit var showsService: ShowsService\n\n    @DgsData(parentType = \"Query\", field = \"shows\")\n    fun shows(@InputArgument(\"titleFilter\") titleFilter: String?): List&lt;Show&gt; {\n        return if (titleFilter != null) {\n            showsService.shows().filter { it.title.contains(titleFilter) }\n        } else {\n            showsService.shows()\n        }\n    }\n}\n</code></pre> <p>For the sake of the example the shows are still in-memory, imagine that the service would actually call out to an external data store. Let's try to mock this service in the test!</p> JavaKotlin <pre><code>@SpringBootTest(classes = {ShowsDataFetcher.class})\n@EnableDgsTest\npublic class ShowsDataFetcherTests {\n\n    @Autowired\n    DgsQueryExecutor dgsQueryExecutor;\n\n    @MockBean\n    ShowsService showsService;\n\n    @BeforeEach\n    public void before() {\n        Mockito.when(showsService.shows()).thenAnswer(invocation -&gt; List.of(new Show(\"mock title\", 2020)));\n    }\n\n    @Test\n    public void showsWithQueryApi() {\n        GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest(\n                new ShowsGraphQLQuery.Builder().build(),\n                new ShowsProjectionRoot().title()\n        );\n\n        List&lt;String&gt; titles = dgsQueryExecutor.executeAndExtractJsonPath(graphQLQueryRequest.serialize(), \"data.shows[*].title\");\n        assertThat(titles).containsExactly(\"mock title\");\n    }\n}\n</code></pre> <pre><code>@SpringBootTest(classes = [ShowsDataFetcher::class])\n@EnableDgsTest\nclass ShowsDataFetcherTest {\n\n    @Autowired\n    lateinit var\n    dgsQueryExecutor:DgsQueryExecutor\n\n    @MockBean\n    lateinit var\n    showsService:ShowsService\n\n    @BeforeEach\n\n    fun before() {\n        Mockito.`when`(showsService.shows()).thenAnswer {\n            listOf(ShowsDataFetcher.Show(\"mock title\", 2020))\n        }\n    }\n\n    @Test\n    fun shows() {\n        val titles :List&lt;String&gt; =dgsQueryExecutor.executeAndExtractJsonPath(\"\"\"\n                    {\n                        shows {\n                            title\n                            releaseYear\n                        }\n                    }\n                \"\"\".trimIndent(), \"data.shows[*].title\")\n\n        assertThat(titles).contains(\"mock title\")\n    }\n}\n</code></pre>"},{"location":"query-execution-testing/#testing-exceptions","title":"Testing Exceptions","text":"<p>The tests you wrote so far are mostly happy paths. Failure scenarios are also easy to test. We use the mocked example from above to force an exception.</p> JavaKotlin <pre><code>@Test\nvoid showsWithException() {\nMockito.when(showsService.shows()).thenThrow(new RuntimeException(\"nothing to see here\"));\n    ExecutionResult result = dgsQueryExecutor.execute(\" { shows { title releaseYear }}\");\n    assertThat(result.getErrors()).isNotEmpty();\n    assertThat(result.getErrors().get(0).getMessage()).isEqualTo(\"java.lang.RuntimeException: nothing to see here\");\n}\n</code></pre> <pre><code>@Test\nfun showsWithException() {\n    Mockito.`when`(showsService.shows()).thenThrow(RuntimeException(\"nothing to see here\"))\n\n    val result = dgsQueryExecutor.execute(\"\"\"\n        {\n            shows {\n                title\n                releaseYear\n            }\n        }\n    \"\"\".trimIndent())\n\n    assertThat(result.errors).isNotEmpty\n    assertThat(result.errors[0].message).isEqualTo(\"java.lang.RuntimeException: nothing to see here\")\n}\n</code></pre> <p>When an error happens while executing the query, the errors are wrapped in a <code>QueryException</code>. This allows you to easily inspect the error. The <code>message</code> of the <code>QueryException</code> is the concatenation of all the errors. The <code>getErrors()</code> method gives access to the individual errors for further inspection.</p>"},{"location":"query-execution-testing/#testing-with-spring-mockmvc-and-httpgraphqltester","title":"Testing with Spring MockMvc and HttpGraphQlTester","text":"<p>While testing queries without the web layer is the preferred approach for most tests, it is sometimes useful to include the web layer as well. For example, to test integration of filters, security and such. It's useful to have a least one such a test as a \"smoke test\" as part of your testing strategy. While for a smoketest you probably want to bootstrap all application components (by using <code>@SpringBootTest</code> without arguments), instead of test slices, you can use <code>HttpGrahpQlTester</code> in either scenario.</p> <pre><code>@SpringBootTest(classes = {ShowsDataFetcher.class})\n@EnableDgsMockMvcTest //Enable DGS and MockMvc\n@AutoConfigureHttpGraphQlTester //Enable HttpGraphQlTester\npublic class SmokeTestWithSpringGraphQL {\n    @Autowired\n    private HttpGraphQlTester graphQlTester;\n\n    @Test\n    void testShows() {\n        @Language(\"GraphQL\")\n        var query = \"\"\"\n                query {\n                    shows {\n                        title\n                    }\n                }\n                \"\"\";\n\n        graphQlTester.document(query)\n                .execute()\n                .path(\"shows[*].title\")\n                .entityList(String.class)\n                .containsExactly(\"The Last Dance\", \"Cheer\", \"GLOW\");\n    }\n}\n</code></pre>"},{"location":"query-execution-testing/#testing-with-mockmvc-directly","title":"Testing with MockMvc directly","text":"<p>While it's easier to use <code>HttpGraphQlTester</code> as shown above, you can also write a test using <code>MockMvc</code> directly and crafting the GraphQl request by hand. You can easily send a GraphQL request using <code>MockMvc</code> by creating a wrapper object and using Jackson for JSON serialization.</p> <pre><code>@SpringBootTest\n@AutoConfigureMockMvc\npublic class SmokeTestWithRequest {\n    @Autowired\n    MockMvc mockMvc;\n\n    @Autowired\n    ObjectMapper objectMapper;\n\n    @Test\n    public void showsSmokeTest() throws Exception {\n        @Language(\"GraphQL\")\n        var query = \"\"\"                \n                {\n                    shows {\n                        title\n                    }\n                }\n                \"\"\";\n\n        mockMvc.perform(post(\"/graphql\")\n                        .secure(true)\n                        .content(objectMapper.writeValueAsBytes(new GraphqlRequest(query)))\n                        .contentType(MediaType.APPLICATION_JSON)\n                        .accept(MediaType.APPLICATION_JSON))\n                .andExpect(jsonPath(\"data.shows\").isArray())\n                .andExpect(jsonPath(\"data.shows[0].title\").exists());\n    }\n\n    record GraphqlRequest(String query){}\n}\n</code></pre>"},{"location":"query-execution-testing/#testing-with-a-real-client","title":"Testing with a real Client","text":"<p>Testing with a real client is typically not needed, and you should avoid such test in most cases. Running a server on a real port can be problematic in CI. The following test does show how to set this up with the DGS Java GraphQL Client and the server starting on a random port. Note that this example starts the whole application instead of just starting individual components.</p> <pre><code>import com.netflix.graphql.dgs.client.GraphQLResponse;\nimport com.netflix.graphql.dgs.client.MonoGraphQLClient;\nimport org.junit.jupiter.api.Test;\nimport org.springframework.boot.test.context.SpringBootTest;\nimport org.springframework.boot.web.server.LocalServerPort;\nimport org.springframework.web.reactive.function.client.WebClient;\n\nimport java.util.List;\n\nimport static org.junit.jupiter.api.Assertions.assertTrue;\n\n@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT)\nclass ShowsDatafetcherTest {\nfinal MonoGraphQLClient monoGraphQLClient;\n\n    public ShowsDatafetcherTest(@LocalServerPort Integer port) {\n        WebClient webClient = WebClient.create(\"http://localhost:\" + port.toString() + \"/graphql\");\n        this.monoGraphQLClient = MonoGraphQLClient.createWithWebClient(webClient);\n    }\n\n    @Test\n    void shows() {\n        String query = \"{ shows { title releaseYear }}\";\n\n        // Read more about executeQuery() at https://netflix.github.io/dgs/advanced/java-client/\n        GraphQLResponse response =\n                monoGraphQLClient.reactiveExecuteQuery(query).block();\n\n        List&lt;?&gt; titles = response.extractValueAsObject(\"shows[*].title\", List.class);\n\n        assertTrue(titles.contains(\"Ozark\"));\n    }\n}\n</code></pre>"},{"location":"scalars/","title":"Adding Custom Scalars","text":"<p>It is easy to add a custom scalar type in the DGS framework: Create a class that implements the <code>graphql.schema.Coercing</code> interface and annotate it with the <code>@DgsScalar</code> annotation. Also make sure the scalar type is defined in your [GraphQL] schema!</p> <p>For example, this is a simple <code>LocalDateTime</code> implementation:</p> <pre><code>@DgsScalar(name=\"DateTime\")\npublic class DateTimeScalar implements Coercing&lt;LocalDateTime, String&gt; {\n    @Override\n    public String serialize(Object dataFetcherResult) throws CoercingSerializeException {\n        if (dataFetcherResult instanceof LocalDateTime) {\n            return ((LocalDateTime) dataFetcherResult).format(DateTimeFormatter.ISO_DATE_TIME);\n        } else {\n            throw new CoercingSerializeException(\"Not a valid DateTime\");\n        }\n    }\n\n    @Override\n    public LocalDateTime parseValue(Object input) throws CoercingParseValueException {\n        return LocalDateTime.parse(input.toString(), DateTimeFormatter.ISO_DATE_TIME);\n    }\n\n    @Override\n    public LocalDateTime parseLiteral(Object input) throws CoercingParseLiteralException {\n        if (input instanceof StringValue) {\n            return LocalDateTime.parse(((StringValue) input).getValue(), DateTimeFormatter.ISO_DATE_TIME);\n        }\n\n        throw new CoercingParseLiteralException(\"Value is not a valid ISO date time\");\n    }\n\n    @Override\n    public Value valueToLiteral(@NotNull Object input) {\n        return new StringValue(this.serialize(input));\n    }\n}\n</code></pre> <p>Schema: <pre><code>scalar DateTime\n</code></pre></p> <p>Important</p> <p>If you are Building GraphQL Queries for Tests, make sure to pass your custom scalars in <code>GraphQLQueryRequest</code> constructor as descibred in Scalars in DGS Client</p>"},{"location":"scalars/#registering-custom-scalars","title":"Registering Custom Scalars","text":"<p>In more recent versions of <code>graphql-java</code> (&gt;v15.0) some scalars, most notably the <code>Long</code> scalar, are no longer available by default. These are non-standard scalars that are difficult for clients (e.g. JavaScript) to handle reliably. As a result of the deprecation, you will need to add them explicitly, and to do this you have a few options.</p> <p>Tip</p> <p>Go to the graphql-java-extended-scalars project page to see the full list of scalars supported by this library. There you will also find examples of the scalars used in both schemas as well as example queries.</p>"},{"location":"scalars/#automatically-register-scalar-extensions-via-graphql-dgs-extended-scalars","title":"Automatically Register Scalar Extensions via graphql-dgs-extended-scalars","text":"<p>The DGS Framework, as of version 3.9.2, has the <code>graphql-dgs-extended-scalars</code> module. This module provides an auto-configuration that will register automatically the scalar extensions defined in the <code>com.graphql-java:graphql-java-extended-scalars</code> library. To use it you need to...</p> <ol> <li>Add the <code>com.netflix.graphql.dgs:graphql-dgs-extended-scalars</code> dependency to your build. If you are using the    DGS BOM you don't need to specify a version for it, the BOM will recommend one.</li> <li>Define the scalar in your schema</li> </ol> <p>Other mapping available on extended scalars doc</p> <p>The <code>graphql-java-extended-scalars</code> module offers a few knobs you can use to turn off registration.</p> Property Description dgs.graphql.extensions.scalars.time-dates.enabled If set to <code>false</code>, it will not register the DateTime, Date, and Time scalar extensions. dgs.graphql.extensions.scalars.objects.enabled If set to <code>false</code>, it will not register the Object, Json, Url, and Locale scalar extensions. dgs.graphql.extensions.scalars.numbers.enabled If set to <code>false</code>, it will not register all numeric scalar extensions such as PositiveInt, NegativeInt, etc. dgs.graphql.extensions.scalars.chars.enabled If set to <code>false</code>, it will not register the GraphQLChar extension. dgs.graphql.extensions.scalars.enabled If set to <code>false</code>, it will disable automatic registration of all of the above. <p>Important</p> <p>Are you using the code generation Gradle Plugin?</p> <p>The <code>graphql-java-extended-scalars</code>  module doesn't modify the behavior of such plugin, you will need to explicit define the type mappings. For example, let's say we want to use both the <code>Url</code> and <code>PositiveInt</code> Scalars. You will have to add the mapping below to your build file.</p> GradleGradle Kotlin <pre><code>generateJava {\n    typeMapping = [\n        \"Url\" : \"java.net.URL\",\n        \"PositiveInt\" : \"java.lang.Integer\"\n    ]\n}\n</code></pre> <pre><code>generateJava {\n    typeMapping = mutableMapOf(\n        \"Url\" to \"java.net.URL\",\n        \"PositiveInt\" to \"java.lang.Integer\"\n    )\n}\n</code></pre>"},{"location":"scalars/#testing-in-java-using-graphql-dgs-extended-scalars","title":"Testing in Java using <code>graphql-dgs-extended-scalars</code>","text":"<p>Don't forget to provide <code>DgsExtendedScalarsAutoConfiguration.class</code> when testing.</p> <pre><code>@SpringBootTest(classes = {DgsAutoConfiguration.class, DgsExtendedScalarsAutoConfiguration.class})\nclass Test {\n...\n</code></pre>"},{"location":"scalars/#register-scalar-extensions-via-dgsruntimewiring","title":"Register Scalar Extensions via DgsRuntimeWiring","text":"<p>You can also register the Scalar Extensions manually. To do so you need to...</p> <ol> <li>Add the <code>com.graphql-java:graphql-java-extended-scalars</code> dependency to your build. If you are using the    DGS BOM you don't need to specify a version for it, the BOM will recommend one.</li> <li>Define the scalar in your schema</li> <li>Register the scalar.</li> </ol> <p>Here is an example of how you would set that up:</p> <p>Schema: <pre><code>scalar Long\n</code></pre> You can register the <code>Long</code> scalar manually with the DGS Framework as shown here: <pre><code>@DgsComponent\npublic class LongScalarRegistration {\n    @DgsRuntimeWiring\n    public RuntimeWiring.Builder addScalar(RuntimeWiring.Builder builder) {\n        return builder.scalar(Scalars.GraphQLLong);\n    }\n}\n</code></pre></p>"},{"location":"spring-graphql-integration/","title":"Spring GraphQL Integration","text":"<p>The DGS Framework now integrates with Spring for GraphQL internally. Users can continue using the DGS framework as is without additional changes. Please refer to out Getting Started guide for more details. The integration with Spring for GraphQL will allow DGS users to take advantage of any new features that Spring for GraphQL has to offer without having to reimplement it in the framework. While it is technically possible to mix and match the DGS/Spring-GraphQL programming models, we recommend sticking with the DGS programming model for now, if using the DGS Framework. By the DGS programming model, we specifically refer to DGS concepts, such as the annotations for setting up data fetchers, data loaders etc. This will allow users to maintain consistency in the codebase and take full advantage of DGS capabilities.</p> <p>It is important to note that Spring for GraphQL and the DGS framework offer many similar features that may differ in capabilities.  For that reason, Spring for GraphQL features work best for Spring for GraphQL style resolvers and vice versa. A nice benefit to integrating with Spring for GraphQL, is that it paves a way for new features in Spring for GraphQL to be used in the DGS Framework. These will be available via existing spring-graphql extensions, such as the Subscription Callback mechanism in the JVM Federation library.</p> <p>Continue reading to know more about how to use Spring for GraphQL with the DGS Framework.</p>"},{"location":"spring-graphql-integration/#background-two-competing-frameworks","title":"Background - Two competing frameworks","text":"<p>The DGS Framework provides Java developers with a programming model on top of Spring Boot to create GraphQL services.  Netflix open-sourced the DGS framework in 2021, and has been the widely adopted GraphQL Java framework by many companies.</p> <p>Soon after we open-sourced the DGS framework, we learned about parallel efforts by the Spring team to develop a GraphQL framework for Spring Boot.  The Spring for GraphQL project was in the early stages at the time and provided a low-level of integration with graphql-java.  Over the past year, however, Spring for GraphQL has matured and is mostly at feature parity with the DGS Framework.  We now have 2 competing frameworks that solve the same problems for our users.</p> <p>Today, new users must choose between the DGS Framework or Spring for GraphQL, thus missing out on features available in one framework but not the other.  This is not an ideal situation for the GraphQL Java community.</p> <p>For the maintainers of DGS and Spring for GraphQL, it would be far more effective to collaborate on features and improvements instead of having to solve the same problem independently.  Finally, a unified community would provide us with better channels for feedback.</p>"},{"location":"spring-graphql-integration/#why-not-just-eol-one-of-the-frameworks","title":"Why not just EOL one of the frameworks?","text":"<p>The DGS framework is widely used and plays a vital role in the architecture of many companies, including Netflix.  Moving away from the framework in favor of Spring-GraphQL would be a costly migration without any real benefits.</p> <p>From a Spring Framework perspective, it makes sense to have an out-of-the-box GraphQL offering, just like Spring supports REST.</p>"},{"location":"spring-graphql-integration/#the-way-forward","title":"The way forward","text":"<p>With this integration, you can pull in additional features from Spring for GraphQL.  We also eliminate the need for part of the DGS code that integrates the framework with Spring MVC/WebFlux, since there isn't much benefit to duplicating low level functionality.</p> <p>For the near term, the DGS/Spring-Graphql integration will be available as an opt-in feature via a different spring-graphql flavor of the DGS starter.  We plan to make this the default mode towards the latter part of the year, after we see some level of successful adoption.</p> <p></p>"},{"location":"spring-graphql-integration/#technical-implementation","title":"Technical implementation","text":"<p>Both DGS and Spring-GraphQL are designed with modularity and extensibility in mind. This makes it feasible to integrate the two frameworks. The following diagrams show how the frameworks are integrated at a high level.</p> <p>Today, the DGS framework looks as follows. A user uses the DGS programming model, such as <code>@DgsComponent</code> and <code>@DgsQuery</code> to define data fetchers etc. A GraphQL query comes in through either WebMVC or WebFlux, and is executed by the <code>DgsQueryExecutor</code>, which builds on the graphql-java library to execute the query. The <code>DgsQueryExecutor</code> is also used directly when writing tests.</p> <p></p> <p>With the Spring-GraphQL integration, a user can write code with both the DGS programming model and/or the Spring-GraphQL programming model.  A GraphQL query comes in through WebMVC/WebFlux/RSocket, which Spring-GraphQL now handles.  The representation of the schema (a <code>GraphQLSchema</code> from graphql-java) is created by the DGS framework, and used by both the DGS and Spring for GraphQL components.  Spring for GraphQL's <code>ExecutionGraphQLService</code> now handles the actual query execution, while the DGS <code>QueryExecutor</code> becomes a proxy on top of <code>ExecutionGraphQLService</code> so that existing test code continues to work.</p> <p></p>"},{"location":"spring-graphql-integration/#performance","title":"Performance","text":"<p>At Netflix, we tested the DGS/Spring-GraphQL integration on some of our largest services.  Surprisingly, we uncovered a few performance issues in Spring WebMVC/Spring for GraphQL with this integration.  The Spring team has quickly addressed these issues, and the performance is now even better compared to baseline performance of Netflix applications with just the regular DGS Framework.</p>"},{"location":"spring-graphql-integration/#notable-changes-and-improvements","title":"Notable Changes and Improvements","text":"<p>The good news is that the new integration has been mostly a drop-in replacement, not requiring any breaking code changes for the user.  Besides the features and changes listed in this section, everything else should continue to work as expected.</p>"},{"location":"spring-graphql-integration/#dgs-configuration-with-spring-for-graphql","title":"DGS Configuration with Spring for GraphQL","text":"<p>There is some overlap between configuration properties for DGS and Spring-GraphQL. Where properties overlap, we use the DGS property for the best backward compatibility.  The following is the list of overlapping properties:</p> DGS property Spring-GraphQL property What to use <code>dgs.graphql.schema-locations</code> <code>spring.graphql.schema.locations</code> Use <code>dgs.graphql.schema-locations</code> N/A <code>spring.graphql.schema.fileExtensions</code> Not applicable, because <code>dgs.graphql.schema-locations</code> includes the path <code>dgs.graphql.graphiql.enabled</code> <code>spring.graphql.graphiql.enabled</code> Use <code>dgs.graphql.graphiql.enabled</code> <code>dgs.graphql.graphiql.path</code> <code>spring.graphql.graphiql.path</code> Use <code>dgs.graphql.graphiql.path</code> <code>dgs.graphql.websocket.connection-init-timeout</code> <code>spring.graphql.websocket.connection-init-timeout</code> DGS property sets the Spring-GraphQL property <p>New properties for Spring for GraphQL integration are:</p> DGS Property Description <code>dgs.graphql.spring.webmvc.asyncdispatch.enabled</code> To enable async dispatching for GraphQL requests"},{"location":"spring-graphql-integration/#file-uploads","title":"File Uploads","text":"<p>Support for file uploads will no longer be available by default in the DGS framework.  This is supported using an external dependency for spring-graphql via multipart-spring-graphql.</p>"},{"location":"spring-graphql-integration/#subscriptions-over-websockets","title":"Subscriptions over Websockets","text":"<p>Spring for GraphQL supports subscriptions over websockets.  You will now need to use <code>org.springframework.boot:spring-boot-starter-websocket</code>instead of <code>implementation(\"com.netflix.graphql.dgs:graphql-dgs-subscriptions-websockets-autoconfigure</code>. In addition, you will need to set the following configuration property: <code>spring.graphql.websocket.path: /graphql</code>.</p>"},{"location":"spring-graphql-integration/#schema-inspection","title":"Schema Inspection","text":"<p>You can now inspect your schema using Spring for GraphQL's schema inspection feature for DGS data fetchers as well. You can now inspect schema fields and validate existing DGS data fetcher/and or Spring for GraphQL data fetcher registrations, to check if all schema fields are covered either by an explicitly registered DataFetcher, or a matching Java object property.  The inspection also performs a reverse check looking for DataFetcher registrations against schema fields that don\u2019t exist.</p>"},{"location":"spring-graphql-integration/#schema-resource","title":"Schema resource","text":"<p>In DGS we supported a property <code>dgs.graphql.schema-json.enabled</code> which made the schema available a JSON. With the Spring for GraphQL integration this changes a little bit. Spring for GraphQL provides a property <code>spring.graphql.schema.printer.enabled</code> (disabled by default). When enabled, it provides the schema in text format (not JSON) on <code>/graphql/schema</code>.</p>"},{"location":"spring-graphql-integration/#testing-dgs-data-fetchers","title":"Testing DGS Data Fetchers","text":"<p>For testing individual data fetchers without the web layer, you can continue using the existing testing framework provided via <code>DgsQueryExecutor</code> interface.  We have provided a Spring for GraphQL flavor of the <code>DgsQueryExecutor</code> that will continue to work as it does today. Please refer to our testing docs for more details on writing data fetcher tests.</p> <p>For integration testing with the web layer, you can also use the MockMvc test set up that Spring provides.</p> <p>You can also use the recommended <code>HttpGraphQlTester</code> with MockMvc available in Spring for GraphQL to achieve the same. This works just as well for testing your DGS with a mock web layer as shown in the example below:</p> <pre><code>@SpringBootTest\n@AutoConfigureMockMvc\n@AutoConfigureHttpGraphQlTester\npublic class HttpGraphQlTesterTest {\n\n    @Autowired\n    private HttpGraphQlTester graphQlTester;\n\n    @Test\n    void testDgsDataFetcher() {\n        graphQlTester.document(\"query Hello($name: String){ hello(name: $name) }\").variable(\"name\", \"DGS\").execute()\n                .path(\"hello\").entity(String.class).isEqualTo(\"hello, DGS!\");\n    }\n}\n</code></pre>"},{"location":"spring-graphql-integration/#async-dispatch","title":"Async Dispatch","text":"<p>By default, Spring for GraphQL uses async dispatch for handling HTTP GraphQL Requests when using WebMVC.  In this DGS Framework we have turned off this behavior by default to preserve existing functionality, since it requires existing code to be async aware. This implies servlet filters, tests etc. need to be also async aware. You can turn on async behavior by setting the <code>dgs.graphql.spring.webmvc.asyncdispatch.enabled</code> to true. </p> <p>It is worth noting that with the Spring for GraphQL integration, your MockMVC test set up does need to be updated. Since web request processing is now based on async dispatching mechanism, we now require explicit handling for this in the test setup.</p>"},{"location":"spring-graphql-integration/#modifying-response-headers","title":"Modifying Response headers","text":"<p>Previously, the DGS Framework offered a mechanism to add custom response headers based on the result of processing the GraphQL query using a special <code>DgsRestController.DGS_RESPONSE_HEADERS_KEY</code> key. This is no longer supported.  The recommended way forward is to use the <code>WebGraphQlInterceptor</code> in Spring for GraphQL as described here)</p>"},{"location":"spring-graphql-integration/#known-gaps-and-limitations","title":"Known Gaps and Limitations","text":"<p>At this time, we are lacking support for SSE based subscriptions which is available in the original DGS Framework.  This is on the roadmap and will be made available in the near future depending on support in spring-graphql. </p> <p>In the current state of integration, not all DGS features will work seamlessly for Spring for GraphQL data fetchers, and vice versa. For this reason, we recommend using either the DGS programming model or Spring for GraphQL model and not mixing both styles of APIs. Known limitations include data loader specific features, such as Scheduled Dispatch and data loader specific metrics that won't work with Spring for GraphQL data loaders. You should be able to use new Spring for GraphQL features with the framework, such as schema inspection and any new integrations that are compatible with Spring for GraphQL.</p> <p>We intend iteratively improve the state of the integration in the coming releases based on usage patterns. </p>"},{"location":"videos/","title":"Videos","text":""},{"location":"videos/#graphql-and-dgs-at-netflix-an-introduction-to-the-dgs-framework-hosted-by-jetbrains","title":"GraphQL and DGS at Netflix - an introduction to the DGS framework hosted by Jetbrains","text":""},{"location":"videos/#dgs-spring-graphql-integration-coffee-and-software-with-josh-long","title":"DGS / Spring GraphQL integration @ Coffee and Software with Josh Long","text":""},{"location":"videos/#dgs-framework-graphql-for-spring-boot-openvalue","title":"DGS Framework - GraphQL for Spring Boot @ OpenValue","text":""},{"location":"videos/#dgs-new-features-and-tips-march-2021","title":"DGS new features and tips - March 2021","text":""},{"location":"videos/#tech-tips-short-videos-about-a-specific-feature","title":"Tech tips (short videos about a specific feature)","text":""},{"location":"videos/#tech-tip-1-input-arguments","title":"Tech Tip #1 - Input Arguments","text":""},{"location":"videos/#tech-tip-2-code-generation","title":"Tech Tip #2 - Code generation","text":""},{"location":"videos/#tech-tip-3-building-federated-queries-for-tests-using-codegen","title":"Tech Tip #3 - Building federated queries for tests using codegen","text":""},{"location":"videos/#tech-tip-4-type-resolvers","title":"Tech Tip #4 - Type Resolvers","text":""},{"location":"advanced/context-passing/","title":"Nested data fetchers","text":"<p>Commonly, the datafetcher for a nested field requires properties from its parent object to load its data.</p> <p>Take the following schema example.</p> <pre><code>type Query {\n  shows: [Show]\n}\n\ntype Show {\n  # The showId may or may not be there, depending on the scenario.\n  showId: ID\n  title: String\n  reviews: [Review]\n}\n\ntype Review {\n  starRating: Int\n}\n</code></pre> <p>Let's assume our backend already has methods available to Shows and Reviews from a datastore.  Note that for this example, the <code>getShows</code> method does not return reviews. The <code>getReviewsForShow</code> method loads reviews for a show, given the show id.</p> <pre><code>interface ShowsService {\n  List&lt;Show&gt; getShows(); //Does not include reviews\n  List&lt;Review&gt; getReviewsForShow(int showId);   \n}\n</code></pre> <p>For this scenario, you likely want to have two datafetchers, one for shows and one for reviews. There are different options for implementing the datafetcher, which each has pros and cons depending on the scenario. We'll go over the different scenarios and options.</p>"},{"location":"advanced/context-passing/#the-easy-case-using-getsource","title":"The easy case - Using getSource","text":"<p>In the example schema the <code>Show</code> type has a <code>showId</code>. Having the showId available makes loading reviews in a separate datafetcher very easy. The <code>DataFetcherEnvironment</code> has a <code>getSource()</code> method that returns the parent loaded for a field.</p> <pre><code>@DgsData(parentType = \"Query\", field = \"shows\")\nList&lt;Show&gt; shows() {\n  return showsService.getShows();\n}\n\n@DgsData(parentType = \"Show\", field = \"reviews\")\nList&lt;Review&gt; reviews(DgsDataFetchingEnvironment dfe) {\n  Show show = dfe.getSource();\n  return showsService.getReviewsForShow(show.getShowId());\n} \n</code></pre> <p>This example is the easiest and most common scenario, but only possible if the <code>showId</code> field is available on the <code>Show</code> type.</p>"},{"location":"advanced/context-passing/#no-showid-use-an-internal-type","title":"No showId - Use an internal type","text":"<p>Sometimes you don't want to expose the <code>showId</code> field in the schema, or our types are not set up to carry this field for other reasons. For example, for 1:1 and N:1 it's not that common to model the relationship as a key in the Java model. Whatever the reason is, the scenario we look at here is that we don't have the <code>showId</code> available on <code>Show</code>.</p> <p>If we remove <code>showId</code> from the schema and use codegen, the generated <code>Show</code> type will not have <code>showId</code> field either. Not having the <code>showId</code> field makes loading reviews a bit more complicated, because now we can't get the <code>showId</code> from the <code>Show</code> type using <code>getSource()</code>.</p> <p>The <code>getShowsForService(int showId)</code> method indicates that internally (probably in the datastore), a show does have an id. In such a scenario, we likely have a different internal representation of <code>Show</code> than exposed in the API. For the remainder of the example, we'll call this the <code>InternalShow</code> type which the <code>ShowsService</code> returns.</p> <pre><code>interface ShowsService {\n  List&lt;InternalShow&gt; getShows(); //Does not include reviews\n  List&lt;Review&gt; getReviewsForShow(int showId);   \n}\n\nclass InternalShow {\n  int showId;\n  String title;\n\n  // getters and setters\n}\n</code></pre> <p>However, the <code>Show</code> type in the GraphQL schema does not have a <code>showId</code>.</p> <pre><code>type Show {\n  title: String\n  reviews: [Review]\n}\n</code></pre> <p>The good news is that you can have fields set on your internal instances either not in the schema, or not queried. The framework drops this extra data while creating a response.</p> <p>We could create an extra <code>ShowWithId</code> wrapper class that either extends or composes the (generated) <code>Show</code> type, and adds a <code>showId</code> field.</p> <pre><code>class ShowWithId {\n  String showId;\n  Show show;\n\n  //Delegate all show fields\n  String getTitle() {\n    return show.getTitle();\n  }\n\n  static ShowWithId fromInternalShow(InternalShow internal) {\n    //Create Show instance and store id.\n  }\n  ....\n}\n</code></pre> <p>The <code>shows</code> datafetcher should return the wrapper type instead of just <code>Show</code>.</p> <pre><code>@DgsData(parentType = \"Query\", field = \"shows\")\nList&lt;ShowWithId&gt; shows() {\n  return showsService.getShows().stream()\n    .map(ShowWithId::fromInternalShow)\n    .collect(Collectors.toList());\n}\n</code></pre> <p>As said, the extra field doesn't affect the response to the client at all.</p>"},{"location":"advanced/context-passing/#no-showid-use-local-context","title":"No showId - Use local context","text":"<p>Using wrapper types works well when the schema type and internal type are mostly similar. An alternative way is to use \"local context\". A datafetcher can return a <code>DataFetcherResult&lt;T&gt;</code>, which contains <code>data</code>, <code>errors</code> and <code>localContext</code>. The <code>data</code> and <code>errors</code> fields are the data and errors you would normally return directly from your datafetcher. The <code>localContext</code> field can hold any data you want to pass down to child datafetchers. The <code>localContext</code> can be retrieved in the child datafetcher from the <code>DataFetchingEnvironment</code> and is passed down to the next level child datafetchers if not overwritten.</p> <p>In the following example the <code>shows</code> datafetcher creates a <code>DataFetcherResult</code> that holds the list of <code>Show</code> instances (not the internal type). The <code>localContext</code> is set to a map with each <code>show</code> as key, and the <code>showId</code> as value.</p> <pre><code>@DgsData(parentType = \"Query\", field = \"shows\")\npublic DataFetcherResult&lt;List&lt;Show&gt;&gt; shows(@InputArgument(\"titleFilter\") String titleFilter) {\n    List&lt;InternalShow&gt; internalShows = getShows(titleFilter);\n\n    List&lt;Show&gt; shows = internalShows.stream()\n        .map(s -&gt; Show.newBuilder().title(s.getTitle()).build())\n        .collect(Collectors.toList());\n        return DataFetcherResult.&lt;List&lt;Show&gt;&gt;newResult()\n            .data(shows)\n            .localContext(internalShows.stream()\n            .collect(Collectors.toMap(s -&gt; Show.newBuilder().title(s.getTitle()).build(), InternalShow::getId)))\n            .build();\n\n}\n\nprivate List&lt;InternalShow&gt; getShows(String titleFilter) {\n    if (titleFilter == null) {\n        return showsService.shows();\n    }\n\n    return showsService.shows().stream().filter(s -&gt; s.getTitle().contains(titleFilter)).collect(Collectors.toList());\n}\n</code></pre> <p>The <code>reviews</code> datafetcher can now use a combination of the <code>getSource</code> and <code>getLocalContext</code> methods to get the <code>showId</code> for a show.</p> <pre><code>@DgsData(parentType = \"Show\", field = \"reviews\")\npublic CompletableFuture&lt;List&lt;Review&gt;&gt; reviews(DgsDataFetchingEnvironment dfe) {\n\n    Map&lt;Show, Integer&gt; shows = dfe.getLocalContext();\n\n    Show show = dfe.getSource();\n    return showsService.getReviewsForShow(shows.get(show));\n}\n</code></pre> <p>A benefit of this approach is that in contrast with <code>getSource</code>, the <code>localContext</code> gets passed down to the next level of child datafechers as well.</p>"},{"location":"advanced/context-passing/#pre-loading","title":"Pre-loading","text":"<p>Suppose our internal datastore allows us to load shows and reviews together efficiently, for example using a SQL join query.  In that case, it can be more efficient to pre-load reviews in the <code>shows</code> datafetcher. In the <code>shows</code> datafetcher we can check if the <code>reviews</code> field was included in the query, and only if it is, load the reviews. Depending on the Java/Kotlin types we use, the <code>Show</code> type may or may not have a <code>reviews</code> field. If we use DGS codegen it will, and we can just set the <code>reviews</code> field when creating the <code>Show</code> instances in the <code>shows</code> datafetcher. If the type returned by the <code>shows</code> datafetcher does not have a <code>reviews</code> field, we can again use the <code>localContext</code> to pass on the review data to a <code>reviews</code> datafetcher. Below is an example of pre-loading and using <code>localContext</code>.</p> <pre><code>@DgsData(parentType = \"Query\", field = \"shows\")\npublic DataFetcherResult&lt;List&lt;Show&gt;&gt; shows(DataFetchingEnvironment dfe) {\n    List&lt;Show&gt; shows = showsService.shows();\n    if (dfe.getSelectionSet().contains(\"reviews\")) {\n\n        Map&lt;Integer, List&lt;Review&gt;&gt; reviewsForShows = reviewsService.reviewsForShows(shows.stream().map(Show::getId).collect(Collectors.toList()));\n\n        return DataFetcherResult.&lt;List&lt;Show&gt;&gt;newResult()\n            .data(shows)\n            .localContext(reviewsForShows)\n            .build();\n    } else {\n        return DataFetcherResult.&lt;List&lt;Show&gt;&gt;newResult().data(shows).build();\n    }\n}\n\n@DgsData(parentType = \"Show\", field = \"reviews\")\npublic List&lt;Review&gt; reviews(DgsDataFetchingEnvironment dfe) {\n    Show show = dfe.getSource();\n\n    //Load the reviews from the pre-loaded localContext.\n    Map&lt;Integer, List&lt;Review&gt;&gt; reviewsForShows = dfe.getLocalContext();\n    return reviewsForShows.get(show.getId());\n}\n</code></pre>"},{"location":"advanced/custom-datafetcher-context/","title":"Data Fetching Context","text":"<p>Each data fetcher in [GraphQL] Java has a context. A data fetcher gets access to its context by calling <code>DataFetchingEnvironment.getContext()</code>. This is a common mechanism to pass request context to data fetchers and data loaders. The DGS framework has its own <code>DgsContext</code> implementation, which is used for log instrumentation among other things. It is designed in such a way that you can extend it with your own custom context.</p> <p>To create a custom context, implement a Spring bean of type <code>DgsCustomContextBuilder</code>. Write the <code>build()</code> method so that it creates an instance of the type that represents your custom context object:</p> <pre><code>@Component\npublic class MyContextBuilder implements DgsCustomContextBuilder&lt;MyContext&gt; {\n    @Override\n    public MyContext build() {\n        return new MyContext();\n    }\n}\n\npublic class MyContext {\n    private final String customState = \"Custom state!\";\n\n    public String getCustomState() {\n        return customState;\n    }\n}\n</code></pre> <p>A data fetcher can now retrieve the context by calling the <code>getCustomContext()</code> method:</p> <pre><code>@DgsData(parentType = \"Query\", field = \"withContext\")\npublic String withContext(DataFetchingEnvironment dfe) {\n    MyContext customContext = DgsContext.getCustomContext(dfe);\n    return customContext.getCustomState();\n}\n</code></pre> <p>Similarly, custom context can be used in a DataLoader.</p> <pre><code>@DgsDataLoader(name = \"exampleLoaderWithContext\")\npublic class ExampleLoaderWithContext implements BatchLoaderWithContext&lt;String, String&gt; {\n    @Override\n    public CompletionStage&lt;List&lt;String&gt;&gt; load(List&lt;String&gt; keys, BatchLoaderEnvironment environment) {\n\n        MyContext context = DgsContext.getCustomContext(environment);\n\n        return CompletableFuture.supplyAsync(() -&gt; keys.stream().map(key -&gt; context.getCustomState() + \" \" + key).collect(Collectors.toList()));\n    }\n}\n</code></pre>"},{"location":"advanced/custom-directives/","title":"Adding Custom Directives","text":"<p>GraphQL directives provide a way to decorate parts of a GraphQL schema with additional metadata or behavior. Custom directives enable the powerful extension of GraphQL's base functionalities through your own reusable logic.</p>"},{"location":"advanced/custom-directives/#use-cases","title":"Use Cases","text":"<p>Custom directives may be useful for a variety of tasks, including but not limited to:</p> <ul> <li>Validation</li> <li>Authorization</li> <li>Data Transformation or Augmentation</li> <li>Caching or Optimizations</li> <li>Monitoring or Logging</li> <li>Adding Field Metadata</li> </ul>"},{"location":"advanced/custom-directives/#implementing-a-custom-directive","title":"Implementing a Custom Directive","text":"<p>Consider a scenario where you want to modify the formatting of a returned <code>String</code> field so that the value is transformed to uppercase. You have a couple options: either you can write a utility class within your application code and manually invoke it wherever necessary, or you can create a custom <code>@uppercase</code> GraphQL directive and apply the transformation in a declarative way across your schema where needed:</p> <pre><code>directive @uppercase on FIELD_DEFINITION\n\ntype Query {\n  greeting: String @uppercase\n}\n</code></pre> <p>Here is the backing code needed to support this <code>@uppercase</code> directive:</p> <pre><code>package com.example.demo.directives;\n\nimport com.netflix.graphql.dgs.DgsDirective;\nimport graphql.schema.DataFetcher;\nimport graphql.schema.DataFetcherFactories;\nimport graphql.schema.GraphQLFieldDefinition;\nimport graphql.schema.GraphQLFieldsContainer;\nimport graphql.schema.idl.SchemaDirectiveWiring;\nimport graphql.schema.idl.SchemaDirectiveWiringEnvironment;\n\n@DgsDirective(name = \"uppercase\")\npublic class UppercaseDirective implements SchemaDirectiveWiring {\n\n    @Override\n    public GraphQLFieldDefinition onField(SchemaDirectiveWiringEnvironment&lt;GraphQLFieldDefinition&gt; env) {\n        GraphQLFieldsContainer fieldsContainer = env.getFieldsContainer();\n        GraphQLFieldDefinition fieldDefinition = env.getFieldDefinition();\n\n        DataFetcher&lt;?&gt; originalDataFetcher = env.getCodeRegistry().getDataFetcher(fieldsContainer, fieldDefinition);\n        DataFetcher&lt;?&gt; dataFetcher = DataFetcherFactories.wrapDataFetcher(\n                originalDataFetcher,\n                (dataFetchingEnvironment, value) -&gt; {\n                    if (value instanceof String) {\n                        return ((String) value).toUpperCase();\n                    }\n                    return value;\n                }\n        );\n\n        env.getCodeRegistry().dataFetcher(fieldsContainer, fieldDefinition, dataFetcher);\n\n        return fieldDefinition;\n    }\n}\n</code></pre> <p>In this example, the <code>UppercaseDirective</code> class implements <code>SchemaDirectiveWiring</code> and overrides its <code>onField</code> method, where the logic for transforming the value to uppercase lives. The original <code>DataFetcher</code> for the field is wrapped in a new one, which applies the uppercase logic before returning the value. The <code>@DgsDirective</code> annotation ensures that the custom directive is registered with the Spring framework.</p> <p>Custom directives can be implemented for various components of your GraphQL schema, not just field definitions. To learn more, explore the graphql-java SDL directives documentation.</p>"},{"location":"advanced/custom-object-mapper/","title":"Custom Object Mapper","text":"<p>In scenarios where a custom object mapper is required, you can customize Spring's underlying object mapper.  However, note that this will affect the behavior for all usages of the mapper, and therefore should be done carefully. Also, this mechanism will NOT affect how custom scalars are serialized - those would rely on your scalar implementation's serialization logic handled by <code>graphql-java</code></p> <pre><code>@Bean\npublic Jackson2ObjectMapperBuilder jackson2ObjectMapperBuilder() {\n    return new Jackson2ObjectMapperBuilder().serializers(LOCAL_DATETIME_SERIALIZER).serializationInclusion(JsonInclude.Include.NON_NULL);\n}\n</code></pre>"},{"location":"advanced/dynamic-schemas/","title":"Dynamic schemas","text":"<p>We strongly recommend primarily using schema-first development. Most DGSs have a schema file and use the declarative, annotation-based programming model to create data fetchers and such. That said, there are scenarios where generating the schema from another source, possibly dynamically, is required.</p>"},{"location":"advanced/dynamic-schemas/#creating-a-schema-from-code","title":"Creating a schema from code","text":"<p>Create a schema from code by using the <code>@DgsTypeDefinitionRegistry</code> annotation. Use the <code>@DgsTypeDefinitionRegistry</code> on methods inside a <code>@DgsComponent</code> class to provide a <code>TypeDefinitionRegistry</code>.</p> <p>The <code>TypeDefinitionRegistry</code> is part of the graphql-java API. You use a <code>TypeDefinitionRegistry</code> to programmatically define a schema.</p> <p>Note that you can mix static schema files with one or more <code>DgsTypeDefinitionRegistry</code> methods. The result is a schema with all the registered types merged. This way, you can primarily use a schema-first workflow while falling back to <code>@DgsTypeDefinitionRegistry</code> to add some dynamic parts to the schema.</p> <p>The following is an example of a <code>DgsTypeDefinitionRegistry</code>.</p> <pre><code>@DgsComponent\npublic class DynamicTypeDefinitions {\n    @DgsTypeDefinitionRegistry\n    public TypeDefinitionRegistry registry() {\n        TypeDefinitionRegistry typeDefinitionRegistry = new TypeDefinitionRegistry();\n\n        ObjectTypeExtensionDefinition query = ObjectTypeExtensionDefinition.newObjectTypeExtensionDefinition().name(\"Query\").fieldDefinition(\n                FieldDefinition.newFieldDefinition().name(\"randomNumber\").type(new TypeName(\"Int\")).build()\n        ).build();\n\n        typeDefinitionRegistry.add(query);\n\n        return typeDefinitionRegistry;\n    }\n}\n</code></pre> <p>This <code>TypeDefinitionRegistry</code> creates a field <code>randomNumber</code> on the <code>Query</code> object type. <pre><code>@DgsComponent\npublic class DynamicTypeDefinitions {\n    @DgsTypeDefinitionRegistry\n    public TypeDefinitionRegistry registry(TypeDefinitionRegistry registry) {\n        TypeDefinitionRegistry typeDefinitionRegistry = new TypeDefinitionRegistry();\n        ...\n        typeDefinitionRegistry.add(query);\n\n        return typeDefinitionRegistry;\n    }\n}\n</code></pre></p> <p>You can also pass in an existing <code>TypeDefinitionRegistry</code> as a parameter in case you need access to existing types.</p>"},{"location":"advanced/dynamic-schemas/#creating-datafetchers-programmatically","title":"Creating datafetchers programmatically","text":"<p>If you're creating schema elements dynamically, it's likely you also need to create datafetchers dynamically. You can use the <code>@DgsCodeRegistry</code> annotation to add datafetchers programmatically. A method annotated <code>@DgsCodeRegistry</code> takes two arguments:</p> <p>GraphQLCodeRegistry.Builder codeRegistryBuilder TypeDefinitionRegistry registry</p> <p>The method must return the modified GraphQLCodeRegistry.Builder.</p> <p>The following is an example of a programmatically created datafetcher for the field created in the previous example.</p> <pre><code>@DgsComponent\npublic class DynamicDataFetcher {\n    @DgsCodeRegistry\n    public GraphQLCodeRegistry.Builder registry(GraphQLCodeRegistry.Builder codeRegistryBuilder, TypeDefinitionRegistry registry) {\n        DataFetcher&lt;Integer&gt; df = (dfe) -&gt; new Random().nextInt();\n        FieldCoordinates coordinates = FieldCoordinates.coordinates(\"Query\", \"randomNumber\");\n\n        return codeRegistryBuilder.dataFetcher(coordinates, df);\n    }\n}\n</code></pre>"},{"location":"advanced/dynamic-schemas/#changing-schemas-at-runtime","title":"Changing schemas at runtime","text":"<p>It's helpful to combine creating schemas/datafetchers at runtime with dynamically re-loading the schema in some very rare use-cases. You can achieve this by implementing your own <code>ReloadSchemaIndicator</code>.  You can use an external signal (e.g., reading from a message queue) to have the framework recreate the schema by executing the <code>@DgsTypeDefinitionRegistry</code> and <code>@DgsCodeRegistry</code> again.  If these methods create the schema based on external input, you have a system that can dynamically rewire its API.</p> <p>For obvious reasons, this isn't an approach that you should use for typical APIs; stable APIs are generally the thing to aim for!</p>"},{"location":"advanced/federated-testing/","title":"Federated Testing","text":"<p>Federation allows you to extend or reference existing types in a graph.  Your DGS fulfills a part of the query based on the schema that is owned by your DGS, while the gateway is responsible for fetching data from other DGSs. </p>"},{"location":"advanced/federated-testing/#testing-federated-queries-without-the-gateway","title":"Testing Federated Queries without the Gateway","text":"<p>You can test federated queries for your DGS in isolation by replicating the format of the query that the gateway would send to your DGS.  This does not involve the gateway, and thus the parts of the query response that your DGS is not responsible for will not be hydrated.  This technique is useful if you want to verify that your DGS is able to return the appropriate data, in response to a federated query. </p> <p>Let's look at an example of a schema that extends the <code>Movie</code> type that is already defined by another DGS. <pre><code>type Movie @key(fields: \"movieId\")  @extends {\n    movieId: Int @external\n    script: MovieScript\n}\n\ntype MovieScript  {\n    title: String\n    director: String\n    actors: [Actor]\n}\n\ntype Actor {\n    name: String\n    gender: String\n    age: Int\n}\n</code></pre> Now you want to verify that your DGS is able to fulfill the Movie query by hydrating the <code>script</code> field based on the <code>movieId</code> field.  Normally, the gateway would send an _entities query in the following format: <pre><code> query ($representations: [_Any!]!) {\n        _entities(representations: $representations) {\n            ... on Movie {\n                movieId\n                script { title }\n        }}}\n</code></pre> The <code>representations</code> input is a variable map containing the <code>__typename</code> field set to <code>Movie</code> and <code>movieId</code> set to a value, e.g., <code>12345</code>.</p> <p>You can now set up a Query Executor test by either manually constructing the query, or you can generate the federated query using the <code>Entities Query Builder API</code> available through client code generation.</p> <p>Here is an example of a test that uses a manually constructed <code>_entities</code> query for <code>Movie</code>: <pre><code>@Test\n    void federatedMovieQuery() throws IOException {\n         String query = \"query ($representations: [_Any!]!) {\" +\n              \"_entities(representations: $representations) {\" +\n                  \"... on Movie {\" +\n                      \"movieId \" +\n                      \"script { title }\" +\n         \"}}}\";\n\n        Map&lt;String, Object&gt; variables = new HashMap&lt;&gt;();\n        Map&lt;String,Object&gt; representation = new HashMap&lt;&gt;();\n        representation.put(\"__typename\", \"Movie\");\n        representation.put(\"movieId\", 1);\n        variables.put(\"representations\", List.of(representation));\n\n        DocumentContext context = queryExecutor.executeAndGetDocumentContext(query, variables);\n        GraphQLResponse response = new GraphQLResponse(context.jsonString());\n        Movie movie = response.extractValueAsObject(\"data._entities[0]\", Movie.class);\n        assertThat(movie.getScript().getTitle()).isEqualTo(\"Top Secret\");\n    }\n</code></pre></p>"},{"location":"advanced/federated-testing/#using-the-entities-query-builder-api","title":"Using the Entities Query Builder API","text":"<p>Alternatively, you can generate the federated query by using EntitiesGraphQLQuery to build the graphql request in combination with the code generation plugin to generate the classes needed to use the request builder.  This provides a convenient type-safe way to build your queries.</p> <p>To set up code generation to generate the required classes to use for building your queries, follow the instructions here.</p> <p>You will also need to add <code>com.netflix.graphql.dgs:graphql-dgs-client:latest.release</code> dependency to build.gradle.  </p> <p>Now we can write a test that uses <code>EntitiesGraphQLQuery</code> along with <code>GraphQLQueryRequest</code> and <code>EntitiesProjectionRoot</code> to build the query. Finally, you can also extract the response using <code>GraphQLResponse</code>. </p> <p>This set up is shown here: <pre><code>@Test\n    void federatedMovieQueryAPI() throws IOException {\n        // constructs the _entities query with variable $representations containing a \n        // movie representation that represents { __typename: \"Movie\"  movieId: 12345 }\n        EntitiesGraphQLQuery entitiesQuery = new EntitiesGraphQLQuery.Builder()\n                    .addRepresentationAsVariable(\n                            MovieRepresentation.newBuilder().movieId(1122).build()\n                    )\n                    .build();\n        // sets up the query and the field selection set using the EntitiesProjectionRoot\n        GraphQLQueryRequest request = new GraphQLQueryRequest(\n                    entitiesQuery,\n                    new EntitiesProjectionRoot().onMovie().movieId().script().title());\n\n        String query  = request.serialize();\n        // pass in the constructed _entities query with the variable map containing representations\n        DocumentContext context = queryExecutor.executeAndGetDocumentContext(query, entitiesQuery.getVariables());\n\n        GraphQLResponse response = new GraphQLResponse(context.jsonString());\n        Movie movie = response.extractValueAsObject(\"data._entities[0]\", Movie.class);\n        assertThat(movie.getScript().getTitle()).isEqualTo(\"Top Secret\");\n    }\n</code></pre> Check out this video for a demo on how to configure and write the above test.</p>"},{"location":"advanced/file-uploads/","title":"File Uploads","text":"<p>In GraphQL, you model a file upload operation as a GraphQL mutation request from a client to your DGS.</p> <p>The following sections describe how you implement file uploads and downloads using a Multipart POST request. For more context on file uploads and best practices, see Apollo Server File Upload Best Practices by Khalil Stemmler from Apollo Blog.</p>"},{"location":"advanced/file-uploads/#file-uploads-using-spring-graphql-starter","title":"File Uploads using Spring GraphQL starter","text":"<p>If you are using the Spring GraphQL integration, you will need to add an explicit dependency for file uploads functionality. Specifically, you need to add this to your <code>build.gradle</code></p> <pre><code>dependencies {\n    implementation(\"name.nkonev.multipart-spring-graphql:multipart-spring-graphql:version\")\n}\n</code></pre> <p>If you are using the regular DGS starter, you don't need to do add that dependency.</p>"},{"location":"advanced/file-uploads/#multipart-file-upload","title":"Multipart File Upload","text":"<p>A multipart request is an HTTP request that contains multiple parts in a single request: the mutation query, file data, JSON objects, and whatever else you like. You can use Apollo\u2019s upload client, or even a simple cURL, to send along a stream of file data using a multipart request that you model in your schema as a Mutation.</p> <p></p> <p>See GraphQL multipart request specification for the specification of a multipart <code>POST</code> request for uploading files using GraphQL mutations.</p> <p>The DGS framework supports the <code>Upload</code> scalar with which you can specify files in your mutation query as a <code>MultipartFile</code>. When you send a multipart request for file upload, the framework processes each part and assembles the final GraphQL query that it hands to your data fetcher for further processing.</p> <p>Here is an example of a Mutation query that uploads a file to your DGS:</p> <pre><code>scalar Upload\n\nextend type Mutation  {\n    uploadScriptWithMultipartPOST(input: Upload!): Boolean\n}\n</code></pre> <p>Note that you need to declare the <code>Upload</code> scalar in your schema, although the implementation is provided by the DGS framework. In your DGS, add a data fetcher to handle this as a <code>MultipartFile</code> as shown here:</p> <pre><code>@DgsData(parentType = DgsConstants.MUTATION.TYPE_NAME, field = \"uploadScriptWithMultipartPOST\")\n    public boolean uploadScript(DataFetchingEnvironment dfe) throws IOException {\n        // NOTE: Cannot use @InputArgument  or Object Mapper to convert to class, because MultipartFile cannot be\n        // deserialized\n        MultipartFile file = dfe.getArgument(\"input\");\n        String content = new String(file.getBytes());\n        return ! content.isEmpty();\n    }\n</code></pre> <p>Note that you will not be able to use a Jackson object mapper to deserialize a type that contains a <code>MultipartFile</code>, so you will need to explicitly get the file argument from your input.</p> <p>On your client, you can use <code>apollo-upload-client</code> to send your Mutation query as a multipart <code>POST</code> request with file data. Here\u2019s how you configure your link:</p> <pre><code>import { createUploadLink } from 'apollo-upload-client'\n\nconst uploadLink = createUploadLink({ uri: uri })\n\nconst authedClient = authLink &amp;&amp; new ApolloClient({\n        link: uploadLink)),\n        cache: new InMemoryCache()\n})\n</code></pre> <p>Once you set this up, set up your Mutation query and the pass the file that the user selected as a variable:</p> <pre><code>// query for file uploads using multipart post\nconst UploadScriptMultipartMutation_gql = gql`\n  mutation uploadScriptWithMultipartPOST($input: Upload!) {\n    uploadScriptWithMultipartPOST(input: $input)\n  }\n`;\n\nfunction MultipartScriptUpload() {\n  const [\n    uploadScriptMultipartMutation,\n    {\n      loading: mutationLoading,\n      error: mutationError,\n      data: mutationData,\n    },\n  ] = useMutation(UploadScriptMultipartMutation_gql);\n  const [scriptMultipartInput, setScriptMultipartInput] = useState&lt;any&gt;();\n\n  const onSubmitScriptMultipart = () =&gt; {\n    const fileInput = scriptMultipartInput.files[0];\n    uploadScriptMultipartMutation({\n      variables: { input: fileInput },\n    });\n  };\n\n  return (\n    &lt;div&gt;\n      &lt;h3&gt; Upload script using multipart HTTP POST&lt;/h3&gt;\n      &lt;form\n        onSubmit={e =&gt; {\n          e.preventDefault();\n          onSubmitScriptMultipart();\n        }}&gt;\n        &lt;label&gt;\n          &lt;input\n            type=\"file\"\n            ref={ref =&gt; {\n              setScriptMultipartInput(ref!);\n            }}\n          /&gt;\n        &lt;/label&gt;\n        &lt;br /&gt;\n        &lt;br /&gt;\n        &lt;button type=\"submit\"&gt;Submit&lt;/button&gt;\n      &lt;/form&gt;\n    &lt;/div&gt;\n  );\n}\n</code></pre> <p>The <code>Upload</code> scalar is mapped to <code>MultipartFile</code> in the <code>build.gradle</code> file of the Codegen plugin. You can rename or remove the <code>Upload</code> scalar by modifying this <code>typeMapping</code>. Read more about the <code>typeMapping</code> configuration here. <pre><code>generateJava {\n    typeMapping = [Upload: \"org.springframework.web.multipart.MultipartFile\"]\n}\n</code></pre></p>"},{"location":"advanced/graphqlcontext-leveraging/","title":"Leveraging GraphQLContext","text":"<p>As of graphql-java version 17, GraphQLContext is now the approved context mechanism (replacing the previously used opaque Context) allowing passing of pertinent context via key/value pairs to provide frameworks and userspace code the capability to contribute, share and leverage various  pieces of context independent of each other.</p> <p>Note that if you simply require interception/modification of request and response headers, you can use a <code>WebGraphQlInterceptor</code> provided by Spring GraphQL as described here.</p> <p>To make this easily leverageable by DGS customers, a new interface has been provided for which any Spring Beans registered  that implement the <code>GraphQLContextContributor</code>  interface, their <code>contribute</code> method will be invoked before any normal instrumentation classes are invoked, allowing  implementors to inspect anything provided via the DgsRequestData (i.e.: http headers), and set values on the provided  GraphQLContext via a provided GraphQLContext.Builder.</p>"},{"location":"advanced/graphqlcontext-leveraging/#example","title":"Example:","text":"<pre><code>@Component\npublic class ExampleGraphQLContextContributor implements GraphQLContextContributor {\n\n    @Override\n    public void contribute(@NotNull GraphQLContext.Builder builder, @Nullable Map&lt;String, ?&gt; extensions, @Nullable DgsRequestData dgsRequestData) {\n        if (dgsRequestData != null &amp;&amp; dgsRequestData.getHeaders() != null) {\n            String contributedContextHeader = dgsRequestData.getHeaders().getFirst(\"x-context-contributor-header\");\n            if (CONTEXT_CONTRIBUTOR_HEADER_VALUE.equals(\"enabled\")) {\n                builder.put('exampleContributorEnabled', \"true\");\n            }\n        }\n    }\n}\n</code></pre> <p>You can now also use the GraphQLContext with the data loader as of DGS Framework 6.0.0: <pre><code>@DgsDataLoader(name = \"exampleLoaderWithGraphQLContext\")\npublic class ExampleLoaderWithGraphQLContext implements BatchLoaderWithContext&lt;String, String&gt; {\n    @Override\n    public CompletionStage&lt;List&lt;String&gt;&gt; load(List&lt;String&gt; keys, BatchLoaderEnvironment environment) {\n        GraphQLContext graphQLContext = environment.getContext();\n        return CompletableFuture.supplyAsync(() -&gt; keys.stream().map((Function&lt;String, String&gt;) graphQLContext::get).collect(Collectors.toList()));\n    }\n}\n</code></pre></p>"},{"location":"advanced/instrumentation/","title":"Adding instrumentation for tracing and logging","text":"<p>It can be extremely valuable to add tracing, metrics and logging to your GraphQL API. At Netflix we publish tracing spans and metrics for each datafetcher to our distributed tracing/metrics backends, and log queries and query results to our logging backend. The implementations we use at Netflix are highly specific for our infrastructure, but it's easy to add your own to the framework!</p> <p>Internally the DGS framework uses GraphQL Java. GraphQL Java supports the concept of <code>instrumentation</code>. In the DGS framework we can easily add one or more instrumentation classes by implementing the <code>graphql.execution.instrumentation.Instrumentation</code> interface and register the class as <code>@Component</code>. The easiest way to implement the <code>Instrumentation</code> interface is to extend <code>graphql.execution.instrumentation.SimpleInstrumentation</code>.</p> <p>The following is an example of an implementation that outputs the execution time for each data fetcher, and the total query execution time, to the logs. Most likely you would want to replace the log output with writing to your tracing/metrics backend. Note that the code example accounts for async data fetchers. If we wouldn't do this, the result for an async data fetcher would always be 0, because the actual processing happens later.</p> JavaKotlin <pre><code>@Component\npublic class ExampleTracingInstrumentation extends SimpleInstrumentation {\n    private final static Logger LOGGER = LoggerFactory.getLogger(ExampleTracingInstrumentation.class);\n\n    @Override\n    public InstrumentationState createState() {\n        return new TracingState();\n    }\n\n    @Override\n    public InstrumentationContext&lt;ExecutionResult&gt; beginExecution(InstrumentationExecutionParameters parameters) {\n        TracingState tracingState = parameters.getInstrumentationState();\n        tracingState.startTime = System.currentTimeMillis();\n        return super.beginExecution(parameters);\n    }\n\n    @Override\n    public DataFetcher&lt;?&gt; instrumentDataFetcher(DataFetcher&lt;?&gt; dataFetcher, InstrumentationFieldFetchParameters parameters) {\n        // We only care about user code\n        if(parameters.isTrivialDataFetcher()) {\n            return dataFetcher;\n        }\n\n        return environment -&gt; {\n            long startTime = System.currentTimeMillis();\n            Object result = dataFetcher.get(environment);\n            if(result instanceof CompletableFuture) {\n                ((CompletableFuture&lt;?&gt;) result).whenComplete((r, ex) -&gt; {\n                    long totalTime = System.currentTimeMillis() - startTime;\n                    LOGGER.info(\"Async datafetcher {} took {}ms\", findDatafetcherTag(parameters), totalTime);\n                });\n            } else {\n                long totalTime = System.currentTimeMillis() - startTime;\n                LOGGER.info(\"Datafetcher {} took {}ms\", findDatafetcherTag(parameters), totalTime);\n            }\n\n            return result;\n        };\n    }\n\n    @Override\n    public CompletableFuture&lt;ExecutionResult&gt; instrumentExecutionResult(ExecutionResult executionResult, InstrumentationExecutionParameters parameters) {\n        TracingState tracingState = parameters.getInstrumentationState();\n        long totalTime = System.currentTimeMillis() - tracingState.startTime;\n        LOGGER.info(\"Total execution time: {}ms\", totalTime);\n\n        return super.instrumentExecutionResult(executionResult, parameters);\n    }\n\n    private String findDatafetcherTag(InstrumentationFieldFetchParameters parameters) {\n        GraphQLOutputType type = parameters.getExecutionStepInfo().getParent().getType();\n        GraphQLObjectType parent;\n        if (type instanceof GraphQLNonNull) {\n            parent = (GraphQLObjectType) ((GraphQLNonNull) type).getWrappedType();\n        } else {\n            parent = (GraphQLObjectType) type;\n        }\n\n        return  parent.getName() + \".\" + parameters.getExecutionStepInfo().getPath().getSegmentName();\n    }\n\n    static class TracingState implements InstrumentationState {\n        long startTime;\n    }\n}\n</code></pre> <pre><code>@Component\nclass ExampleTracingInstrumentation: SimpleInstrumentation() {\n\n    val logger : Logger = LoggerFactory.getLogger(ExampleTracingInstrumentation::class.java)\n\n    override fun createState(): InstrumentationState {\n        return TraceState()\n    }\n\n    override fun beginExecution(parameters: InstrumentationExecutionParameters): InstrumentationContext&lt;ExecutionResult&gt; {\n        val state: TraceState = parameters.getInstrumentationState()\n        state.traceStartTime = System.currentTimeMillis()\n\n        return super.beginExecution(parameters)\n    }\n\n    override fun instrumentDataFetcher(dataFetcher: DataFetcher&lt;*&gt;, parameters: InstrumentationFieldFetchParameters): DataFetcher&lt;*&gt; {\n        // We only care about user code\n        if(parameters.isTrivialDataFetcher) {\n            return dataFetcher\n        }\n\n        val dataFetcherName = findDatafetcherTag(parameters)\n\n        return DataFetcher { environment -&gt;\n            val startTime = System.currentTimeMillis()\n            val result = dataFetcher.get(environment)\n            if(result is CompletableFuture&lt;*&gt;) {\n                result.whenComplete { _,_ -&gt;\n                    val totalTime = System.currentTimeMillis() - startTime\n                    logger.info(\"Async datafetcher '$dataFetcherName' took ${totalTime}ms\")\n                }\n            } else {\n                val totalTime = System.currentTimeMillis() - startTime\n                logger.info(\"Datafetcher '$dataFetcherName': ${totalTime}ms\")\n            }\n\n            result\n        }\n    }\n\n    override fun instrumentExecutionResult(executionResult: ExecutionResult, parameters: InstrumentationExecutionParameters): CompletableFuture&lt;ExecutionResult&gt; {\n        val state: TraceState = parameters.getInstrumentationState()\n        val totalTime = System.currentTimeMillis() - state.traceStartTime\n        logger.info(\"Total execution time: ${totalTime}ms\")\n\n        return super.instrumentExecutionResult(executionResult, parameters)\n    }\n\n    private fun findDatafetcherTag(parameters: InstrumentationFieldFetchParameters): String {\n        val type = parameters.executionStepInfo.parent.type\n        val parentType = if (type is GraphQLNonNull) {\n            type.wrappedType as GraphQLObjectType\n        } else {\n            type as GraphQLObjectType\n        }\n\n        return \"${parentType.name}.${parameters.executionStepInfo.path.segmentName}\"\n    }\n\n    data class TraceState(var traceStartTime: Long = 0): InstrumentationState\n}\n</code></pre> <p>Datafetcher 'Query.shows': 0ms</p> <p>Total execution time: 3ms</p>"},{"location":"advanced/instrumentation/#enabling-apollo-tracing","title":"Enabling Apollo Tracing","text":"<p>If you want to leverage Apollo Tracing, as supported by <code>graphql-java</code>, you can create a bean of type <code>TracingInstrumentation</code>. In this example, we added a conditional property on the bean to enable/disable the Apollo Tracing. This property is enabled by default, but you can turn it off by setting <code>graphql.tracing.enabled=false</code> in your application properties.</p> <pre><code>import graphql.execution.instrumentation.tracing.TracingInstrumentation;\n\n@SpringBootApplication\npublic class ReviewsDgs {\n    @Bean\n    @ConditionalOnProperty( prefix = \"graphql.tracing\", name = \"enabled\", matchIfMissing = true)\n    public Instrumentation tracingInstrumentation(){\n        return new TracingInstrumentation();\n    }\n\n}\n</code></pre> <p>For federated tracing, you will need to use the instrumentation provided by Apollo's jvm federation library <pre><code>import com.apollographql.federation.graphqljava.tracing.FederatedTracingInstrumentation;\n\n@SpringBootApplication\npublic class ReviewsDgs {\n\n@Bean\n@ConditionalOnProperty( prefix = \"graphql.tracing\", name = \"enabled\", matchIfMissing = true)\n    public Instrumentation tracingInstrumentation(){\n        return new FederatedTracingInstrumentation();\n    }\n}\n</code></pre></p> <p>It's important to note that the default behavior in Apollo's JVM federation library is to trace requests, even if the federated gateway does not request it (i.e. the gateway does not add <code>FEDERATED_TRACING_HEADER_NAME</code> when forwarding the request). Including the following component in your DGS project will explicitly ask Apollo's JVM federation library to not trace a request in the event that the gateway does not request it.</p> <pre><code>@Component\npublic class ApolloFederatedTracingHeaderForwarder implements GraphQLContextContributor {\n        @Override\n        public void contribute(@NotNull GraphQLContext.Builder builder, @Nullable Map&lt;String, ?&gt; extensions, @Nullable DgsRequestData dgsRequestData) {\n            if (dgsRequestData == null || dgsRequestData.getHeaders() == null) {\n                return;\n            }\n\n            final HttpHeaders headers = dgsRequestData.getHeaders();\n\n            // if the header exists, we should just forward it.\n            if (headers.containsKey(FederatedTracingInstrumentation.FEDERATED_TRACING_HEADER_NAME)) {\n                builder.put(\n                        FederatedTracingInstrumentation.FEDERATED_TRACING_HEADER_NAME,\n                        headers\n                                .get(FederatedTracingInstrumentation.FEDERATED_TRACING_HEADER_NAME)\n                                .stream()\n                                .findFirst()\n                                .get()\n                );\n            }  else {\n                //otherwise, place a value != \"ftv1\" so when it gets checked for == ftv1 it fails\n                // and trace does not happen.\n                builder.put(FederatedTracingInstrumentation.FEDERATED_TRACING_HEADER_NAME, \"DO_NOT_TRACE\");\n            }\n        }\n}\n</code></pre>"},{"location":"advanced/instrumentation/#metrics-out-of-the-box","title":"Metrics Out of The Box","text":"<p>tl;dr</p> <ul> <li>Supported via the opt-in <code>graphql-dgs-spring-boot-micrometer</code> module.</li> <li>Provides specific GraphQL metrics such as <code>gql.query</code>, <code>gql.error</code>, and <code>gql.dataLoader</code>.</li> <li>Supports several backend implementations since it's implemented via Micrometer.</li> </ul> Gradle GroovyGradle KotlinMaven <pre><code>dependencies {\n    implementation 'com.netflix.graphql.dgs:graphql-dgs-spring-boot-micrometer'\n}\n</code></pre> <pre><code>dependencies {\n    implementation(\"com.netflix.graphql.dgs:graphql-dgs-spring-boot-micrometer\")\n}\n</code></pre> <pre><code>&lt;dependencies&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;com.netflix.graphql.dgs&lt;/groupId&gt;\n        &lt;artifactId&gt;graphql-dgs-spring-boot-micrometer&lt;/artifactId&gt;\n    &lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre> <p>Hint</p> <p>Note that the version is missing since we assume you are using the latest BOM. We recommend you use the DGS Platform BOM to handle such versions.</p>"},{"location":"advanced/instrumentation/#shared-tags","title":"Shared Tags","text":"<p>The following are tags shared across most of the meters.</p> <p>Tags:</p> tag name values description <code>gql.operation</code> QUERY, MUTATION, SUBSCRIPTION are the possible values. These represent the GraphQL operation that is executed. <code>gql.operation.name</code> GraphQL operation name if any, else <code>anonymous</code>. Since the cardinality of the value is high it will be limited. <code>gql.query.complexity</code> one in [5, 10, 20, 50, 100, 200, 500, 1000] The total number of nodes in the query. Refer to Query Complexity section. for additional information. <code>gql.query.sig.hash</code> Query Signature Hash of the query that was executed. Since the cardinality of the value is high it will be limited."},{"location":"advanced/instrumentation/#graphql-query-complexity","title":"GraphQL Query Complexity","text":"<p>The <code>gql.query.complexity</code> is typically calculated as 1 + Child's Complexity. The query complexity is valuable to calculate the cost of a query as this can vary based on input arguments to the query. The computed value is represented as one of the bucketed values to reduce the cardinality of the metric.</p> <p>Example Query:</p> <pre><code>query {\n  viewer {\n    repositories(first: 50) {\n      edges {\n        repository:node {\n          name\n\n          issues(first: 10) {\n            totalCount\n            edges {\n              node {\n                title\n                bodyHTML\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Example Calculation:</p> <pre><code>50          = 50 repositories\n+\n50 x 10     = 500 repository issues\n\n            = 550 total nodes\n</code></pre>"},{"location":"advanced/instrumentation/#graphql-query-signature-hash","title":"GraphQL Query Signature Hash","text":"<p>The Query Signature is defined as the tuple of the GraphQL AST Signature of the GraphQL Document and the GraphQL AST Signature Hash. The GraphQL AST Signature of a GraphQL Document is defined as follows:</p> <p>A canonical AST which removes excess operations, removes any field aliases, hides literal values and sorts the result into a canonical query Ref graphql-java</p> <p>The GraphQL AST Signature Hash is the Hex 256 SHA string produced by encoding the AST Signature. While we can't tag a metric by its signature, due its length, we can use the hash, as now expressed by the <code>gql.query.sig.hash</code> tag.</p> <p>There are a few configuration parameters that can change the behavior of the <code>gql.query.sig.hash</code> tag.</p> <ul> <li><code>management.metrics.dgs-graphql.query-signature.enabled</code>:    Defaulting to <code>true</code>, it enables the calculation of the  GQL Query Signature. The <code>gql.query.sig.hash</code> will express the GQL Query Signature Hash.</li> <li><code>management.metrics.dgs-graphql.query-signature.caching.enabled</code>:    Defaulting to <code>true</code>, it will cache the GQL Query Signature. If set to <code>false</code> it will just disable the cache but will    not turn the calculation of the signature off. If you want to turn such calculation off use the    <code>management.metrics.dgs-graphql.query-signature.enabled</code> property.</li> </ul>"},{"location":"advanced/instrumentation/#cardinality-limiter","title":"Cardinality Limiter","text":"<p>The cardinality of a given tag, the number of different values that a tag can express, can be problematic to servers supporting metrics. In order to prevent the cardinality of some of the tags supported out of the box there are some limiters by default. The limited tag values will only see the first 100 different values by default, from there new values will be expressed as <code>--others--</code>.</p> <p>You can change the limiter via the following configuration:</p> <ul> <li><code>management.metrics.dgs-graphql.tags.limiter.limit</code>: Defaults to<code>100</code>, sets the number of different values expressed per limited tag.</li> </ul> <p>Not all tags are limited, currently, only following are:</p> <ul> <li><code>gql.operation.name</code></li> <li><code>gql.query.sig.hash</code></li> </ul>"},{"location":"advanced/instrumentation/#query-timer-gqlquery","title":"Query Timer: gql.query","text":"<p>Captures the elapsed time that a given GraphQL query, or mutation, takes.</p> <p>Name: <code>gql.query</code></p> <p>Tags:</p> tag name values description <code>outcome</code> <code>success</code> or <code>failure</code> Result of the operation, as defined by the ExecutionResult."},{"location":"advanced/instrumentation/#error-counter-gqlerror","title":"Error Counter: gql.error","text":"<p>Captures the number of GraphQL errors encountered during query execution of a query or mutation. Remember that one graphql request can have multiple errors.</p> <p>Name: <code>gql.error</code></p> <p>Tags:</p> tag name description <code>gql.errorCode</code> The GraphQL error code, such as <code>VALIDATION</code>, <code>INTERNAL</code>, etc. <code>gql.path</code> The sanitized query path that resulted in the error. <code>gql.errorDetail</code> Optional flag containing additional details, if present."},{"location":"advanced/instrumentation/#data-loader-timer-gqldataloader","title":"Data Loader Timer: gql.dataLoader","text":"<p>Captures the elapsed time for a data loader invocation for a batch of queries. This is useful if you want to find data loaders that might be responsible for poor query performance.</p> <p>Name: <code>gql.dataLoader</code></p> <p>Tags:</p> tag name description <code>gql.loaderName</code> The name of the data loader, may or may not be the same as the type of entity. <code>gql.loaderBatchSize</code> The number of queries executed in the batch."},{"location":"advanced/instrumentation/#data-fetcher-timer-gqlresolver","title":"Data Fetcher Timer: gql.resolver","text":"<p>Captures the elapsed time of each data fetcher invocation. This is useful if you want to find data fetchers that might be responsible for poor query performance. That said, there might be times where you want to remove a data fetcher from being measured/included in this meter. You can do so by annotating the method with <code>@DgsEnableDataFetcherInstrumentation(false)</code>.</p> <p>Info</p> <p>This metric is not available if:</p> <ul> <li>The data is resolved via a Batch Loader.</li> <li>The method is annotated with <code>@DgsEnableDataFetcherInstrumentation(false)</code>.</li> <li>The DataFetcher is TrivialDataFetcher. A trivial DataFetcher is one that simply maps data from an object to a field.   This is defined directly in <code>graphql-java</code>.</li> </ul> <p>Name: <code>gql.resolver</code></p> <p>Tags:</p> tag name description <code>gql.field</code> Name of the data fetcher. This has the <code>${parentType}.${field}</code> format as specified in the <code>@DgsData</code> annotation."},{"location":"advanced/instrumentation/#data-fetcher-timer-as-a-counter","title":"Data Fetcher Timer as a Counter","text":"<p>The data fetcher, or resolver, timer can be used as a counter as well. If used in this manner it will reflect the number of invocations of each data fetcher. This is useful if you want to find out which data fetchers are used often.</p>"},{"location":"advanced/instrumentation/#further-tag-customization","title":"Further Tag Customization","text":"<p>You can customize the tags applied to the metrics above by providing beans that implement the following functional interfaces.</p> Interface Description <code>DgsContextualTagCustomizer</code> Used to add common, contextual tags. Example of these could be used to describe the deployment environment, application profile, application version, etc <code>DgsExecutionTagCustomizer</code> Used to add tags specific to the ExecutionResult of the query. The SimpleGqlOutcomeTagCustomizer is an example of this. <code>DgsFieldFetchTagCustomizer</code> Used to add tags specific to the execution of data fetchers. The SimpleGqlOutcomeTagCustomizer is an example of this as well. <p>Important</p> <p>Note that backend metrics systems commonly have cardinality limits for additional tags. Please check with your backend metrics provider to ensure additional tag customizations do not exceed tag limits.</p>"},{"location":"advanced/instrumentation/#additional-metrics-configuration","title":"Additional Metrics Configuration","text":"<ul> <li><code>management.metrics.dgs-graphql.enabled</code>: Enables the metrics provided out of the box; defaults to <code>true</code>.</li> <li><code>management.metrics.dgs-graphql.tag-customizers.outcome.enabled</code>: Enables the tag customizer that will label the <code>gql.query</code> and <code>gql.resolver</code> timers with an <code>outcome</code> reflecting    the result of the GraphQL outcome, either <code>success</code> or <code>failure</code>; defaults to <code>true</code>.</li> <li><code>management.metrics.dgs-graphql.data-loader-instrumentation.enabled</code>: Enables instrumentation of data loaders; defaults to <code>true</code>.</li> <li><code>management.metrics.dgs-graphql.resolver.enabled</code>: Enables instrumentation of graphql resolvers; defaults to <code>true</code>.</li> <li><code>management.metrics.dgs-graphql.query.enabled</code>: Enables instrumentation of graphql query; defaults to <code>true</code>.</li> </ul>"},{"location":"advanced/intercepting-http-request-response/","title":"Request and Response Header Instrumentation","text":"<p>The DGS framework internally uses <code>GraphQL Java</code> and <code>Spring for GraphQL</code>.</p> <p>If you need to modify the HTTP request and response headers, you can leverage the <code>WebGraphQlInterceptor</code> in Spring for GraphQL to accomplish this. This provides a hook to update the request and response headers based using the <code>GraphQLContext</code></p>"},{"location":"advanced/intercepting-http-request-response/#example","title":"Example:","text":"<pre><code>@Component\npublic class MyInterceptor implements WebGraphQlInterceptor {\n    @Override\n    public Mono&lt;WebGraphQlResponse&gt; intercept(WebGraphQlRequest request, Chain chain) {\n        String value = request.getHeaders().getFirst(\"myHeader\");\n        request.configureExecutionInput((executionInput, builder) -&gt;\n                builder.graphQLContext(Collections.singletonMap(\"myHeader\", value)).build());\n        return chain.next(request).doOnNext((response) -&gt; {\n            String value = response.getExecutionInput().getGraphQLContext().get(\"myContext\");\n            response.getResponseHeaders().add(\"MyContext\", value);\n        });\n    }\n}\n</code></pre>"},{"location":"advanced/java-client/","title":"Java GraphQL Client","text":""},{"location":"advanced/java-client/#usage","title":"Usage","text":"<p>The DGS framework provides a GraphQL client that can be used to retrieve data from a GraphQL endpoint. The client is also useful for integration testing a DGS. The client has two components, each usable by itself, or in combination together.</p> <ul> <li>GraphQL Client - A HTTP client wrapper that provides easy parsing of GraphQL responses</li> <li>Query API codegen - Generate type-safe Query builders</li> </ul> <p>The client has multiple interfaces and implementations for different needs. The interfaces are the following:</p> <ul> <li>GraphQLClient - Client interface for blocking implementations. Only recommended when using a blocking HTTP client.</li> <li>MonoGraphQLClient - The same as GraphQLClient, but based on the reactive <code>Mono</code> interface meant for non-blocking implementations. Comes with an out-of-the-box WebClient implementation: <code>MonoGraphQLClient.createWithWebClient(...)</code>.</li> <li>ReactiveGraphQLClient - Client interface for streaming responses such as Subscriptions, where multiple results are expected. Based on the reactive <code>Flux</code> interface. Implemented by <code>SSESubscriptionGraphQLClient</code> and <code>WebSocketGraphQLClient</code>.</li> </ul>"},{"location":"advanced/java-client/#graphql-client-with-webclient","title":"GraphQL Client with WebClient","text":"<p>The easiest way to use the DGS GraphQL Client is to use the WebClient implementation. WebClient is the recommended HTTP client in Spring, and is the best choice for most use cases, unless you have a specific reason to use a different HTTP client. Because WebClient is Reactive, the client returns a <code>Mono</code> for all operations.</p> Java <pre><code>    //Configure a WebClient for your needs, e.g. including authentication headers and TLS.\n    WebClient webClient = WebClient.create(\"http://localhost:8080/graphql\");\n    WebClientGraphQLClient client = MonoGraphQLClient.createWithWebClient(webClient);\n\n    //The GraphQLResponse contains data and errors.\n    Mono&lt;GraphQLResponse&gt; graphQLResponseMono = graphQLClient.reactiveExecuteQuery(query);\n\n    //GraphQLResponse has convenience methods to extract fields using JsonPath.\n    Mono&lt;String&gt; somefield = graphQLResponseMono.map(r -&gt; r.extractValue(\"somefield\"));\n\n    //Don't forget to subscribe! The request won't be executed otherwise.\n    somefield.subscribe();\n</code></pre> Kotlin <pre><code>    //Configure a WebClient for your needs, e.g. including authentication headers and TLS.\n    val client = MonoGraphQLClient.createWithWebClient(WebClient.create(\"http://localhost:8080/graphql\"))\n\n    //Executing the query returns a Mono of GraphQLResponse.\n    val result = client.reactiveExecuteQuery(\"{hello}\").map { r -&gt; r.extractValue&lt;String&gt;(\"hello\") }\n\n    //Don't forget to subscribe! The request won't be executed otherwise.\n    somefield.subscribe();\n</code></pre> <p>The <code>reactiveExecuteQuery</code> method takes a query String as input, and optionally a Map of variables and an operation name. Instead of using a query String, you can use code generation to create a type-safe query builder API.</p> <p>The <code>GraphQLResponse</code> provides methods to parse and retrieve data and errors in a variety of ways. Refer to the <code>GraphQLResponse</code> JavaDoc for the complete list of supported methods.</p> method description example getData Get the data as a Map <code>Map&lt;String,Object&gt; data = response.getData()</code> dataAsObject Parse data as the provided class, using the Jackson Object Mapper <code>TickResponse data = response.dataAsObject(TicksResponse.class)</code> extractValue Extract values given a JsonPath. The return type will be whatever type you expect, but depends on the JSON shape. For JSON objects, a Map is returned. Although this looks type safe, it really isn't. It's mostly useful for \"simple\" types like String, Int etc., and Lists of those types. <code>List&lt;String&gt; name = response.extractValue(\"movies[*].originalTitle\")</code> extractValueAsObject Extract values given a JsonPath and deserialize into the given class <code>Ticks ticks = response.extractValueAsObject(\"ticks\", Ticks.class)</code> extractValueAsObject Extract values given a JsonPath and deserialize into the given TypeRef. Useful for Maps and Lists of a certain class. <code>List&lt;Route&gt; routes = response.extractValueAsObject(\"ticks.edges[*].node.route\", new TypeRef&lt;List&lt;Route&gt;&gt;(){})</code> getRequestDetails Extract a <code>RequestDetails</code> object. This only works if requestDetails was requested in the query, and against the Gateway. RequestDetails requestDetails = <code>response.getRequestDetails()</code> getParsed Get the parsed <code>DocumentContext</code> for further JsonPath processing <code>response.getDocumentContext()</code> <p>The client can be used against any GraphQL endpoint (it doesn't have to be implemented with the DGS framework), but provides extra conveniences for parsing Gateway and DGS responses. This includes support for the Errors Spec.</p>"},{"location":"advanced/java-client/#headers","title":"Headers","text":"<p>HTTP headers can easily be added to the request.</p> <pre><code>WebClientGraphQLClient client = MonoGraphQLClient.createWithWebClient(webClient, headers -&gt; headers.add(\"myheader\", \"test\"));\n</code></pre> <p>By default, the client already sets the <code>Content-type</code> and <code>Accept</code> headers.</p>"},{"location":"advanced/java-client/#errors","title":"Errors","text":"<p>The GraphQLClient checks both for HTTP level errors (based on the response status code) and the <code>errors</code> block in a GraphQL response. The GraphQLClient is compatible with the Errors Spec used by the Gateway and DGS, and makes it easy to extract error information such as the ErrorType.</p> <p>For example, for following GraphQL response the GraphQLClient lets you easily get the ErrorType and ErrorDetail fields. Note that the <code>ErrorType</code> is an enum as specified by the Errors Spec.</p> <pre><code>{\n  \"errors\": [\n    {\n      \"message\": \"java.lang.RuntimeException: test\",\n      \"locations\": [],\n      \"path\": [\n        \"hello\"\n      ],\n      \"extensions\": {\n        \"errorType\": \"BAD_REQUEST\",\n        \"errorDetail\": \"FIELD_NOT_FOUND\"\n      }\n    }\n  ],\n  \"data\": {\n    \"hello\": null\n  }\n}\n</code></pre> <pre><code>assertThat(graphQLResponse.errors.get(0).extensions.errorType).isEqualTo(ErrorType.BAD_REQUEST)\nassertThat(graphQLResponse.errors.get(0).extensions.errorDetail).isEqualTo(\"FIELD_NOT_FOUND\")\n</code></pre>"},{"location":"advanced/java-client/#plug-in-your-own-http-client","title":"Plug in your own HTTP client","text":"<p>Instead of using WebClient, you can also plug in your own HTTP client. This is useful if you already have a configured client for your backend with authN/authZ, TLS, etc. In this case you are responsible for making the actual request, but the GraphQL client wraps the HTTP client and provides easy parsing of GraphQL responses.</p> <p>There are two interfaces that you can pick from:</p> <ul> <li>GraphQLClient: For blocking HTTP clients</li> <li>MonoGraphQLClient: For non-blocking HTTP clients</li> </ul> <p>Both interfaces return a <code>GraphQLResponse</code> for each query execution, but <code>MonoGraphQLClient</code> wraps the result in a <code>Mono</code>, making it a better fit for non-blocking clients. Create an instance by using the factory method on the interface. This returns an instance of <code>CustomGraphQLClient</code> or <code>CustomMonoGraphQLClient</code>. The implementations are named <code>Custom*</code> to indicate you need to provide handling of the actual HTTP request.</p> JavaKotlin <pre><code>CustomGraphQLClient client = GraphQLClient.createCustom(url,  (url, headers, body) -&gt; {\n    HttpHeaders httpHeaders = new HttpHeaders();\n    headers.forEach(httpHeaders::addAll);\n    ResponseEntity&lt;String&gt; exchange = restTemplate.exchange(url, HttpMethod.POST, new HttpEntity&lt;&gt;(body, httpHeaders),String.class);\n    return new HttpResponse(exchange.getStatusCodeValue(), exchange.getBody());\n});\n\nGraphQLResponse graphQLResponse = client.executeQuery(query, emptyMap(), \"SubmitReview\");\nString submittedBy = graphQLResponse.extractValueAsObject(\"submitReview.submittedBy\", String.class);\n</code></pre> <pre><code>//Configure your HTTP client\nval restTemplate = RestTemplate();\n\nval client = GraphQLClient.createCustom(\"http://localhost:8080/graphql\") { url, headers, body -&gt;\n    //Prepare the request, e.g. set up headers.\n    val httpHeaders = HttpHeaders()\n    headers.forEach { httpHeaders.addAll(it.key, it.value) }\n\n    //Use your HTTP client to send the request to the server.\n    val exchange = restTemplate.exchange(url, HttpMethod.POST, HttpEntity(body, httpHeaders), String::class.java)\n\n    //Transform the response into a HttpResponse\n    HttpResponse(exchange.statusCodeValue, exchange.body)\n}\n\n//Send a query and extract a value out of the result.\nval result = client.executeQuery(\"{hello}\").extractValue&lt;String&gt;(\"hello\")\n</code></pre> <p>Alternatively, use <code>MonoGraphQLClient.createCustomReactive(...)</code> to create the reactive equivalent. The provided <code>RequestExecutor</code> must now return <code>Mono&lt;HttpResponse&gt;</code>.</p> <pre><code>CustomMonoGraphQLClient client = MonoGraphQLClient.createCustomReactive(url, (requestUrl, headers, body) -&gt; {\n    HttpHeaders httpHeaders = new HttpHeaders();\n    headers.forEach(httpHeaders::addAll);\n    ResponseEntity&lt;String&gt; exchange = restTemplate.exchange(url, HttpMethod.POST, new HttpEntity&lt;&gt;(body, httpHeaders),String.class);\n    return Mono.just(new HttpResponse(exchange.getStatusCodeValue(), exchange.getBody(), exchange.getHeaders()));\n});\nMono&lt;GraphQLResponse&gt; graphQLResponse = client.reactiveExecuteQuery(query, emptyMap(), \"SubmitReview\");\nString submittedBy = graphQLResponse.map(r -&gt; r.extractValueAsObject(\"submitReview.submittedBy\", String.class)).block();\n</code></pre> <p>Note that in this example we just use <code>Mono.just</code> to create a Mono. This doesn't make the call non-blocking.</p>"},{"location":"advanced/java-client/#migrating-from-defaultgraphqlclient","title":"Migrating from DefaultGraphQLClient","text":"<p>In previous versions of the framework we provided the <code>DefaultGraphQLClient</code> class. This has been deprecated for the following reasons:</p> <ul> <li>The \"Default\" in the name suggested that it should be the implementation for most use cases. However, the new WebClient implementation is a much better option now. Naming things is hard.</li> <li>The API required you to pass in the <code>RequestExecutor</code> for each query execution. This wasn't ergonomic for the new WebClient implementation, because no <code>RequestExecutor</code> is required.</li> </ul> <p>If you want to migrate existing usage of <code>DefaultGraphQLClient</code> you can either use the WebClient implementation and get rid of your <code>RequestExecutor</code> entirely, or alternatively use <code>CustomGraphQLClient</code> / <code>CustomMonoGraphQLClient</code> which has almost the same API. To migrate to <code>CustomGraphQLClient</code> you pass in your existing <code>RequestExecutor</code> to the <code>GraphQLClient.createCustom(url, requestExecutor)</code> factory method, and remove it from the <code>executeQuery</code> methods.</p> <p>We plan to eventually remove the <code>DefaultGraphQLClient</code>, because its API is confusing.</p>"},{"location":"advanced/java-client/#type-safe-query-api","title":"Type safe Query API","text":"<p>Based on a GraphQL schema a type safe query API can be generated for Java/Kotlin. The generated API is a builder style API that lets you build a GraphQL query, and it's projection (field selection). Because the code gets re-generated when the schema changes, it helps catch errors in the query. It's arguably also more readable, although multiline String support in Java and Kotlin do mitigate that issue as well.</p> <p>If you own a DGS and want to generate a client for this DGS (e.g. for testing purposes) the client generation is just an extra property on the Codegen configuration. Specify the following in your <code>build.gradle</code>.</p> <pre><code>// Using plugins DSL\nplugins {\n    id \"com.netflix.dgs.codegen\" version \"[REPLACE_WITH_CODEGEN_PLUGIN_VERSION]\"\n}\n\ngenerateJava{\n   packageName = 'com.example.packagename' // The package name to use to generate sources\n   generateClient = true\n}\n</code></pre> <p>Code will be generated on build, and the generated code will be under <code>builder/generated</code>, which is added to the classpath by the plugin.</p> <p>With codegen configured correctly, a builder style API will be generated when building the project. Using the same query example as above, the query can be build using the generated builder API.</p> <pre><code>GraphQLQueryRequest graphQLQueryRequest =\n                new GraphQLQueryRequest(\n                    new TicksGraphQLQuery.Builder()\n                        .first(first)\n                        .after(after)\n                        .build(),\n                    new TicksConnectionProjectionRoot()\n                        .edges()\n                            .node()\n                                .date()\n                                .route()\n                                    .name()\n                                    .votes()\n                                        .starRating()\n                                        .parent()\n                                    .grade());\n\nString query = graphQLQueryRequest.serialize();\n</code></pre> <p>The <code>GraphQLQueryRequest</code> is a class made available by the Codegen plugin, specifically the <code>graphql-dgs-codegen-shared-core</code> module. Such module will be added as an implementation dependency if the plugin is applied to the project. The <code>TicksGraphQLQuery</code> and <code>TicksConnectionProjectionRoot</code> are generated. After building the query, it can be serialized to a String, and executed using the <code>GraphQLClient</code>.</p> <p>Note that the <code>edges</code> and <code>node</code> fields are because the example schema is using Relay pagination.</p>"},{"location":"advanced/java-client/#scalars-in-dgs-client","title":"Scalars in DGS Client","text":"<p>Custom scalars can be used in input types in GraphQL. Let's take the example of a <code>DateTimeScalar</code> (created in Adding Custom Scalars). In Java, we want to represent this as a <code>LocalDateTime</code> class. When sending the query, we somehow have to serialize this. There are many ways to represent a date, so how do we make sure that we use the same representation as the server expects?</p> <p>In this release we added an optional <code>scalars</code> argument to the <code>GraphQLQueryRequest</code> constructor. This is a <code>Map&lt;Class&lt;?&gt;, Coercing&lt;?,?&gt;&gt;</code> that maps the Java class representing the input to an actual Scalar implementation. We will generate the query API with <code>DateTimeScalar</code> as follows:</p> <pre><code>Map&lt;Class&lt;?&gt;, Coercing&lt;?, ?&gt;&gt; scalars = Map.of(LocalDateTime.class, DateTimeScalar.INSTANCE.getCoercing());\n\nnew GraphQLQueryRequest(\n                ReviewsGraphQLQuery.newRequest().dateRange(new DateRange(LocalDate.of(2020, 1, 1), LocalDate.now())).build(),\n                new ReviewsProjectionRoot().submittedDate().starScore(), scalars);\n</code></pre> <p>This way you can re-use exactly the same serialization code that you already have for your scalar implementation or one of the existing ones from - for example - the <code>graphql-dgs-extended-scalars</code> module.</p>"},{"location":"advanced/java-client/#interface-projections","title":"Interface projections","text":"<p>When a field returns an interface, fields on the concrete types are specified using a fragment.</p> <pre><code>type Query @extends {\n    script(name: String): Script\n}\n\ninterface Script {\n    title: String\n    director: String\n    actors: [Actor]\n}\n\ntype MovieScript implements Script {\n    title: String\n    director: String\n    length: Int\n}\n\ntype ShowScript implements Script {\n    title: String\n    director: String\n    episodes: Int\n}\n</code></pre> <pre><code>query {\n    script(name: \"Top Secret\") {\n        title\n        ... on MovieScript {\n            length\n        }\n    }\n}\n</code></pre> <p>This syntax is supported by the Query builder as well.</p> <pre><code> GraphQLQueryRequest graphQLQueryRequest =\n    new GraphQLQueryRequest(\n        new ScriptGraphQLQuery.Builder()\n            .name(\"Top Secret\")\n            .build(),\n        new ScriptProjectionRoot()\n            .title()\n            .onMovieScript()\n                .length();\n    );\n</code></pre>"},{"location":"advanced/java-client/#building-federated-queries","title":"Building Federated Queries","text":"<p>You can use <code>GraphQLQueryRequest</code> along with <code>EntitiesGraphQLQuery</code> to generate federated queries. The API provides a type-safe way to construct the _entities query with the associated <code>representations</code> based on the input schema. The <code>representations</code> are passed in as a map of variables. Each representation class is generated based on the <code>key</code> fields defined  on the entity in your schema, along with the <code>__typename</code>. The <code>EntitiesProjectionRoot</code> is used to select query fields on the specified type.</p> <p>For example, let us look at a schema that extends a <code>Movie</code> type:</p> <pre><code>type Movie @key(fields: \"movieId\") @extends {\n    movieId: Int @external\n    script: MovieScript\n}\n\ntype MovieScript  {\n    title: String\n    director: String\n    actors: [Actor]\n}\n\ntype Actor {\n    name: String\n    gender: String\n    age: Int\n}\n</code></pre> <p>With client code generation, you will now have a <code>MovieRepresentation</code> containing the key field, i.e., <code>movieId</code>, and the <code>__typename</code> field already set to type <code>Movie</code>. Now you can add each representation to the <code>EntitiesGraphQLQuery</code> as a <code>representations</code> variable. You will also have a <code>EntitiesProjectionRoot</code> with <code>onMovie()</code> to select fields on <code>Movie</code> from. Finally, you put them all together as a <code>GraphQLQueryRequest</code>, which you serialize into the final query string. The map of <code>representations</code> variables is available via <code>getVariables</code> on the <code>EntitiesGraphQLQuery</code>.</p> <p>Here is an example for the schema shown earlier: <pre><code>        EntitiesGraphQLQuery entitiesQuery = new EntitiesGraphQLQuery.Builder()\n                    .addRepresentationAsVariable(\n                            MovieRepresentation.newBuilder().movieId(1122).build()\n                    )\n                    .build();\n        GraphQLQueryRequest request = new GraphQLQueryRequest(\n                    entitiesQuery,\n                    new EntitiesProjectionRoot().onMovie().movieId().script().title()\n                    );\n\n        String query  = request.serialize();\n        Map&lt;String, Object&gt; representations = entitiesQuery.getVariables();\n</code></pre></p>"},{"location":"advanced/java-client/#subscriptions","title":"Subscriptions","text":"<p>Subscriptions are supported through the <code>ReactiveGraphQLClient</code> interface. The interface has two implementations:</p> <ul> <li><code>WebSocketGraphQLClient</code>: For subscriptions over WebSockets</li> <li><code>SSESubscriptionGraphQLClient</code>: For subscriptions over Server Sent Events (SSE)</li> </ul> <p>Both implementations require the use of WebClient, and cannot be used with other HTTP clients (in contrast to the \"normal\" DGS client). The clients return a <code>Flux</code> of <code>GraphQLResponse</code>. Each <code>GraphQLResponse</code> represents a message pushed from the subscription, and contains <code>data</code> and <code>errors</code>. It also offers convenience methods to parse data using JsonPath.</p> JavaKotlin <pre><code>    WebClient webClient = WebClient.create(\"http://localhost:\" + port);\n    SSESubscriptionGraphQLClient client = new SSESubscriptionGraphQLClient(\"/subscriptions\", webClient);\n    Flux&lt;GraphQLResponse&gt; numbers = client.reactiveExecuteQuery(\"subscription {numbers}\", Collections.emptyMap());\n\n    numbers\n        .mapNotNull(r -&gt; r.extractValue(\"data.numbers\"))\n        .log()\n        .subscribe();\n</code></pre> <pre><code>val webClient = WebClient.create(\"http://localhost:$port\")\nval client = SSESubscriptionGraphQLClient(\"/subscriptions\", webClient)\nval reactiveExecuteQuery = client.reactiveExecuteQuery(\"subscription {numbers}\", emptyMap())\n\nreactiveExecuteQuery\n    .mapNotNull { r -&gt; r.data[\"numbers\"] }\n    .log()\n    .subscribe()\n</code></pre> <p>In case the connection fails to set up, either because of a connection error, or because of an invalid query, a <code>WebClientResponseException</code> will be thrown. Errors later on in the process will be errors in the stream.</p> <p>Don't forget to <code>subscribe()</code> to the stream, otherwise the connection doesn't get started!</p> <p>An example of using the client to write subscription integration tests is available here.</p>"},{"location":"advanced/kotlin-codegen/","title":"About","text":"<p>In addition to the Java and Kotlin code-gen, we also have an experimental Kotlin API that generates more idiomatic Kotlin classes. Instead of generating Java code that can be used in Kotlin, it generates native Kotlin code that leans into techniques that only Kotlin supports. This is split primarily into two parts: the data classes used to serialize responses (both in the server and in the client), and the client query projections</p>"},{"location":"advanced/kotlin-codegen/#data-classes","title":"Data Classes","text":"<p>Kotlin introduces strict nullability as a language feature. This is at odds with GraphQL in that it is often the case that response objects are partially defined. Because some fields may be absent simply because the caller didn't request them, all fields must support being absent. To solve for this, we must split these absent fields into two categories: those that are nullable fields that the user requested, where GQL defines the schema as nullable &amp; the Kotlin type is nullable, and those that the user did not request. For the latter, if they are consumed anywhere, we should throw an exception to alert the user that they are trying to use a field that was not requested. This split is advantageous because it can help us to catch errors earlier in the process: if a field is usually null, a client may not notice that it wasn't requested in the query, or the server may not notice that it was never being populated in responses.</p> <p>The generated classes in this mode wrap each field in a <code>Supplier&lt;T&gt;</code>, or in Kotlin, <code>() -&gt; T</code>. This is very similar to lazy values, but with some differences. What this allows us to do is defer the evaluation of a field until it is accessed, either on the server for serialization, or on the client for consumption. Luckily, the Kotlin closure syntax allows us a compact way to express these, simply by wrapping them in curly braces:</p> <pre><code>val series = Series(\n    title = { \"Stranger Things\" },\n    actors = { listOf(\"Millie Bobby Brown\", \"Finn Wolfhard\", \"Winona Ryder\", \"David Harbour\") },\n)\n</code></pre> <p>Alternatively, we also generate builder methods where you can construct these objects as such:</p> <pre><code>val series = Series.Builder()\n    .withTitle(\"Stranger Things\")\n    .withReleaseDate(2016)\n    .withEndDate(2024)\n</code></pre> <p>Note that in each of these examples, we're only populating partial objects, which is frequently the case in GQL. If the user were to access either the release dates in the first example, or the list of actors in the second, an exception would be thrown indicating that those fields are not populated.</p> <p>We can also specify explicit null response values for when we want to return an explicit null value for a field that was requested:</p> <pre><code>val series = Series(\n    title = { \"Black Mirror\" },\n    releaseDate = { 2011 },\n    endDate = { null },\n)\n</code></pre> <p>In this example, <code>endDate</code> would be a nullable Kotlin field, whereas <code>title</code> would be non-nullable. Also note that the supplier closure is not exposed when accessing the fields. These are all properties that fetch the value when accessed (or throw an exception if it was not populated)</p> <pre><code>val title: String = series.title\nval releaseDate: Int = series.releaseDate\nval endDate: Int? = series.endDate\n</code></pre>"},{"location":"advanced/kotlin-codegen/#query-projections","title":"Query Projections","text":"<p>Kotlin supports function literals with a receiver and this allows us to mimic the GQL query syntax, directly in the language. An advantage of writing queries directly in the language is that when the schema changes, any incompatibilities will show up as compile time errors, and the IDE can guide users to craft queries. Additionally, nested projections are nested in the query, just as they appear in a GQL query. For example:</p> <pre><code>val query: String = DgsClient.buildQuery {\n    series(title = \"Stranger Things\") {\n        actors {        // slightly different schema for example\n            name \n            age\n        }\n        releaseDate\n        endDate\n    }\n}\n</code></pre> <p>In the end, the only difference between a GQL query and the syntax above is that in GQL, the projection arguments are delineated with a <code>:</code> whereas in Kotlin we use <code>=</code>.</p>"},{"location":"advanced/kotlin-codegen/#usage","title":"Usage","text":"<p>To generate these Kotlin classes, use the following properties when configuring the code-gen plugin:</p> <pre><code>    language = 'kotlin'\n    generateClient = true\n    generateKotlinNullableClasses = true\n    generateKotlinClosureProjections = true\n</code></pre> <p>In order, they do the following:</p> <ul> <li>Use kotlin instead of java</li> <li>Enable client generation at all</li> <li>Generate the data classes described above</li> <li>Generate the query projections described above</li> </ul> <p>As part of your generated code, you'll have a <code>DgsClient</code> class, which will serve as the entrypoint for queries/mutations/subscriptions</p>"},{"location":"advanced/logging/","title":"Disabling logging of sensitive information","text":"<p>The notprivacysafe SLF4J logger from graphql-java provides logging at different steps of the query execution process.</p> <p>By default, all errors and invalid queries are logged by graphql-java. To disable this, include the following in your application.yml to turn off the logger: <pre><code>logging:\n  level:\n    notprivacysafe: OFF\n</code></pre></p> <p>When set to the debug level, the notprivacysafe logger will also log at the query execution, parsing, and validation steps: <pre><code>logging:\n  level:\n    notprivacysafe: DEBUG\n</code></pre></p> <p>This graphql-java issue adds support for configuring the logger.</p>"},{"location":"advanced/operation-caching/","title":"Operation Caching","text":"<p>Before operations (queries, mutations, and subscriptions) can be executed, their request string needs to be parsed and validated. Performing these two steps can be expensive.</p> <p>The GraphQL Java library opens up a special <code>PreparsedDocumentProvider</code> interface which intercepts these two steps and allows library consumers to cache, or modify the resulting operation.</p> <p>The DGS Framework supports injecting a <code>PreparsedDocumentProvider</code> by defining a bean of the same type.</p> <p>The following example uses Caffeine to cache the 2500 most recent operations for a maximum of 1 hour.</p> <pre><code>@Component // Resolved by Spring\npublic class CachingPreparsedDocumentProvider implements PreparsedDocumentProvider {\n\n    private final Cache&lt;String, PreparsedDocumentEntry&gt; cache = Caffeine\n            .newBuilder()\n            .maximumSize(2500)\n            .expireAfterAccess(Duration.ofHours(1))\n            .build();\n\n    @Override\n    public PreparsedDocumentEntry getDocument(ExecutionInput executionInput,\n                                              Function&lt;ExecutionInput, PreparsedDocumentEntry&gt; parseAndValidateFunction) {\n        return cache.get(executionInput.getQuery(), operationString -&gt; parseAndValidateFunction.apply(executionInput));\n    }\n}\n</code></pre> <p>The bean can also be injected using an annotated <code>@Bean</code> method:</p> <pre><code>@Configuration\npublic class MyDgsConfiguration {\n\n    @Bean\n    public PreparsedDocumentProvider cachingPreparsedDocumentProvider() {\n        return new CachingPreparsedDocumentProvider();\n    }\n}\n</code></pre>"},{"location":"advanced/operation-caching/#using-operation-variables","title":"Using operation variables","text":"<p>When using <code>PreparsedDocumentProvider</code> this way, it is important that you use operation variables in your operation. Otherwise, your cache may fill up with operations that are used only once, or contain personal information.</p> <p>This means that operations like the following:</p> <pre><code>query DgsPersonQuery {\n     person(id: \"123\") {\n        id\n        firstName\n     }\n}\n</code></pre> <p>Should be written as:</p> <pre><code>query DgsPersonQuery($personId: String!) {\n     person(id: $personId) {\n        id\n        firstName\n     }\n}\n</code></pre> <p>With the <code>personId</code> variable set to <code>\"123\"</code> in your specific client implementation.</p>"},{"location":"advanced/platform-bom/","title":"Using the Platform BOM","text":"<p>Using the Platform Bill of Materials (BOM)</p> <p>Both Gradle<sup>1</sup> and Maven<sup>2</sup> define a mechanism that developers can leverage to align the versions of dependencies that belong to the same framework, or an umbrella of dependencies that need to be aligned to work well together. Using them will prevent version conflicts and aide you figure out which dependency versions work well with each other.</p> <p>Let's go through a scenario, and assume you are using both the <code>graphql-dgs-spring-boot-starter</code> and the <code>graphql-dgs-subscriptions-websockets-autoconfigure</code>. Without using the platform/BOM you will have to define a version for each; unless the versions are explicitly maintained there is a chance that in the future they diverge. Manually aligning the versions of the dependencies becomes harder if your have a multi-module project where each module is using different dependencies of the DGS Framework, for example, the <code>graphql-dgs-client</code>. If you are using the platform/BOM you define the version of the DGS Framework in one place only, it will make sure that all other DGS Framework dependencies are using the same version.</p> <p>In the case of the DGS Framework we have two different BOM definitions, the <code>graphql-dgs-platform-dependencies</code> and the <code>graphql-dgs-platform</code>. The latter only defines version alignment for the DGS modules while the first also defines versions for the dependencies of the DGS framework, such as Spring, Jackson, and Kotlin.</p>"},{"location":"advanced/platform-bom/#using-the-platformbom","title":"Using the Platform/BOM","text":"<p>Warning</p> <p>If you are using the Spring Boot Dependency Management Plugin, please use the following syntax, such that the DGS BOM provides the version of <code>graphql-java</code>, and other managed dependencies. <pre><code>dependencyManagement {\n    imports {\n        // We need to define the DGS BOM as follows such that the\n        // io.spring.dependency-management plugin respects the versions expressed in the DGS BOM, e.g. graphql-java\n        mavenBom(\"com.netflix.graphql.dgs:graphql-dgs-platform-dependencies:latest.release\")\n    }\n}\n</code></pre></p> <p>If the DGS BOM is not expressed in the <code>dependencyManagement</code> block as an imported Maven BOM, the Spring Boot Dependency Management Plugin will force the <code>graphql-java</code> version to a lower version. This will cause conflicts with what the DGS framework, and other artifacts such as the federation library, expect.</p> <p>Let's go through an example and assume that we want to use the DGS Framework 3.10.2...</p> GradleGradle KotlinMaven <pre><code>repositories {\n    mavenCentral()\n}\n\ndependencies {\n    //DGS BOM/platform dependency. This is the only place you set version of DGS\n    implementation(platform('com.netflix.graphql.dgs:graphql-dgs-platform-dependencies:3.10.2'))\n\n    //DGS dependencies. We don't have to specify a version here!\n    implementation 'com.netflix.graphql.dgs:graphql-dgs-spring-boot-starter'\n    implementation 'com.netflix.graphql.dgs:graphql-dgs-subscriptions-websockets-autoconfigure'\n\n    //Additional Jackson dependency. We don't need to specify a version, because Jackson is part of the BOM/platform definition.\n    implementation 'com.fasterxml.jackson.datatype:jackson-datatype-joda'\n\n    //Other dependencies...\n}\n</code></pre> <pre><code>repositories {\n    mavenCentral()\n}\n\ndependencies {\n    //DGS BOM/platform dependency. This is the only place you set version of DGS\n    implementation(platform(\"com.netflix.graphql.dgs:graphql-dgs-platform-dependencies:3.10.2\"))\n\n    //DGS dependencies. We don't have to specify a version here!\n    implementation(\"com.netflix.graphql.dgs:graphql-dgs-spring-boot-starter\")\n    implementation(\"com.netflix.graphql.dgs:graphql-dgs-subscriptions-websockets-autoconfigure\")\n\n    //Additional Jackson dependency. We don't need to specify a version, because Jackson is part of the BOM/platform definition.\n    implementation(\"com.fasterxml.jackson.datatype:jackson-datatype-joda\")\n\n    //Other dependencies...\n}\n</code></pre> <pre><code>&lt;dependencyManagement&gt;\n    &lt;dependencies&gt;\n      &lt;dependency&gt;\n        &lt;groupId&gt;com.netflix.graphql.dgs&lt;/groupId&gt;\n        &lt;artifactId&gt;graphql-dgs-platform-dependencies&lt;/artifactId&gt;\n        &lt;!-- The DGS BOM/platform dependency. This is the only place you set version of DGS --&gt;\n        &lt;version&gt;3.10.2&lt;/version&gt;\n        &lt;type&gt;pom&lt;/type&gt;\n        &lt;scope&gt;import&lt;/scope&gt;\n      &lt;/dependency&gt;\n    &lt;/dependencies&gt;\n&lt;/dependencyManagement&gt;\n&lt;dependencies&gt;\n    &lt;!-- DGS dependencies. We don't have to specify a version here! --&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;com.netflix.graphql.dgs&lt;/groupId&gt;\n        &lt;artifactId&gt;graphql-dgs-spring-boot-starter&lt;/artifactId&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;com.netflix.graphql.dgs&lt;/groupId&gt;\n        &lt;artifactId&gt;graphql-dgs-subscriptions-websockets-autoconfigure&lt;/artifactId&gt;\n    &lt;/dependency&gt;\n    &lt;!-- Additional Jackson dependency. We don't need to specify a version, because Jackson is part of the BOM/platform definition. --&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;com.fasterxml.jackson.datatype&lt;/groupId&gt;\n        &lt;artifactId&gt;jackson-datatype-joda&lt;/artifactId&gt;\n    &lt;/dependency&gt;\n    &lt;!-- Other dependencies --&gt;\n&lt;/dependencies&gt;\n</code></pre> <p>Notice that the version is only specified on the platform dependency, and not on the <code>graphql-dgs-spring-boot-starter</code> and <code>graphql-dgs-subscriptions-websockets-autoconfigure</code>. The BOM will make sure that all DGS dependencies are aligned, in other words, using the same version. In addition, since we are using the <code>graphql-dgs-platform-dependencies</code>, we can use the DGS chosen version of some dependencies as well, such as Jackson.</p> <p>Note</p> <p>Versions in the platform are recommendations. The versions can be overridden by the user, or by other platforms you might be using.</p> <ol> <li> <p>Gradle supports this via the Java Platform, checkout the section that describes how to consume a Java platform.\u00a0\u21a9</p> </li> <li> <p>Maven supports this via the BOM. Note that the BOM will be consumed via the <code>dependencyManagement</code> block.\u00a0\u21a9</p> </li> </ol>"},{"location":"advanced/relay-pagination/","title":"Relay Pagination","text":""},{"location":"advanced/relay-pagination/#relay-pagination","title":"Relay Pagination","text":"<p>The DGS framework supports dynamic generation of schema types for cursor based pagination based on the relay spec. When a type in the graphql schema is annotated with the <code>@connection</code> directive, the framework generates the corresponding <code>Connection</code> and <code>Edge</code> types, along with the common <code>PageInfo</code>.</p> <p>This avoids boilerplate code around defining related Connection and Edge types in the schema for every type that needs to be paginated. </p> <p>Note</p> <p>The <code>@connection</code> directive only works for DGSs that are not required to register the static schema file with an external service (since the relay types are dynamically generated). For example, in a federated architecture involving a gateway, some gateway implementations may or may not recognize the <code>@connection</code> directive when working with a static schema file.</p>"},{"location":"advanced/relay-pagination/#set-up","title":"Set up","text":"<p>To enable the use of <code>@connection</code> directive for generating the schema for pagination, add the following module to dependencies in your build.gradle:</p> <pre><code>dependencies {\n    implementation 'com.netflix.graphql.dgs:graphql-dgs-pagination'\n}\n</code></pre> <p>Next, add the directive on the type you want to paginate.</p> <pre><code>type Query {\n      hello: MessageConnection\n}\n\ntype Message @connection {\n    name: String\n}\n</code></pre> <p>Note that the <code>@connection</code> directive is defined automatically by the framework, so there is no need to add it to your schema file.</p> <p>This results in the following relay types dynamically generated and added to the schema:</p> <pre><code>\"MessageConnection\"\ntype MessageConnection {\n  \"Field edges\"\n  edges: [MessageEdge]\n  \"Field pageInfo\"\n  pageInfo: PageInfo\n}\n\n\"MessageEdge\"\ntype MessageEdge {\n    \"Field node\"\n    node: Message\n    \"Field cursor\"\n    cursor: String\n}\n\n\"PageInfo\"\ntype PageInfo {\n    \"Field hasPreviousPage\"\n    hasPreviousPage: Boolean!\n    \"Field hasNextPage\"\n    hasNextPage: Boolean!\n    \"Field startCursor\"\n    startCursor: String\n    \"Field endCursor\"\n    endCursor: String\n}\n</code></pre> <p>You can now use the corresponding <code>graphql.relay</code> types for <code>Connection&lt;T&gt;</code>, <code>Edge&lt;T&gt;</code> and <code>PageInfo</code> to set up your datafetcher as shown here:</p> <pre><code>@DgsData(parentType = \"Query\", field = \"hello\")\npublic Connection&lt;Message&gt; hello(DataFetchingEnvironment env) {\n    return new SimpleListConnection&lt;&gt;(Collections.singletonList(new Message(\"This is a generated connection\"))).get(env);\n}\n</code></pre> <p>If your schema references a pagination type in a nested type, and you are using the code generation plugin, you will need some additional configuration, as described in the next section.</p>"},{"location":"advanced/relay-pagination/#testing-in-java","title":"Testing in Java","text":"<p>Don't forget to provide <code>DgsPaginationTypeDefinitionRegistry.class</code> and <code>PageInfo.class</code> when testing.</p> <pre><code>@SpringBootTest(classes = {DgsAutoConfiguration.class, DgsPaginationTypeDefinitionRegistry.class, PageInfo.class})\nclass Test {\n...\n\n## Configuring Code Generation \nIf you are using the [DGS Codegen Plugin](https://netflix.github.io/dgs/generating-code-from-schema/) for generating your data model, you will need to also add a type mapping for the relay types.\nThe code generation plugin does not process the `@connection` directive and therefore needs to be configured so the generated classes can refer to the mapped type.\n\nFor example,\n```gradle\ngenerateJava{\n  ...\n  typeMapping = [\"MessageConnection\": \"graphql.relay.SimpleListConnection&lt;Message&gt;\"]\n}\n</code></pre>"},{"location":"advanced/schema-reloading/","title":"Hot reloading schemas","text":"<p>The DGS framework is designed to work well with tools such as JRebel. In large Spring Boot codebases with many dependencies, it can take some time to restart the application during development. Waiting for the application to start can be disruptive to the development workflow.</p>"},{"location":"advanced/schema-reloading/#enabling-development-mode-for-hot-reloading","title":"Enabling development mode for hot reloading","text":"<p>Tools like JRebel allow for hot-reloading code. You make code changes compile, and without restarting the application, see the changes in the running application. Actively developing a DGS often includes making schema changes and wiring datafetchers. Some initialization needs to happen to pick up such changes. Out-of-the-box the DGS framework caches this initialization to be as efficient as possible in production, so the initialization only happens during startup.</p> <p>We can configure the DGS framework to run in development mode during development, which re-initializes the schema on each request. You can enable development mode in three ways:</p> <ol> <li>Set the <code>dgs.reload</code> configuration property to <code>true</code> (e.g. in <code>application.yml</code>)</li> <li>Enable the <code>laptop</code> profile</li> <li>Implement your own <code>ReloadIndicator</code> bean to be fully in control over when to reload. This is useful when working with fully dynamic schemas.</li> </ol>"},{"location":"advanced/security/","title":"Security","text":""},{"location":"advanced/security/#fine-grained-access-control-with-secured","title":"Fine-grained Access Control with @Secured","text":"<p>The DGS Framework integrates with Spring Security using the well known <code>@Secured</code> annotation. Spring Security itself can be configured in many ways, which goes beyond the scope of this documentation. Once Spring Security is set up however, you can apply <code>@Secured</code> to your data fetchers, very similarly to how you apply it to a REST Controller in Spring MVC.</p> <pre><code>@DgsComponent\npublic class SecurityExampleFetchers {\n    @DgsData(parentType = \"Query\", field = \"hello\")\n    public String hello() {\n        return \"Hello to everyone\";\n    }      \n\n    @Secured(\"admin\")\n    @DgsData(parentType = \"Query\", field = \"secureGroup\")\n    public String secureGroup() {\n        return \"Hello to admins only\";\n    }\n}\n</code></pre>"},{"location":"advanced/subscriptions/","title":"Subscriptions","text":"<p>GraphQL Subscriptions enable a client to receive updates for a query from the server over time. Pushing update notifications from the server is a good example.</p> <p>The DGS framework supports subscriptions out of the box.</p>"},{"location":"advanced/subscriptions/#the-server-side-programming-model","title":"The Server Side Programming Model","text":"<p>In the DGS framework a Subscription is implemented as a data fetcher with the <code>@DgsSubscription</code> annotation. The <code>@DgsSubscription</code> is just short-hand for <code>@DgsData(parentType = \"Subscription\")</code>. The difference with a normal data fetcher is that a subscription must return a <code>org.reactivestreams.Publisher</code>.</p> <pre><code>import reactor.core.publisher.Flux;\nimport org.reactivestreams.Publisher;\n\n@DgsSubscription\npublic Publisher&lt;Stock&gt; stocks() {\n    //Create a never-ending Flux that emits an item every second\n    return Flux.interval(Duration.ofSeconds(1)).map({ t -&gt; Stock(\"NFLX\", 500 + t) })\n}\n</code></pre> <p>The <code>Publisher</code> interface is from Reactive Streams. The Spring Framework comes with the Reactor library to work with Reactive Streams.</p> <p>A complete example can be found in <code>SubscriptionDatafetcher.java</code>.</p>"},{"location":"advanced/subscriptions/#websockets","title":"WebSockets","text":"<p>Spring for GraphQL provides the WebSockets transport layer for subscriptions. The framework supports the <code>graphql-ws</code> library which uses the <code>graphql-transport-ws</code> sub-protocol for Websockets for both WebMVC and Webflux stacks.</p> <p>To enable WebSockets you need to explicitly set the <code>spring.graphql.websocket.path</code> property.</p> <p>On WebMVC you also have to add the following dependency. No extra dependency is needed for Webflux.</p> <pre><code>implementation 'org.springframework.boot:spring-boot-starter-websocket'\n</code></pre>"},{"location":"advanced/subscriptions/#server-sent-events","title":"Server Sent Events","text":"<p>Server Sent Events are also supported by the Spring for GraphQL transport layer, as an alternative to Websockets. This can be useful for environments where only HTTP is supported, and Websockets is not an option. No extra configuration or dependencies are needed for SSE. To sent SSE requests you use the regular <code>/graphql</code> endpoint, but with the <code>text/event-stream</code> media type.</p>"},{"location":"advanced/subscriptions/#unit-testing-subscriptions","title":"Unit Testing Subscriptions","text":"<p>Similar to a \"normal\" data fetcher test, you use the <code>DgsQueryExecutor</code> to execute a query. Just like a normal query, this results in a <code>ExecutionResult</code>. Instead of returning a result directly in the <code>getData()</code> method, a subscription query returns a <code>Publisher</code>. A <code>Publisher</code> can be asserted using the testing capabilities from Reactor. Each <code>onNext</code> of the <code>Publisher</code> is another <code>ExecutionResult</code>. This <code>ExecutionResult</code> contains the actual data!</p> <p>It might take a minute to wrap your head around the concept of this nested <code>ExecutionResult</code>, but it gives an excellent way to test Subscriptions, including corner cases.</p> <p>The following is a simple example of such a test. The example tests the <code>stocks</code> subscription from above. The <code>stocks</code> subscription produces a result every second, so the test uses <code>VirtualTime</code> to fast-forward time, without needing to wait in the test.</p> <p>Also note that the emitted <code>ExecutionResult</code> returns a <code>Map&lt;String, Object&gt;</code>, and not the Java type that your data fetcher returns. Use the <code>Jackson Objectmapper</code> to convert the map to a Java object.</p> <pre><code>@SpringBootTest(classes = {SubscriptionDataFetcher.class, DgsExtendedScalarsAutoConfiguration.class, DgsPaginationAutoConfiguration.class, UploadScalar.class})\n@EnableDgsTest\nclass SubscriptionDataFetcherTest {\n\n    @Autowired\n    DgsQueryExecutor queryExecutor;\n\n    ObjectMapper objectMapper = new ObjectMapper();\n\n    @Test\n    void stocks() {\n        ExecutionResult executionResult = queryExecutor.execute(\"subscription Stocks { stocks { name, price } }\");\n        Publisher&lt;ExecutionResult&gt; publisher = executionResult.getData();\n\n        VirtualTimeScheduler virtualTimeScheduler = VirtualTimeScheduler.create();\n        StepVerifier.withVirtualTime(() -&gt; publisher, 3)\n                .expectSubscription()\n                .thenRequest(3)\n                .assertNext(result -&gt; assertThat(toStock(result).getPrice()).isEqualTo(500))\n                .assertNext(result -&gt; assertThat(toStock(result).getPrice()).isEqualTo(501))\n                .assertNext(result -&gt; assertThat(toStock(result).getPrice()).isEqualTo(502))\n                .thenCancel()\n                .verify();\n    }\n\n    private Stock toStock(ExecutionResult result) {\n        Map&lt;String, Object&gt; data = result.getData();\n        return objectMapper.convertValue(data.get(\"stocks\"), Stock.class);\n    }\n}\n</code></pre> <p>In this example the subscription works in isolation; it just emits a result every second. In other scenarios a subscription could depend on something else happening in the system, such as the processing of a mutation. Such scenarios are easy to set up in a unit test, simply run multiple queries/mutations in your test to see it all work together.</p> <p>Notice that the unit tests really only test your code. It doesn't care about transport protocols. This is exactly what you need for your tests, because your tests should focus on testing your code, not the framework code.</p>"},{"location":"advanced/subscriptions/#integration-testing-subscriptions","title":"Integration testing subscriptions","text":"<p>Although most subscription logic should be tested in unit tests, it can be useful to test end-to-end with a client. This can be achieved with the DGS client, and works well in a <code>@SpringBootTest</code> with a random port, and the <code>WebSocketGraphQLTester</code>. The example below starts a subscription, and sends to mutations that should result in updates on the subscription.</p> <pre><code>@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT)\npublic class SubscriptionsGraphQlTesterTest {\n\n    @LocalServerPort\n    private int port;\n\n    @Value(\"http://localhost:${local.server.port}/graphql\")\n    private String baseUrl;\n\n    private GraphQlTester graphQlTester;\n\n\n    @BeforeEach\n    void setUp() {\n        URI url = URI.create(baseUrl);\n        this.graphQlTester = WebSocketGraphQlTester.builder(url, new ReactorNettyWebSocketClient()).build();\n    }\n\n    @Test\n    void stocks() {\n        Flux&lt;Stock&gt; result = graphQlTester.document(\"subscription Stocks { stocks { name, price } }\").executeSubscription().toFlux(\"stocks\", Stock.class);\n\n        StepVerifier.create(result)\n                .assertNext(res -&gt; Assertions.assertThat(res.getPrice()).isEqualTo(500))\n                .assertNext(res -&gt; Assertions.assertThat(res.getPrice()).isEqualTo(501))\n                .assertNext(res -&gt; Assertions.assertThat(res.getPrice()).isEqualTo(502))\n                .thenCancel()\n                .verify();\n    }\n}\n</code></pre>"},{"location":"advanced/type-resolvers-for-abstract-types/","title":"Interfaces and Unions","text":"<p>You must register type resolvers whenever you use interface types or union types in your schema. Interface types and union types are explained in the GraphQL documentation.</p> <p>As an example, the following schema defines a <code>Movie</code> interface type with two different concrete object type implementations.</p> <pre><code>type Query {\n    movies: [Movie]\n}\n\ninterface Movie {\n    title: String\n}\n\ntype ScaryMovie implements Movie {\n    title: String\n    gory: Boolean\n    scareFactor: Int\n}\n\ntype ActionMovie implements Movie {\n    title: String\n    nrOfExplosions: Int\n}\n</code></pre> <p>The following data fetcher is registered to return a list of movies. The data fetcher returns a combination <code>Movie</code> types.</p> <pre><code>@DgsComponent\npublic class MovieDataFetcher {\n    @DgsData(parentType = \"Query\", field = \"movies\")\n    public List&lt;Movie&gt; movies() {\n        return Lists.newArrayList(\n                new ActionMovie(\"Crouching Tiger\", 0),\n                new ActionMovie(\"Black hawk down\", 10),\n                new ScaryMovie(\"American Horror Story\", true, 10),\n                new ScaryMovie(\"Love Death + Robots\", false, 4)\n            );\n    }\n}\n</code></pre> <p>The GraphQL runtime needs to know that a Java instance of <code>ActionMovie</code> represents the <code>ActionMovie</code> GraphQL type. This mapping is the responsibility of a <code>TypeResolver</code>.</p>  Tip:     If your Java type names and GraphQL type names are the same, the DGS framework creates a `TypeResolver` automatically.      No code needs to be added!"},{"location":"advanced/type-resolvers-for-abstract-types/#registering-a-type-resolver","title":"Registering a Type Resolver","text":"<p>If the name of your Java type and GraphQL type don't match, you need to provide a <code>TypeResolver</code>. A type resolver helps the framework map from concrete Java types to the correct object type in the schema.</p> <p>Use the <code>@DgsTypeResolver</code> annotation to register a type resolver. The annotation has a <code>name</code> property; set this to the name of the interface type or union type in the [GraphQL] schema. The resolver takes an object of the Java interface type, and returns a String which is the concrete object type of the instance as defined in the schema. The following is a type resolver for the <code>Movie</code> interface type introduced above:</p> <pre><code>@DgsTypeResolver(name = \"Movie\")\npublic String resolveMovie(Movie movie) {\n    if(movie instanceof ScaryMovie) {\n        return \"ScaryMovie\";\n    } else if(movie instanceof ActionMovie) {\n        return \"ActionMovie\";\n    } else {\n        throw new RuntimeException(\"Invalid type: \" + movie.getClass().getName() + \" found in MovieTypeResolver\");\n    }\n}\n</code></pre> <p>You can add the <code>@DgsTypeResolver</code> annotation to any <code>@DgsComponent</code> class. This means you can either keep the type resolver in the same class as the data fetcher responsible for returning the data for this type, or you can create a separate class for it.</p>"},{"location":"advanced/virtual-threads/","title":"Using Virtual Threads","text":""},{"location":"advanced/virtual-threads/#datafetching-threading-model-without-virtual-threads","title":"Datafetching threading model without Virtual Threads","text":"<p>Without virtual threads, the DGS framework does not create or manage threads for data fetchers. Every data fetcher will run on the same request thread. This effectively means that data fetchers do not have any concurrent/parallel behavior, except when explicitly returning <code>CompletableFuture</code>. The reason for not having concurrent behavior by default is that this would require using a thread per field, which would not scale well.</p> <p>To achieve concurrency in data fetchers, you can return <code>CompletableFuture</code> from a data fetcher. A <code>CompletableFuture</code> will let you define the <code>Executor</code> it will run on, giving you control over managing thread pools.</p>"},{"location":"advanced/virtual-threads/#virtual-threads-jdk-21","title":"Virtual Threads (JDK 21+)","text":"<p>Working with <code>CompletableFuture</code> to get concurrent data fetcher behavior isn't ideal for the developer. It adds complexity to the code, and managing and tuning thread pools can be complicated when running at scale.</p> <p>With the introduction of Virtual Threads in JDK 21 we no longer have to worry about the scalability limitations of creating threads. The DGS framework has support for Virtual Threads, which brings concurrent data fetchers out of the box. Virtual Thread support can be enabled with the following property:</p> <pre><code>dgs.graphql.virtualthreads.enabled=true\n</code></pre> <p>When enabled, each user defined data fetcher will run in a new virtual thread. Because virtual threads don't need thread pool management, you do not need to configure any thread pool. A \"user defined data fetcher\" is a data fetcher defined with <code>@DgsQuery</code>/<code>@DgsMutation</code>/<code>@DgsData</code>. Technically, every field of a POJO also has a data fetcher, but those \"trivial\" data fetchers will not run in separate virtual threads.</p> <p>Because each data fetcher runs in its own virtual thread, you get concurrent/parallel behavior out of the box.</p> <p>Note</p> <p>Note that data fetchers explicitly returning <code>CompletableFuture</code> do not get wrapped in a virtual thread. This way you keep the option of managing your own thread pools if needed for your use case.</p> <p>Spring Framework 6.1 (Spring Boot 3.2) comes with virtual thread support as well. When using Spring WebMVC on Tomcat, virtual threads can be used for the tomcat work threads.</p> <p>This is separate from the DGS virtual thread support, but combining the two gives a fully virtual thread based stack, making scaling much easier.</p> <p>Spring Framework support for virtual threads is enabled with the following property.</p> <pre><code>spring.threads.virtual.enabled=true\n</code></pre> <p>With virtual threads enabled for both DGS and Spring, a request would be processed as follows.</p> <p></p>"},{"location":"advanced/virtual-threads/#threading-for-data-loaders","title":"Threading for data loaders","text":"<p>The Dataloader API requires you to interact with the <code>CompletableFuture</code> API. The <code>CompletableFuture</code> API does not allow to change the thread it runs on after creating, so the DGS Framework can't automagically leverage virtual threads. You are still in control of which <code>Executor</code> is used when creating a <code>CompletableFuture</code>. If you want to use virtual threads for a <code>CompletableFuture</code>, you can pass a virtual thread based <code>Executor</code> as the second argument of many of the <code>CompletableFuture</code> factory methods. DGS provides a virtual threads executor that you can use for this.</p> <pre><code>@Autowired\n@Qualifier(\"dgsAsyncTaskExecutor\")\nExecutor executor;\n</code></pre> <p>When creating the <code>CompletableFuture</code> you pass in this executor as the second argument of many of the <code>CompletableFuture</code> factory methods.</p> <pre><code>return CompletableFuture.supplyAsync(() -&gt; \"My result\", executor);\n</code></pre>"},{"location":"advanced/virtual-threads/#virtual-threading-gotchas","title":"Virtual threading gotchas","text":"<p>Virtual threads work the same as platform threads when it comes to writing code. When data fetchers run on virtual threads, you might need to take care of context propagation if you store anything on <code>ThreadLocal</code>.</p> <p>When running tests using the <code>DgsQueryExecutor</code> you should be aware that the data fetchers run in a different thread from your test. This can lead to different behavior when using Spring's <code>@Transactional</code> for data access. A <code>@Transactional</code> <code>@SpringBootTest</code> would normally start a transaction from the test, and roll back the transaction at the end of the test, so that tests don't interfere with each other's data. Transactional behavior in Spring does not span multiple threads. Effectively this means that the automatic transaction rollback in tests does not work. You can fix this issue by either not depending on this behavior in tests (preferred), or disable DGS virtual threads in the <code>@SpringBootTest</code> itself.</p>"}]}